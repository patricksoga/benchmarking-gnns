2023-01-22 02:34:09,910:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-01-22 02:34:09,910:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-22 02:45:34,860:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-22 02:46:11,335:ogbdata.py:332 -             __init__(): Time taken: 721.4249s
2023-01-22 02:46:11,335:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-22 02:46:11,335:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-22 02:46:11,335:ogbdata.py:348 -             __init__(): [I] Data load time: 721.4252s
2023-01-22 02:46:11,335:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'residual': True, 'readout': 'sum', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'wl_pos_enc': False, 'full_graph': False, 'lpe_layers': 1, 'pos_enc_dim': 10, 'lpe_n_heads': 4, 'edge_feat': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'partial_rw_pos_enc': False, 'lpe_dim': 8, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/SAGraphTransformer/no_pe/gt_ogb_b1024-baseline-noedge/10_DEBUG_0_41.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [41]}
2023-01-22 02:46:11,335:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 10, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b128-noedge', 'job_num': 10}
2023-01-22 02:46:11,337:pe_layer.py:99 -             __init__(): no_pe
2023-01-22 02:46:11,337:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:46:11,337:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:46:11,369:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-22 02:46:11,372:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: SAGraphTransformer, 570215
2023-01-22 02:46:11,372:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 41 in [41]...
2023-01-22 02:46:11,372:pe_layer.py:99 -             __init__(): no_pe
2023-01-22 02:46:11,372:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:46:11,372:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:46:12,756:main_OGB_graph_regression.py:83 -   train_val_pipeline(): [!] Adding Laplacian decompositions for spectral attention.
2023-01-22 04:41:08,174:main_OGB_graph_regression.py:85 -   train_val_pipeline(): Time PE:6896.770631313324
2023-01-22 04:41:08,580:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-22 04:41:08,580:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-22 04:41:08,612:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-22 05:00:03,838:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.26198938488960266
2023-01-22 05:00:03,967:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1135.35s, LR: 0.00070, Train Loss: 0.2748, Train MAE: 0.2748,
                            Val Loss: 0.2626, Val MAE: 0.2620
2023-01-22 05:00:03,967:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-22 05:16:02,152:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2400563359260559
2023-01-22 05:16:02,181:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 958.21s, LR: 0.00070, Train Loss: 0.1898, Train MAE: 0.1898,
                            Val Loss: 0.2407, Val MAE: 0.2401
2023-01-22 05:16:02,182:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-22 05:32:01,879:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22207774221897125
2023-01-22 05:32:01,959:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.78s, LR: 0.00070, Train Loss: 0.1747, Train MAE: 0.1747,
                            Val Loss: 0.2226, Val MAE: 0.2221
2023-01-22 05:32:01,960:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-22 05:47:59,412:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.21139322221279144
2023-01-22 05:47:59,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 957.48s, LR: 0.00070, Train Loss: 0.1658, Train MAE: 0.1658,
                            Val Loss: 0.2117, Val MAE: 0.2114
2023-01-22 05:47:59,440:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-22 06:03:54,008:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18069376051425934
2023-01-22 06:03:54,047:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.61s, LR: 0.00070, Train Loss: 0.1584, Train MAE: 0.1584,
                            Val Loss: 0.1811, Val MAE: 0.1807
2023-01-22 06:03:54,048:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-22 06:19:47,801:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.75s, LR: 0.00070, Train Loss: 0.1533, Train MAE: 0.1533,
                            Val Loss: 0.1935, Val MAE: 0.1929
2023-01-22 06:19:47,828:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-22 06:35:46,732:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17037245631217957
2023-01-22 06:35:46,763:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 958.93s, LR: 0.00070, Train Loss: 0.1498, Train MAE: 0.1498,
                            Val Loss: 0.1709, Val MAE: 0.1704
2023-01-22 06:35:46,764:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-22 06:51:46,485:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1674632728099823
2023-01-22 06:51:46,557:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.79s, LR: 0.00070, Train Loss: 0.1460, Train MAE: 0.1460,
                            Val Loss: 0.1680, Val MAE: 0.1675
2023-01-22 06:51:46,559:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-22 07:07:46,208:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.65s, LR: 0.00070, Train Loss: 0.1439, Train MAE: 0.1439,
                            Val Loss: 0.1783, Val MAE: 0.1775
2023-01-22 07:07:46,280:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-22 07:23:46,694:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14959466457366943
2023-01-22 07:23:46,710:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 960.43s, LR: 0.00070, Train Loss: 0.1408, Train MAE: 0.1408,
                            Val Loss: 0.1500, Val MAE: 0.1496
2023-01-22 07:23:46,711:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-22 07:39:45,925:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.21s, LR: 0.00070, Train Loss: 0.1383, Train MAE: 0.1383,
                            Val Loss: 0.1583, Val MAE: 0.1577
2023-01-22 07:39:45,954:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-22 07:55:46,178:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 960.22s, LR: 0.00070, Train Loss: 0.1358, Train MAE: 0.1358,
                            Val Loss: 0.1697, Val MAE: 0.1692
2023-01-22 07:55:46,214:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-22 08:11:45,948:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.73s, LR: 0.00070, Train Loss: 0.1336, Train MAE: 0.1336,
                            Val Loss: 0.1774, Val MAE: 0.1767
2023-01-22 08:11:45,959:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-22 08:27:45,720:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.76s, LR: 0.00070, Train Loss: 0.1321, Train MAE: 0.1321,
                            Val Loss: 0.1518, Val MAE: 0.1513
2023-01-22 08:27:45,746:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-22 08:43:44,119:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 958.37s, LR: 0.00070, Train Loss: 0.1303, Train MAE: 0.1303,
                            Val Loss: 0.1679, Val MAE: 0.1671
2023-01-22 08:43:44,149:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-22 09:01:03,630:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1495397388935089
2023-01-22 09:01:03,644:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1039.49s, LR: 0.00070, Train Loss: 0.1295, Train MAE: 0.1295,
                            Val Loss: 0.1502, Val MAE: 0.1495
2023-01-22 09:01:03,645:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-22 09:17:04,328:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 960.68s, LR: 0.00070, Train Loss: 0.1281, Train MAE: 0.1281,
                            Val Loss: 0.1570, Val MAE: 0.1566
2023-01-22 09:17:04,355:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-22 09:32:59,028:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.67s, LR: 0.00070, Train Loss: 0.1269, Train MAE: 0.1269,
                            Val Loss: 0.1528, Val MAE: 0.1518
2023-01-22 09:32:59,058:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-22 09:48:51,830:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 952.77s, LR: 0.00070, Train Loss: 0.1249, Train MAE: 0.1249,
                            Val Loss: 0.1563, Val MAE: 0.1553
2023-01-22 09:48:51,853:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-22 10:04:45,975:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14318139851093292
2023-01-22 10:04:46,000:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.15s, LR: 0.00070, Train Loss: 0.1241, Train MAE: 0.1241,
                            Val Loss: 0.1438, Val MAE: 0.1432
2023-01-22 10:04:46,001:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-22 10:20:38,080:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14147260785102844
2023-01-22 10:20:38,095:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 952.09s, LR: 0.00070, Train Loss: 0.1237, Train MAE: 0.1237,
                            Val Loss: 0.1420, Val MAE: 0.1415
2023-01-22 10:20:38,096:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-22 10:36:30,860:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 952.76s, LR: 0.00070, Train Loss: 0.1218, Train MAE: 0.1218,
                            Val Loss: 0.1534, Val MAE: 0.1527
2023-01-22 10:36:30,861:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-22 10:52:24,034:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.17s, LR: 0.00070, Train Loss: 0.1215, Train MAE: 0.1215,
                            Val Loss: 0.1453, Val MAE: 0.1449
2023-01-22 10:52:24,068:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-22 11:08:17,194:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1362992376089096
2023-01-22 11:08:17,196:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.13s, LR: 0.00070, Train Loss: 0.1202, Train MAE: 0.1202,
                            Val Loss: 0.1367, Val MAE: 0.1363
2023-01-22 11:08:17,198:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-22 11:24:11,443:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.24s, LR: 0.00070, Train Loss: 0.1204, Train MAE: 0.1204,
                            Val Loss: 0.1492, Val MAE: 0.1483
2023-01-22 11:24:11,475:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-22 11:40:05,091:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13483725488185883
2023-01-22 11:40:05,093:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.62s, LR: 0.00070, Train Loss: 0.1188, Train MAE: 0.1188,
                            Val Loss: 0.1353, Val MAE: 0.1348
2023-01-22 11:40:05,094:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-22 11:55:59,313:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.22s, LR: 0.00070, Train Loss: 0.1180, Train MAE: 0.1180,
                            Val Loss: 0.1468, Val MAE: 0.1461
2023-01-22 11:55:59,349:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-22 12:11:53,548:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.20s, LR: 0.00070, Train Loss: 0.1174, Train MAE: 0.1174,
                            Val Loss: 0.1507, Val MAE: 0.1501
2023-01-22 12:11:53,549:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-22 12:27:46,604:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1333952099084854
2023-01-22 12:27:46,632:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.08s, LR: 0.00070, Train Loss: 0.1166, Train MAE: 0.1166,
                            Val Loss: 0.1338, Val MAE: 0.1334
2023-01-22 12:27:46,633:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-22 12:43:41,612:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.98s, LR: 0.00070, Train Loss: 0.1163, Train MAE: 0.1163,
                            Val Loss: 0.1537, Val MAE: 0.1530
2023-01-22 12:43:41,614:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-22 12:59:35,241:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.63s, LR: 0.00070, Train Loss: 0.1159, Train MAE: 0.1159,
                            Val Loss: 0.1435, Val MAE: 0.1429
2023-01-22 12:59:35,242:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-22 13:15:29,397:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.15s, LR: 0.00070, Train Loss: 0.1153, Train MAE: 0.1153,
                            Val Loss: 0.1397, Val MAE: 0.1390
2023-01-22 13:15:29,433:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-22 13:31:23,566:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.13s, LR: 0.00070, Train Loss: 0.1146, Train MAE: 0.1146,
                            Val Loss: 0.1565, Val MAE: 0.1558
2023-01-22 13:31:23,603:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-22 13:47:18,233:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.63s, LR: 0.00070, Train Loss: 0.1144, Train MAE: 0.1144,
                            Val Loss: 0.1386, Val MAE: 0.1380
2023-01-22 13:47:18,265:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-22 14:03:12,278:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.01s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1362, Val MAE: 0.1357
2023-01-22 14:03:12,279:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-22 14:20:28,001:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1035.72s, LR: 0.00070, Train Loss: 0.1135, Train MAE: 0.1135,
                            Val Loss: 0.1378, Val MAE: 0.1371
2023-01-22 14:20:28,038:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-22 14:36:22,817:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.78s, LR: 0.00070, Train Loss: 0.1133, Train MAE: 0.1133,
                            Val Loss: 0.1411, Val MAE: 0.1403
2023-01-22 14:36:22,819:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-01-22 14:52:16,345:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.132280632853508
2023-01-22 14:52:16,379:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.56s, LR: 0.00070, Train Loss: 0.1127, Train MAE: 0.1127,
                            Val Loss: 0.1327, Val MAE: 0.1323
2023-01-22 14:52:16,380:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-01-22 15:08:10,371:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13145047426223755
2023-01-22 15:08:10,407:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.03s, LR: 0.00070, Train Loss: 0.1123, Train MAE: 0.1123,
                            Val Loss: 0.1321, Val MAE: 0.1315
2023-01-22 15:08:10,408:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-01-22 15:24:03,846:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.44s, LR: 0.00070, Train Loss: 0.1117, Train MAE: 0.1117,
                            Val Loss: 0.1331, Val MAE: 0.1324
2023-01-22 15:24:03,876:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-01-22 15:39:58,450:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.57s, LR: 0.00070, Train Loss: 0.1114, Train MAE: 0.1114,
                            Val Loss: 0.1394, Val MAE: 0.1386
2023-01-22 15:39:58,451:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-01-22 15:55:52,327:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1299601048231125
2023-01-22 15:55:52,359:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.91s, LR: 0.00070, Train Loss: 0.1111, Train MAE: 0.1111,
                            Val Loss: 0.1305, Val MAE: 0.1300
2023-01-22 15:55:52,360:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-01-22 16:11:46,435:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.07s, LR: 0.00070, Train Loss: 0.1109, Train MAE: 0.1109,
                            Val Loss: 0.1356, Val MAE: 0.1350
2023-01-22 16:11:46,437:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-01-22 16:27:40,605:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.17s, LR: 0.00070, Train Loss: 0.1107, Train MAE: 0.1107,
                            Val Loss: 0.1323, Val MAE: 0.1317
2023-01-22 16:27:40,632:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-01-22 16:43:34,244:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.61s, LR: 0.00070, Train Loss: 0.1100, Train MAE: 0.1100,
                            Val Loss: 0.1331, Val MAE: 0.1325
2023-01-22 16:43:34,245:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-01-22 16:59:29,831:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 955.59s, LR: 0.00070, Train Loss: 0.1095, Train MAE: 0.1095,
                            Val Loss: 0.1314, Val MAE: 0.1308
2023-01-22 16:59:29,833:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-01-22 17:15:24,429:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.59s, LR: 0.00070, Train Loss: 0.1095, Train MAE: 0.1095,
                            Val Loss: 0.1358, Val MAE: 0.1353
2023-01-22 17:15:24,468:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-01-22 17:31:18,701:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1272869110107422
2023-01-22 17:31:18,704:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.24s, LR: 0.00070, Train Loss: 0.1093, Train MAE: 0.1093,
                            Val Loss: 0.1277, Val MAE: 0.1273
2023-01-22 17:31:18,705:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-01-22 17:47:13,138:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.43s, LR: 0.00070, Train Loss: 0.1090, Train MAE: 0.1090,
                            Val Loss: 0.1386, Val MAE: 0.1380
2023-01-22 17:47:13,158:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-01-22 18:03:07,750:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.59s, LR: 0.00070, Train Loss: 0.1082, Train MAE: 0.1082,
                            Val Loss: 0.1377, Val MAE: 0.1372
2023-01-22 18:03:07,787:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-01-22 18:19:01,937:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.15s, LR: 0.00070, Train Loss: 0.1085, Train MAE: 0.1085,
                            Val Loss: 0.1301, Val MAE: 0.1297
2023-01-22 18:19:01,973:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-01-22 18:34:56,041:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.07s, LR: 0.00070, Train Loss: 0.1080, Train MAE: 0.1080,
                            Val Loss: 0.1359, Val MAE: 0.1353
2023-01-22 18:34:56,042:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-01-22 18:50:51,057:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 955.01s, LR: 0.00070, Train Loss: 0.1079, Train MAE: 0.1079,
                            Val Loss: 0.1296, Val MAE: 0.1291
2023-01-22 18:50:51,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-01-22 19:06:44,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.45s, LR: 0.00070, Train Loss: 0.1077, Train MAE: 0.1077,
                            Val Loss: 0.1396, Val MAE: 0.1389
2023-01-22 19:06:44,538:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-01-22 19:23:59,468:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1034.93s, LR: 0.00070, Train Loss: 0.1072, Train MAE: 0.1072,
                            Val Loss: 0.1381, Val MAE: 0.1375
2023-01-22 19:23:59,503:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-01-22 19:39:53,019:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.52s, LR: 0.00070, Train Loss: 0.1072, Train MAE: 0.1072,
                            Val Loss: 0.1283, Val MAE: 0.1277
2023-01-22 19:39:53,020:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-01-22 19:55:47,815:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.79s, LR: 0.00070, Train Loss: 0.1067, Train MAE: 0.1067,
                            Val Loss: 0.1326, Val MAE: 0.1317
2023-01-22 19:55:47,852:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-01-22 20:11:42,089:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12660644948482513
2023-01-22 20:11:42,091:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.24s, LR: 0.00070, Train Loss: 0.1066, Train MAE: 0.1066,
                            Val Loss: 0.1273, Val MAE: 0.1266
2023-01-22 20:11:42,091:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-01-22 20:27:36,764:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12606650590896606
2023-01-22 20:27:36,801:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.71s, LR: 0.00070, Train Loss: 0.1064, Train MAE: 0.1064,
                            Val Loss: 0.1266, Val MAE: 0.1261
2023-01-22 20:27:36,803:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-01-22 20:43:31,232:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12217408418655396
2023-01-22 20:43:31,234:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.43s, LR: 0.00070, Train Loss: 0.1060, Train MAE: 0.1060,
                            Val Loss: 0.1226, Val MAE: 0.1222
2023-01-22 20:43:31,235:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-01-22 20:59:24,997:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12208151817321777
2023-01-22 20:59:24,998:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.76s, LR: 0.00070, Train Loss: 0.1060, Train MAE: 0.1060,
                            Val Loss: 0.1224, Val MAE: 0.1221
2023-01-22 20:59:24,999:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-01-22 21:15:19,519:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.52s, LR: 0.00070, Train Loss: 0.1057, Train MAE: 0.1057,
                            Val Loss: 0.1309, Val MAE: 0.1304
2023-01-22 21:15:19,553:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-01-22 21:31:13,157:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.60s, LR: 0.00070, Train Loss: 0.1054, Train MAE: 0.1054,
                            Val Loss: 0.1297, Val MAE: 0.1289
2023-01-22 21:31:13,181:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-01-22 21:47:08,364:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 955.18s, LR: 0.00070, Train Loss: 0.1053, Train MAE: 0.1053,
                            Val Loss: 0.1258, Val MAE: 0.1254
2023-01-22 21:47:08,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-01-22 22:03:02,351:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 953.97s, LR: 0.00070, Train Loss: 0.1054, Train MAE: 0.1054,
                            Val Loss: 0.1292, Val MAE: 0.1285
2023-01-22 22:03:02,353:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-01-22 22:19:00,932:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 958.58s, LR: 0.00070, Train Loss: 0.1049, Train MAE: 0.1049,
                            Val Loss: 0.1275, Val MAE: 0.1269
2023-01-22 22:19:00,967:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-01-22 22:34:58,054:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 957.09s, LR: 0.00070, Train Loss: 0.1048, Train MAE: 0.1048,
                            Val Loss: 0.1253, Val MAE: 0.1247
2023-01-22 22:34:58,056:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-01-22 22:50:52,815:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 954.76s, LR: 0.00070, Train Loss: 0.1047, Train MAE: 0.1047,
                            Val Loss: 0.1255, Val MAE: 0.1249
2023-01-22 22:50:52,840:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 69/1000
2023-01-22 23:06:51,384:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 958.54s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1394, Val MAE: 0.1387
2023-01-22 23:06:51,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 70/1000
2023-01-22 23:22:49,294:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 957.91s, LR: 0.00070, Train Loss: 0.1044, Train MAE: 0.1044,
                            Val Loss: 0.1287, Val MAE: 0.1280
2023-01-22 23:22:49,331:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 71/1000
2023-01-22 23:38:49,795:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 960.46s, LR: 0.00070, Train Loss: 0.1040, Train MAE: 0.1040,
                            Val Loss: 0.1336, Val MAE: 0.1324
2023-01-22 23:38:49,796:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 72/1000
2023-01-22 23:54:52,837:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 963.04s, LR: 0.00070, Train Loss: 0.1037, Train MAE: 0.1037,
                            Val Loss: 0.1247, Val MAE: 0.1242
2023-01-22 23:54:52,871:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 73/1000
2023-01-23 00:10:52,330:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11898169666528702
2023-01-23 00:10:52,361:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 959.49s, LR: 0.00035, Train Loss: 0.0964, Train MAE: 0.0964,
                            Val Loss: 0.1197, Val MAE: 0.1190
2023-01-23 00:10:52,361:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 74/1000
2023-01-23 00:28:18,953:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1046.59s, LR: 0.00035, Train Loss: 0.0952, Train MAE: 0.0952,
                            Val Loss: 0.1234, Val MAE: 0.1225
2023-01-23 00:28:18,985:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 75/1000
2023-01-23 00:44:23,458:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 964.47s, LR: 0.00035, Train Loss: 0.0946, Train MAE: 0.0946,
                            Val Loss: 0.1233, Val MAE: 0.1224
2023-01-23 00:44:23,489:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 76/1000
2023-01-23 01:00:28,719:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11874029040336609
2023-01-23 01:00:28,722:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 965.23s, LR: 0.00035, Train Loss: 0.0943, Train MAE: 0.0943,
                            Val Loss: 0.1195, Val MAE: 0.1187
2023-01-23 01:00:28,722:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 77/1000
2023-01-23 01:16:33,649:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11871995031833649
2023-01-23 01:16:33,679:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 964.96s, LR: 0.00035, Train Loss: 0.0940, Train MAE: 0.0940,
                            Val Loss: 0.1194, Val MAE: 0.1187
2023-01-23 01:16:33,681:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 78/1000
2023-01-23 01:32:40,187:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 966.51s, LR: 0.00035, Train Loss: 0.0936, Train MAE: 0.0936,
                            Val Loss: 0.1239, Val MAE: 0.1229
2023-01-23 01:32:40,188:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 79/1000
2023-01-23 01:48:46,571:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11870025843381882
2023-01-23 01:48:46,603:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 966.41s, LR: 0.00035, Train Loss: 0.0934, Train MAE: 0.0934,
                            Val Loss: 0.1195, Val MAE: 0.1187
2023-01-23 01:48:46,604:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 80/1000
2023-01-23 02:04:51,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 965.18s, LR: 0.00035, Train Loss: 0.0932, Train MAE: 0.0932,
                            Val Loss: 0.1249, Val MAE: 0.1238
2023-01-23 02:04:51,789:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 81/1000
2023-01-23 02:20:57,751:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 965.96s, LR: 0.00035, Train Loss: 0.0930, Train MAE: 0.0930,
                            Val Loss: 0.1208, Val MAE: 0.1200
2023-01-23 02:20:57,784:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 82/1000
2023-01-23 02:37:03,881:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11776544153690338
2023-01-23 02:37:03,883:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 966.10s, LR: 0.00035, Train Loss: 0.0929, Train MAE: 0.0929,
                            Val Loss: 0.1185, Val MAE: 0.1178
2023-01-23 02:37:03,884:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 83/1000
2023-01-23 02:53:08,523:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 964.64s, LR: 0.00035, Train Loss: 0.0927, Train MAE: 0.0927,
                            Val Loss: 0.1232, Val MAE: 0.1223
2023-01-23 02:53:08,540:main_OGB_graph_regression.py:231 -   train_val_pipeline(): -----------------------------------------------------------------------------------------
2023-01-23 02:53:08,540:main_OGB_graph_regression.py:232 -   train_val_pipeline(): Max_time for training elapsed 24.00 hours, so stopping
2023-01-23 03:02:36,236:main_OGB_graph_regression.py:248 -   train_val_pipeline(): Val MAE: 0.1223
2023-01-23 03:02:36,238:main_OGB_graph_regression.py:249 -   train_val_pipeline(): Train MAE: 0.0943
2023-01-23 03:02:36,239:main_OGB_graph_regression.py:250 -   train_val_pipeline(): Best Train MAE Corresponding to Best Val MAE: inf
2023-01-23 03:02:36,240:main_OGB_graph_regression.py:251 -   train_val_pipeline(): Convergence Time (Epochs): 82.0000
2023-01-23 03:02:36,241:main_OGB_graph_regression.py:252 -   train_val_pipeline(): TOTAL TIME TAKEN: 87384.8692s
2023-01-23 03:02:36,275:main_OGB_graph_regression.py:253 -   train_val_pipeline(): AVG TIME PER EPOCH: 962.8901s
2023-01-23 03:02:36,282:main_OGB_graph_regression.py:257 -   train_val_pipeline(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 10, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b128-noedge', 'job_num': 10}
2023-01-23 03:02:36,377:main_OGB_graph_regression.py:258 -   train_val_pipeline(): train history: [tensor(0.0943)]
2023-01-23 03:02:36,377:main_OGB_graph_regression.py:260 -   train_val_pipeline(): val history: [tensor(0.1223)]
