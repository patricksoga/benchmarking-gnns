2023-01-20 21:27:08,035:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-01-20 21:27:08,035:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-20 21:37:13,329:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-20 21:37:40,493:ogbdata.py:332 -             __init__(): Time taken: 632.4582s
2023-01-20 21:37:40,493:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-20 21:37:40,493:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-20 21:37:40,493:ogbdata.py:348 -             __init__(): [I] Data load time: 632.4585s
2023-01-20 21:37:40,494:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'residual': True, 'readout': 'sum', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'wl_pos_enc': False, 'full_graph': False, 'lpe_layers': 1, 'pos_enc_dim': 10, 'lpe_n_heads': 4, 'edge_feat': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'partial_rw_pos_enc': False, 'lpe_dim': 8, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/SAGraphTransformer/no_pe/gt_ogb_b1024-baseline-noedge/10_DEBUG_0_35.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [35]}
2023-01-20 21:37:40,494:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 10, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [35], 'save_name': 'b128-noedge', 'job_num': 10}
2023-01-20 21:37:40,499:pe_layer.py:99 -             __init__(): no_pe
2023-01-20 21:37:40,499:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-20 21:37:40,499:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-20 21:37:40,583:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-20 21:37:40,585:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: SAGraphTransformer, 570215
2023-01-20 21:37:40,585:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 35 in [35]...
2023-01-20 21:37:40,585:pe_layer.py:99 -             __init__(): no_pe
2023-01-20 21:37:40,586:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-20 21:37:40,586:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-20 21:37:52,459:main_OGB_graph_regression.py:83 -   train_val_pipeline(): [!] Adding Laplacian decompositions for spectral attention.
2023-01-20 23:21:11,843:main_OGB_graph_regression.py:85 -   train_val_pipeline(): Time PE:6211.257648468018
2023-01-20 23:21:11,865:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-20 23:21:11,865:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-20 23:21:11,906:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-20 23:44:46,560:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.29562699794769287
2023-01-20 23:44:46,562:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1414.66s, LR: 0.00070, Train Loss: 0.2698, Train MAE: 0.2698,
                            Val Loss: 0.2970, Val MAE: 0.2956
2023-01-20 23:44:46,562:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-21 00:05:51,729:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.25703737139701843
2023-01-21 00:05:51,731:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.17s, LR: 0.00070, Train Loss: 0.1927, Train MAE: 0.1927,
                            Val Loss: 0.2591, Val MAE: 0.2570
2023-01-21 00:05:51,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-21 00:26:57,104:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.25239625573158264
2023-01-21 00:26:57,106:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.37s, LR: 0.00070, Train Loss: 0.1764, Train MAE: 0.1764,
                            Val Loss: 0.2547, Val MAE: 0.2524
2023-01-21 00:26:57,107:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-21 00:48:02,566:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2402372658252716
2023-01-21 00:48:02,568:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.46s, LR: 0.00070, Train Loss: 0.1660, Train MAE: 0.1660,
                            Val Loss: 0.2423, Val MAE: 0.2402
2023-01-21 00:48:02,569:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-21 01:09:07,974:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.40s, LR: 0.00070, Train Loss: 0.1587, Train MAE: 0.1587,
                            Val Loss: 0.3111, Val MAE: 0.3080
2023-01-21 01:09:07,975:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-21 01:30:13,573:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.20002762973308563
2023-01-21 01:30:13,575:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.60s, LR: 0.00070, Train Loss: 0.1534, Train MAE: 0.1534,
                            Val Loss: 0.2018, Val MAE: 0.2000
2023-01-21 01:30:13,576:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-21 01:51:18,916:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18263137340545654
2023-01-21 01:51:18,918:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.34s, LR: 0.00070, Train Loss: 0.1478, Train MAE: 0.1478,
                            Val Loss: 0.1842, Val MAE: 0.1826
2023-01-21 01:51:18,919:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-21 02:12:23,695:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.77s, LR: 0.00070, Train Loss: 0.1456, Train MAE: 0.1456,
                            Val Loss: 0.2241, Val MAE: 0.2220
2023-01-21 02:12:23,696:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-21 02:33:28,494:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.80s, LR: 0.00070, Train Loss: 0.1412, Train MAE: 0.1412,
                            Val Loss: 0.1955, Val MAE: 0.1939
2023-01-21 02:33:28,496:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-21 02:54:33,666:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.17s, LR: 0.00070, Train Loss: 0.1378, Train MAE: 0.1378,
                            Val Loss: 0.2080, Val MAE: 0.2065
2023-01-21 02:54:33,667:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-21 03:15:38,108:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1747937947511673
2023-01-21 03:15:38,110:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.44s, LR: 0.00070, Train Loss: 0.1357, Train MAE: 0.1357,
                            Val Loss: 0.1767, Val MAE: 0.1748
2023-01-21 03:15:38,111:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-21 03:36:42,857:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.74s, LR: 0.00070, Train Loss: 0.1337, Train MAE: 0.1337,
                            Val Loss: 0.1915, Val MAE: 0.1893
2023-01-21 03:36:42,858:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-21 03:57:47,432:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17166341841220856
2023-01-21 03:57:47,434:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.57s, LR: 0.00070, Train Loss: 0.1310, Train MAE: 0.1310,
                            Val Loss: 0.1735, Val MAE: 0.1717
2023-01-21 03:57:47,435:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-21 04:18:52,061:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.63s, LR: 0.00070, Train Loss: 0.1297, Train MAE: 0.1297,
                            Val Loss: 0.1900, Val MAE: 0.1881
2023-01-21 04:18:52,062:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-21 04:39:56,336:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.27s, LR: 0.00070, Train Loss: 0.1271, Train MAE: 0.1271,
                            Val Loss: 0.1868, Val MAE: 0.1850
2023-01-21 04:39:56,338:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-21 05:02:17,419:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1706058531999588
2023-01-21 05:02:17,421:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1341.08s, LR: 0.00070, Train Loss: 0.1250, Train MAE: 0.1250,
                            Val Loss: 0.1724, Val MAE: 0.1706
2023-01-21 05:02:17,422:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-21 05:23:22,018:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.59s, LR: 0.00070, Train Loss: 0.1239, Train MAE: 0.1239,
                            Val Loss: 0.1929, Val MAE: 0.1910
2023-01-21 05:23:22,019:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-21 05:44:26,752:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.73s, LR: 0.00070, Train Loss: 0.1228, Train MAE: 0.1228,
                            Val Loss: 0.1865, Val MAE: 0.1845
2023-01-21 05:44:26,753:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-21 06:05:30,837:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.15880481898784637
2023-01-21 06:05:30,839:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.08s, LR: 0.00070, Train Loss: 0.1214, Train MAE: 0.1214,
                            Val Loss: 0.1601, Val MAE: 0.1588
2023-01-21 06:05:30,840:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-21 06:26:34,697:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.15614081919193268
2023-01-21 06:26:34,699:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.86s, LR: 0.00070, Train Loss: 0.1198, Train MAE: 0.1198,
                            Val Loss: 0.1575, Val MAE: 0.1561
2023-01-21 06:26:34,700:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-21 06:47:38,940:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.24s, LR: 0.00070, Train Loss: 0.1184, Train MAE: 0.1184,
                            Val Loss: 0.1799, Val MAE: 0.1780
2023-01-21 06:47:38,941:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-21 07:08:43,384:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.44s, LR: 0.00070, Train Loss: 0.1174, Train MAE: 0.1174,
                            Val Loss: 0.1626, Val MAE: 0.1609
2023-01-21 07:08:43,386:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-21 07:29:47,891:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14216624200344086
2023-01-21 07:29:47,895:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1264.51s, LR: 0.00070, Train Loss: 0.1165, Train MAE: 0.1165,
                            Val Loss: 0.1433, Val MAE: 0.1422
2023-01-21 07:29:47,897:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-21 07:50:53,077:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1265.18s, LR: 0.00070, Train Loss: 0.1159, Train MAE: 0.1159,
                            Val Loss: 0.1710, Val MAE: 0.1691
2023-01-21 07:50:53,078:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-21 08:11:57,066:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.99s, LR: 0.00070, Train Loss: 0.1152, Train MAE: 0.1152,
                            Val Loss: 0.1644, Val MAE: 0.1624
2023-01-21 08:11:57,067:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-21 08:33:00,209:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.14s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1726, Val MAE: 0.1704
2023-01-21 08:33:00,210:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-21 08:54:03,495:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.28s, LR: 0.00070, Train Loss: 0.1137, Train MAE: 0.1137,
                            Val Loss: 0.1831, Val MAE: 0.1800
2023-01-21 08:54:03,497:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-21 09:15:06,885:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.39s, LR: 0.00070, Train Loss: 0.1129, Train MAE: 0.1129,
                            Val Loss: 0.1633, Val MAE: 0.1609
2023-01-21 09:15:06,886:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-21 09:36:10,104:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.22s, LR: 0.00070, Train Loss: 0.1124, Train MAE: 0.1124,
                            Val Loss: 0.1942, Val MAE: 0.1901
2023-01-21 09:36:10,105:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-21 09:57:13,614:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.51s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.2018, Val MAE: 0.1977
2023-01-21 09:57:13,615:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-21 10:18:16,496:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.88s, LR: 0.00070, Train Loss: 0.1112, Train MAE: 0.1112,
                            Val Loss: 0.1824, Val MAE: 0.1794
2023-01-21 10:18:16,498:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-21 10:39:19,697:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.20s, LR: 0.00070, Train Loss: 0.1105, Train MAE: 0.1105,
                            Val Loss: 0.1779, Val MAE: 0.1745
2023-01-21 10:39:19,699:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-21 11:00:22,818:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.12s, LR: 0.00070, Train Loss: 0.1102, Train MAE: 0.1102,
                            Val Loss: 0.1764, Val MAE: 0.1732
2023-01-21 11:00:22,820:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-21 11:21:26,194:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.37s, LR: 0.00070, Train Loss: 0.1096, Train MAE: 0.1096,
                            Val Loss: 0.1951, Val MAE: 0.1910
2023-01-21 11:21:26,195:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-21 11:42:29,192:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.00s, LR: 0.00035, Train Loss: 0.1005, Train MAE: 0.1005,
                            Val Loss: 0.1631, Val MAE: 0.1603
2023-01-21 11:42:29,193:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-21 12:04:51,865:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1342.67s, LR: 0.00035, Train Loss: 0.0992, Train MAE: 0.0992,
                            Val Loss: 0.1796, Val MAE: 0.1763
2023-01-21 12:04:51,867:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-21 12:25:55,108:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1263.24s, LR: 0.00035, Train Loss: 0.0983, Train MAE: 0.0983,
                            Val Loss: 0.1668, Val MAE: 0.1640
2023-01-21 12:25:55,109:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-01-21 12:46:57,262:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.15s, LR: 0.00035, Train Loss: 0.0980, Train MAE: 0.0980,
                            Val Loss: 0.1736, Val MAE: 0.1705
2023-01-21 12:46:57,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-01-21 13:07:59,940:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.68s, LR: 0.00035, Train Loss: 0.0973, Train MAE: 0.0973,
                            Val Loss: 0.1623, Val MAE: 0.1597
2023-01-21 13:07:59,941:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-01-21 13:29:02,419:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.48s, LR: 0.00035, Train Loss: 0.0970, Train MAE: 0.0970,
                            Val Loss: 0.1704, Val MAE: 0.1675
2023-01-21 13:29:02,420:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-01-21 13:50:04,784:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.36s, LR: 0.00035, Train Loss: 0.0964, Train MAE: 0.0964,
                            Val Loss: 0.1858, Val MAE: 0.1821
2023-01-21 13:50:04,786:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-01-21 14:11:07,408:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.62s, LR: 0.00035, Train Loss: 0.0962, Train MAE: 0.0962,
                            Val Loss: 0.1864, Val MAE: 0.1825
2023-01-21 14:11:07,410:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-01-21 14:32:10,152:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.74s, LR: 0.00035, Train Loss: 0.0959, Train MAE: 0.0959,
                            Val Loss: 0.1675, Val MAE: 0.1642
2023-01-21 14:32:10,153:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-01-21 14:53:12,421:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.27s, LR: 0.00035, Train Loss: 0.0957, Train MAE: 0.0957,
                            Val Loss: 0.1667, Val MAE: 0.1638
2023-01-21 14:53:12,423:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-01-21 15:14:15,052:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.63s, LR: 0.00035, Train Loss: 0.0954, Train MAE: 0.0954,
                            Val Loss: 0.1660, Val MAE: 0.1627
2023-01-21 15:14:15,053:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-01-21 15:35:17,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.72s, LR: 0.00017, Train Loss: 0.0905, Train MAE: 0.0905,
                            Val Loss: 0.1701, Val MAE: 0.1667
2023-01-21 15:35:17,775:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-01-21 15:56:20,024:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.25s, LR: 0.00017, Train Loss: 0.0897, Train MAE: 0.0897,
                            Val Loss: 0.1705, Val MAE: 0.1667
2023-01-21 15:56:20,025:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-01-21 16:17:22,302:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.28s, LR: 0.00017, Train Loss: 0.0894, Train MAE: 0.0894,
                            Val Loss: 0.1735, Val MAE: 0.1699
2023-01-21 16:17:22,303:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-01-21 16:38:24,644:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.34s, LR: 0.00017, Train Loss: 0.0890, Train MAE: 0.0890,
                            Val Loss: 0.1639, Val MAE: 0.1605
2023-01-21 16:38:24,644:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-01-21 16:59:26,663:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1262.02s, LR: 0.00017, Train Loss: 0.0887, Train MAE: 0.0887,
                            Val Loss: 0.1663, Val MAE: 0.1629
2023-01-21 16:59:26,664:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-01-21 17:20:28,483:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1261.82s, LR: 0.00017, Train Loss: 0.0885, Train MAE: 0.0885,
                            Val Loss: 0.1744, Val MAE: 0.1703
2023-01-21 17:20:28,484:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-01-21 17:41:29,871:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1261.38s, LR: 0.00017, Train Loss: 0.0883, Train MAE: 0.0883,
                            Val Loss: 0.1670, Val MAE: 0.1635
2023-01-21 17:41:29,872:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-01-21 18:02:31,382:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1261.51s, LR: 0.00017, Train Loss: 0.0881, Train MAE: 0.0881,
                            Val Loss: 0.1736, Val MAE: 0.1700
2023-01-21 18:02:31,383:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-01-21 18:23:32,294:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.91s, LR: 0.00017, Train Loss: 0.0879, Train MAE: 0.0879,
                            Val Loss: 0.1675, Val MAE: 0.1641
2023-01-21 18:23:32,295:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-01-21 18:45:50,961:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1338.67s, LR: 0.00017, Train Loss: 0.0879, Train MAE: 0.0879,
                            Val Loss: 0.1761, Val MAE: 0.1721
2023-01-21 18:45:50,962:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-01-21 19:06:51,905:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.94s, LR: 0.00017, Train Loss: 0.0876, Train MAE: 0.0876,
                            Val Loss: 0.1735, Val MAE: 0.1695
2023-01-21 19:06:51,906:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-01-21 19:27:52,588:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.68s, LR: 0.00009, Train Loss: 0.0849, Train MAE: 0.0849,
                            Val Loss: 0.1633, Val MAE: 0.1596
2023-01-21 19:27:52,589:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-01-21 19:48:52,710:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.12s, LR: 0.00009, Train Loss: 0.0844, Train MAE: 0.0844,
                            Val Loss: 0.1726, Val MAE: 0.1685
2023-01-21 19:48:52,711:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-01-21 20:09:52,616:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1259.90s, LR: 0.00009, Train Loss: 0.0842, Train MAE: 0.0842,
                            Val Loss: 0.1685, Val MAE: 0.1646
2023-01-21 20:09:52,617:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-01-21 20:30:52,718:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.10s, LR: 0.00009, Train Loss: 0.0842, Train MAE: 0.0842,
                            Val Loss: 0.1751, Val MAE: 0.1709
2023-01-21 20:30:52,719:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-01-21 20:51:53,244:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1260.52s, LR: 0.00009, Train Loss: 0.0839, Train MAE: 0.0839,
                            Val Loss: 0.1696, Val MAE: 0.1657
2023-01-21 20:51:53,245:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-01-21 21:12:52,488:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1259.24s, LR: 0.00009, Train Loss: 0.0838, Train MAE: 0.0838,
                            Val Loss: 0.1727, Val MAE: 0.1685
2023-01-21 21:12:52,489:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-01-21 21:33:52,260:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1259.77s, LR: 0.00009, Train Loss: 0.0837, Train MAE: 0.0837,
                            Val Loss: 0.1674, Val MAE: 0.1637
2023-01-21 21:33:52,262:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-01-21 21:54:51,420:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1259.16s, LR: 0.00009, Train Loss: 0.0835, Train MAE: 0.0835,
                            Val Loss: 0.1645, Val MAE: 0.1610
2023-01-21 21:54:51,421:main_OGB_graph_regression.py:231 -   train_val_pipeline(): -----------------------------------------------------------------------------------------
2023-01-21 21:54:51,422:main_OGB_graph_regression.py:232 -   train_val_pipeline(): Max_time for training elapsed 24.00 hours, so stopping
2023-01-21 22:06:09,537:main_OGB_graph_regression.py:248 -   train_val_pipeline(): Val MAE: 0.1610
2023-01-21 22:06:09,538:main_OGB_graph_regression.py:249 -   train_val_pipeline(): Train MAE: 0.0818
2023-01-21 22:06:09,539:main_OGB_graph_regression.py:250 -   train_val_pipeline(): Best Train MAE Corresponding to Best Val MAE: inf
2023-01-21 22:06:09,540:main_OGB_graph_regression.py:251 -   train_val_pipeline(): Convergence Time (Epochs): 63.0000
2023-01-21 22:06:09,541:main_OGB_graph_regression.py:252 -   train_val_pipeline(): TOTAL TIME TAKEN: 88108.9561s
2023-01-21 22:06:09,542:main_OGB_graph_regression.py:253 -   train_val_pipeline(): AVG TIME PER EPOCH: 1269.0542s
2023-01-21 22:06:09,544:main_OGB_graph_regression.py:257 -   train_val_pipeline(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 10, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [35], 'save_name': 'b128-noedge', 'job_num': 10}
2023-01-21 22:06:09,561:main_OGB_graph_regression.py:258 -   train_val_pipeline(): train history: [tensor(0.0818)]
2023-01-21 22:06:09,561:main_OGB_graph_regression.py:260 -   train_val_pipeline(): val history: [tensor(0.1610)]
