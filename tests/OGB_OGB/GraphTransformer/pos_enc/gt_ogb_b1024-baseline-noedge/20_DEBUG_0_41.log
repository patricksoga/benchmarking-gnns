2023-02-07 05:16:03,996:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-02-07 05:16:03,997:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:30:05,908:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:30:42,871:ogbdata.py:332 -             __init__(): Time taken: 878.8746s
2023-02-07 05:30:42,871:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:30:42,871:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:30:42,872:ogbdata.py:348 -             __init__(): [I] Data load time: 878.8750s
2023-02-07 05:30:42,872:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_41.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [41]}
2023-02-07 05:30:42,872:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:30:42,877:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:42,966:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:42,966:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:42,966:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:42,985:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:30:43,002:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:30:43,002:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 41 in [41]...
2023-02-07 05:30:43,002:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:43,002:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:43,003:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:43,003:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:48,831:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 07:31:25,974:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7242.972257614136
2023-02-07 07:31:26,067:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 07:31:26,067:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 07:31:26,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 07:52:18,355:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22745250165462494
2023-02-07 07:52:18,697:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1252.61s, LR: 0.00070, Train Loss: 0.3380, Train MAE: 0.3380,
                            Val Loss: 0.2277, Val MAE: 0.2275
2023-02-07 07:52:18,747:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 08:03:57,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.04s, LR: 0.00070, Train Loss: 0.1851, Train MAE: 0.1851,
                            Val Loss: 0.2285, Val MAE: 0.2281
2023-02-07 08:03:57,819:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 08:15:48,668:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.20425397157669067
2023-02-07 08:15:48,726:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.91s, LR: 0.00070, Train Loss: 0.1700, Train MAE: 0.1700,
                            Val Loss: 0.2046, Val MAE: 0.2043
2023-02-07 08:15:48,727:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 08:26:44,708:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1942027062177658
2023-02-07 08:26:44,731:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.00s, LR: 0.00070, Train Loss: 0.1627, Train MAE: 0.1627,
                            Val Loss: 0.1945, Val MAE: 0.1942
2023-02-07 08:26:44,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 08:37:46,163:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.43s, LR: 0.00070, Train Loss: 0.1577, Train MAE: 0.1577,
                            Val Loss: 0.1949, Val MAE: 0.1946
2023-02-07 08:37:46,185:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 08:48:46,915:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16881392896175385
2023-02-07 08:48:46,993:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.81s, LR: 0.00070, Train Loss: 0.1545, Train MAE: 0.1545,
                            Val Loss: 0.1691, Val MAE: 0.1688
2023-02-07 08:48:46,994:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 08:59:47,897:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1579914093017578
2023-02-07 08:59:47,900:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.90s, LR: 0.00070, Train Loss: 0.1511, Train MAE: 0.1511,
                            Val Loss: 0.1584, Val MAE: 0.1580
2023-02-07 08:59:47,900:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 09:11:50,863:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.96s, LR: 0.00070, Train Loss: 0.1484, Train MAE: 0.1484,
                            Val Loss: 0.1780, Val MAE: 0.1777
2023-02-07 09:11:50,894:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 09:22:51,160:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.27s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.2053, Val MAE: 0.2050
2023-02-07 09:22:51,200:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 09:33:52,250:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.05s, LR: 0.00070, Train Loss: 0.1452, Train MAE: 0.1452,
                            Val Loss: 0.1717, Val MAE: 0.1713
2023-02-07 09:33:52,264:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 09:44:52,410:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.15s, LR: 0.00070, Train Loss: 0.1442, Train MAE: 0.1442,
                            Val Loss: 0.1648, Val MAE: 0.1645
2023-02-07 09:44:52,412:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 09:55:52,459:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1487656533718109
2023-02-07 09:55:52,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.12s, LR: 0.00070, Train Loss: 0.1444, Train MAE: 0.1444,
                            Val Loss: 0.1491, Val MAE: 0.1488
2023-02-07 09:55:52,537:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 10:06:52,735:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.20s, LR: 0.00070, Train Loss: 0.1403, Train MAE: 0.1403,
                            Val Loss: 0.1612, Val MAE: 0.1609
2023-02-07 10:06:52,754:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 10:17:53,132:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.38s, LR: 0.00070, Train Loss: 0.1547, Train MAE: 0.1547,
                            Val Loss: 0.1614, Val MAE: 0.1611
2023-02-07 10:17:53,134:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 10:28:48,660:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.53s, LR: 0.00070, Train Loss: 0.1532, Train MAE: 0.1532,
                            Val Loss: 0.1731, Val MAE: 0.1727
2023-02-07 10:28:48,682:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-07 10:39:41,735:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 653.05s, LR: 0.00070, Train Loss: 0.1428, Train MAE: 0.1428,
                            Val Loss: 0.1651, Val MAE: 0.1648
2023-02-07 10:39:41,736:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-07 10:50:37,990:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.25s, LR: 0.00070, Train Loss: 0.1427, Train MAE: 0.1427,
                            Val Loss: 0.1578, Val MAE: 0.1575
2023-02-07 10:50:37,991:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-07 11:01:33,680:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.69s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1506, Val MAE: 0.1503
2023-02-07 11:01:33,692:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-07 11:12:29,318:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.63s, LR: 0.00070, Train Loss: 0.1417, Train MAE: 0.1417,
                            Val Loss: 0.1888, Val MAE: 0.1884
2023-02-07 11:12:29,319:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-07 11:23:25,210:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14843352138996124
2023-02-07 11:23:25,221:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.90s, LR: 0.00070, Train Loss: 0.1456, Train MAE: 0.1456,
                            Val Loss: 0.1487, Val MAE: 0.1484
2023-02-07 11:23:25,222:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-07 11:34:21,227:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1433950662612915
2023-02-07 11:34:21,244:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.02s, LR: 0.00070, Train Loss: 0.1363, Train MAE: 0.1363,
                            Val Loss: 0.1436, Val MAE: 0.1434
2023-02-07 11:34:21,245:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-07 11:45:17,295:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13962598145008087
2023-02-07 11:45:17,327:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.08s, LR: 0.00070, Train Loss: 0.1364, Train MAE: 0.1364,
                            Val Loss: 0.1398, Val MAE: 0.1396
2023-02-07 11:45:17,328:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-07 11:56:13,264:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.93s, LR: 0.00070, Train Loss: 0.1327, Train MAE: 0.1327,
                            Val Loss: 0.1459, Val MAE: 0.1457
2023-02-07 11:56:13,265:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-07 12:08:09,746:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13751888275146484
2023-02-07 12:08:09,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 716.51s, LR: 0.00070, Train Loss: 0.1302, Train MAE: 0.1302,
                            Val Loss: 0.1377, Val MAE: 0.1375
2023-02-07 12:08:09,773:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-07 12:19:11,197:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13397635519504547
2023-02-07 12:19:11,217:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.44s, LR: 0.00070, Train Loss: 0.1290, Train MAE: 0.1290,
                            Val Loss: 0.1342, Val MAE: 0.1340
2023-02-07 12:19:11,217:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-07 12:30:13,420:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.20s, LR: 0.00070, Train Loss: 0.1274, Train MAE: 0.1274,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-02-07 12:30:13,438:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-07 12:41:14,998:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13236258924007416
2023-02-07 12:41:15,033:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.59s, LR: 0.00070, Train Loss: 0.1260, Train MAE: 0.1260,
                            Val Loss: 0.1326, Val MAE: 0.1324
2023-02-07 12:41:15,033:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-07 12:52:16,553:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.52s, LR: 0.00070, Train Loss: 0.1252, Train MAE: 0.1252,
                            Val Loss: 0.1425, Val MAE: 0.1423
2023-02-07 12:52:16,554:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-07 13:03:17,985:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.43s, LR: 0.00070, Train Loss: 0.1245, Train MAE: 0.1245,
                            Val Loss: 0.1402, Val MAE: 0.1400
2023-02-07 13:03:17,998:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-07 13:14:19,487:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12985074520111084
2023-02-07 13:14:19,496:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.50s, LR: 0.00070, Train Loss: 0.1238, Train MAE: 0.1238,
                            Val Loss: 0.1301, Val MAE: 0.1299
2023-02-07 13:14:19,496:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-07 13:25:13,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 653.94s, LR: 0.00070, Train Loss: 0.1224, Train MAE: 0.1224,
                            Val Loss: 0.1354, Val MAE: 0.1351
2023-02-07 13:25:13,439:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-07 13:36:10,068:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12776879966259003
2023-02-07 13:36:10,079:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.64s, LR: 0.00070, Train Loss: 0.1221, Train MAE: 0.1221,
                            Val Loss: 0.1280, Val MAE: 0.1278
2023-02-07 13:36:10,079:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-07 13:47:06,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1212, Train MAE: 0.1212,
                            Val Loss: 0.1307, Val MAE: 0.1304
2023-02-07 13:47:06,455:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-07 13:58:02,730:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.28s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1316, Val MAE: 0.1314
2023-02-07 13:58:02,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-07 14:08:59,096:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1202, Train MAE: 0.1202,
                            Val Loss: 0.1572, Val MAE: 0.1570
2023-02-07 14:08:59,097:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-07 14:19:55,308:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12637346982955933
2023-02-07 14:19:55,324:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.23s, LR: 0.00070, Train Loss: 0.1195, Train MAE: 0.1195,
                            Val Loss: 0.1266, Val MAE: 0.1264
2023-02-07 14:19:55,324:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-07 14:30:51,982:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.66s, LR: 0.00070, Train Loss: 0.1192, Train MAE: 0.1192,
                            Val Loss: 0.1478, Val MAE: 0.1476
2023-02-07 14:30:51,995:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-07 14:41:48,801:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.81s, LR: 0.00070, Train Loss: 0.1183, Train MAE: 0.1183,
                            Val Loss: 0.1486, Val MAE: 0.1485
2023-02-07 14:41:48,810:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-07 14:52:45,439:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.63s, LR: 0.00070, Train Loss: 0.1182, Train MAE: 0.1182,
                            Val Loss: 0.1321, Val MAE: 0.1319
2023-02-07 14:52:45,440:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-07 15:03:41,946:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.51s, LR: 0.00070, Train Loss: 0.1175, Train MAE: 0.1175,
                            Val Loss: 0.1295, Val MAE: 0.1292
2023-02-07 15:03:41,963:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-07 15:15:39,969:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12334844470024109
2023-02-07 15:15:39,986:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.02s, LR: 0.00070, Train Loss: 0.1173, Train MAE: 0.1173,
                            Val Loss: 0.1236, Val MAE: 0.1233
2023-02-07 15:15:39,986:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-07 15:26:41,521:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.53s, LR: 0.00070, Train Loss: 0.1165, Train MAE: 0.1165,
                            Val Loss: 0.1260, Val MAE: 0.1258
2023-02-07 15:26:41,532:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-07 15:37:43,008:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.48s, LR: 0.00070, Train Loss: 0.1169, Train MAE: 0.1169,
                            Val Loss: 0.1304, Val MAE: 0.1302
2023-02-07 15:37:43,009:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-07 15:48:38,638:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.63s, LR: 0.00070, Train Loss: 0.1187, Train MAE: 0.1187,
                            Val Loss: 0.1262, Val MAE: 0.1260
2023-02-07 15:48:38,639:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-07 15:59:34,740:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.10s, LR: 0.00070, Train Loss: 0.1156, Train MAE: 0.1156,
                            Val Loss: 0.1347, Val MAE: 0.1344
2023-02-07 15:59:34,741:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-07 16:10:31,098:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1160, Train MAE: 0.1160,
                            Val Loss: 0.1244, Val MAE: 0.1242
2023-02-07 16:10:31,100:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-07 16:21:28,192:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 657.09s, LR: 0.00070, Train Loss: 0.1153, Train MAE: 0.1153,
                            Val Loss: 0.1305, Val MAE: 0.1302
2023-02-07 16:21:28,204:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-07 16:32:28,289:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.08s, LR: 0.00070, Train Loss: 0.1149, Train MAE: 0.1149,
                            Val Loss: 0.1279, Val MAE: 0.1277
2023-02-07 16:32:28,310:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-07 16:43:37,723:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.41s, LR: 0.00070, Train Loss: 0.1147, Train MAE: 0.1147,
                            Val Loss: 0.1362, Val MAE: 0.1361
2023-02-07 16:43:37,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-07 16:54:38,140:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.41s, LR: 0.00070, Train Loss: 0.1143, Train MAE: 0.1143,
                            Val Loss: 0.1243, Val MAE: 0.1240
2023-02-07 16:54:38,141:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-07 17:05:36,308:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 658.17s, LR: 0.00070, Train Loss: 0.1149, Train MAE: 0.1149,
                            Val Loss: 0.1282, Val MAE: 0.1279
2023-02-07 17:05:36,309:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-07 17:16:39,408:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12228923290967941
2023-02-07 17:16:39,410:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.10s, LR: 0.00070, Train Loss: 0.1148, Train MAE: 0.1148,
                            Val Loss: 0.1225, Val MAE: 0.1223
2023-02-07 17:16:39,411:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-07 17:27:41,976:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.56s, LR: 0.00070, Train Loss: 0.1128, Train MAE: 0.1128,
                            Val Loss: 0.1248, Val MAE: 0.1246
2023-02-07 17:27:41,977:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-07 17:38:42,430:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.45s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.1228, Val MAE: 0.1226
2023-02-07 17:38:42,431:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-07 17:49:43,449:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.02s, LR: 0.00070, Train Loss: 0.1117, Train MAE: 0.1117,
                            Val Loss: 0.1259, Val MAE: 0.1257
2023-02-07 17:49:43,450:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-07 18:00:41,624:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12151527404785156
2023-02-07 18:00:41,626:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 658.18s, LR: 0.00070, Train Loss: 0.1110, Train MAE: 0.1110,
                            Val Loss: 0.1218, Val MAE: 0.1215
2023-02-07 18:00:41,627:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-07 18:11:38,753:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 657.13s, LR: 0.00070, Train Loss: 0.1105, Train MAE: 0.1105,
                            Val Loss: 0.1240, Val MAE: 0.1238
2023-02-07 18:11:38,755:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-07 18:23:36,100:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1195291131734848
2023-02-07 18:23:36,102:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 717.35s, LR: 0.00070, Train Loss: 0.1107, Train MAE: 0.1107,
                            Val Loss: 0.1198, Val MAE: 0.1195
2023-02-07 18:23:36,103:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-07 18:34:34,382:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 658.27s, LR: 0.00070, Train Loss: 0.1097, Train MAE: 0.1097,
                            Val Loss: 0.1251, Val MAE: 0.1248
2023-02-07 18:34:34,454:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-07 18:54:59,086:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 18:54:59,088:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 18:56:11,557:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 18:56:11,559:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 18:56:33,321:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 18:56:33,322:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 18:57:04,943:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 18:57:04,944:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:08:39,843:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 19:09:10,102:ogbdata.py:332 -             __init__(): Time taken: 725.1579s
2023-02-07 19:09:10,102:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 19:09:10,103:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 19:09:10,103:ogbdata.py:348 -             __init__(): [I] Data load time: 725.1587s
2023-02-07 19:09:10,103:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_41.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [41]}
2023-02-07 19:09:10,103:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 19:09:10,124:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:09:10,180:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:09:10,180:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:09:10,180:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:09:10,231:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 19:09:10,258:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 19:09:10,258:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 41 in [41]...
2023-02-07 19:09:10,259:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:09:10,259:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:09:10,259:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:09:10,259:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:09:13,409:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 21:03:21,638:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 6851.379195690155
2023-02-07 21:03:22,045:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 21:03:22,045:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 21:03:22,128:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 21:17:00,801:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.27386075258255005
2023-02-07 21:17:01,030:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 818.89s, LR: 0.00070, Train Loss: 0.3367, Train MAE: 0.3367,
                            Val Loss: 0.2744, Val MAE: 0.2739
2023-02-07 21:17:01,045:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 21:28:04,112:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2151162028312683
2023-02-07 21:28:04,139:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.09s, LR: 0.00070, Train Loss: 0.1963, Train MAE: 0.1963,
                            Val Loss: 0.2156, Val MAE: 0.2151
2023-02-07 21:28:04,140:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 21:39:12,863:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.21450850367546082
2023-02-07 21:39:12,888:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.75s, LR: 0.00070, Train Loss: 0.1763, Train MAE: 0.1763,
                            Val Loss: 0.2149, Val MAE: 0.2145
2023-02-07 21:39:12,889:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 21:50:22,253:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18771615624427795
2023-02-07 21:50:22,289:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.40s, LR: 0.00070, Train Loss: 0.1651, Train MAE: 0.1651,
                            Val Loss: 0.1881, Val MAE: 0.1877
2023-02-07 21:50:22,290:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 22:01:31,083:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18034498393535614
2023-02-07 22:01:31,111:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.82s, LR: 0.00070, Train Loss: 0.1592, Train MAE: 0.1592,
                            Val Loss: 0.1808, Val MAE: 0.1803
2023-02-07 22:01:31,112:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 22:12:39,328:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.15814825892448425
2023-02-07 22:12:39,352:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.24s, LR: 0.00070, Train Loss: 0.1544, Train MAE: 0.1544,
                            Val Loss: 0.1586, Val MAE: 0.1581
2023-02-07 22:12:39,353:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 22:23:49,525:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.17s, LR: 0.00070, Train Loss: 0.1496, Train MAE: 0.1496,
                            Val Loss: 0.1804, Val MAE: 0.1800
2023-02-07 22:23:49,550:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 22:35:57,163:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.15115034580230713
2023-02-07 22:35:57,174:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 727.62s, LR: 0.00070, Train Loss: 0.1467, Train MAE: 0.1467,
                            Val Loss: 0.1516, Val MAE: 0.1512
2023-02-07 22:35:57,175:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 22:47:10,135:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.96s, LR: 0.00070, Train Loss: 0.1445, Train MAE: 0.1445,
                            Val Loss: 0.1547, Val MAE: 0.1543
2023-02-07 22:47:10,147:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 22:58:19,938:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.79s, LR: 0.00070, Train Loss: 0.1420, Train MAE: 0.1420,
                            Val Loss: 0.1660, Val MAE: 0.1656
2023-02-07 22:58:19,963:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 23:09:26,152:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.15032431483268738
2023-02-07 23:09:26,154:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.19s, LR: 0.00070, Train Loss: 0.1393, Train MAE: 0.1393,
                            Val Loss: 0.1507, Val MAE: 0.1503
2023-02-07 23:09:26,155:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 23:20:30,186:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.03s, LR: 0.00070, Train Loss: 0.1382, Train MAE: 0.1382,
                            Val Loss: 0.1886, Val MAE: 0.1882
2023-02-07 23:20:30,211:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 23:31:35,270:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.06s, LR: 0.00070, Train Loss: 0.1366, Train MAE: 0.1366,
                            Val Loss: 0.1537, Val MAE: 0.1533
2023-02-07 23:31:35,271:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 23:42:44,295:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.02s, LR: 0.00070, Train Loss: 0.1347, Train MAE: 0.1347,
                            Val Loss: 0.1695, Val MAE: 0.1692
2023-02-07 23:42:44,310:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 23:53:51,730:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1394113004207611
2023-02-07 23:53:51,752:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.44s, LR: 0.00070, Train Loss: 0.1339, Train MAE: 0.1339,
                            Val Loss: 0.1398, Val MAE: 0.1394
2023-02-07 23:53:51,753:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-08 00:04:53,550:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.80s, LR: 0.00070, Train Loss: 0.1331, Train MAE: 0.1331,
                            Val Loss: 0.1475, Val MAE: 0.1471
2023-02-08 00:04:53,558:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-08 00:15:59,847:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.29s, LR: 0.00070, Train Loss: 0.1311, Train MAE: 0.1311,
                            Val Loss: 0.1748, Val MAE: 0.1745
2023-02-08 00:15:59,858:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-08 00:27:03,153:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13623438775539398
2023-02-08 00:27:03,174:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.32s, LR: 0.00070, Train Loss: 0.1298, Train MAE: 0.1298,
                            Val Loss: 0.1366, Val MAE: 0.1362
2023-02-08 00:27:03,175:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-08 00:38:11,189:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.01s, LR: 0.00070, Train Loss: 0.1288, Train MAE: 0.1288,
                            Val Loss: 0.1530, Val MAE: 0.1527
2023-02-08 00:38:11,190:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-08 00:49:14,823:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.63s, LR: 0.00070, Train Loss: 0.1283, Train MAE: 0.1283,
                            Val Loss: 0.1511, Val MAE: 0.1508
2023-02-08 00:49:14,847:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-08 01:00:17,782:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.93s, LR: 0.00070, Train Loss: 0.1280, Train MAE: 0.1280,
                            Val Loss: 0.1482, Val MAE: 0.1479
2023-02-08 01:00:17,796:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-08 01:11:21,885:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1327028125524521
2023-02-08 01:11:21,888:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.09s, LR: 0.00070, Train Loss: 0.1266, Train MAE: 0.1266,
                            Val Loss: 0.1330, Val MAE: 0.1327
2023-02-08 01:11:21,888:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-08 01:22:30,627:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.74s, LR: 0.00070, Train Loss: 0.1248, Train MAE: 0.1248,
                            Val Loss: 0.1412, Val MAE: 0.1409
2023-02-08 01:22:30,653:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-08 01:34:32,950:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13130027055740356
2023-02-08 01:34:32,951:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.30s, LR: 0.00070, Train Loss: 0.1249, Train MAE: 0.1249,
                            Val Loss: 0.1316, Val MAE: 0.1313
2023-02-08 01:34:32,952:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-08 01:45:39,001:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.05s, LR: 0.00070, Train Loss: 0.1241, Train MAE: 0.1241,
                            Val Loss: 0.1394, Val MAE: 0.1392
2023-02-08 01:45:39,025:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-08 01:56:42,907:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.88s, LR: 0.00070, Train Loss: 0.1232, Train MAE: 0.1232,
                            Val Loss: 0.1344, Val MAE: 0.1342
2023-02-08 01:56:42,909:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-08 02:07:47,170:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.26s, LR: 0.00070, Train Loss: 0.1216, Train MAE: 0.1216,
                            Val Loss: 0.1363, Val MAE: 0.1360
2023-02-08 02:07:47,172:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-08 02:18:53,790:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.62s, LR: 0.00070, Train Loss: 0.1217, Train MAE: 0.1217,
                            Val Loss: 0.1319, Val MAE: 0.1317
2023-02-08 02:18:53,806:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-08 02:29:55,321:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1294236034154892
2023-02-08 02:29:55,323:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.52s, LR: 0.00070, Train Loss: 0.1211, Train MAE: 0.1211,
                            Val Loss: 0.1297, Val MAE: 0.1294
2023-02-08 02:29:55,324:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-08 02:41:00,378:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12764686346054077
2023-02-08 02:41:00,380:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.06s, LR: 0.00070, Train Loss: 0.1211, Train MAE: 0.1211,
                            Val Loss: 0.1279, Val MAE: 0.1276
2023-02-08 02:41:00,380:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-08 02:52:05,641:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.26s, LR: 0.00070, Train Loss: 0.1199, Train MAE: 0.1199,
                            Val Loss: 0.1321, Val MAE: 0.1318
2023-02-08 02:52:05,662:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-08 03:03:11,553:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.89s, LR: 0.00070, Train Loss: 0.1196, Train MAE: 0.1196,
                            Val Loss: 0.1351, Val MAE: 0.1348
2023-02-08 03:03:11,554:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-08 03:14:17,335:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.78s, LR: 0.00070, Train Loss: 0.1188, Train MAE: 0.1188,
                            Val Loss: 0.1283, Val MAE: 0.1280
2023-02-08 03:14:17,360:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-08 03:25:22,920:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.56s, LR: 0.00070, Train Loss: 0.1176, Train MAE: 0.1176,
                            Val Loss: 0.1362, Val MAE: 0.1359
2023-02-08 03:25:22,922:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-08 03:36:23,523:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.60s, LR: 0.00070, Train Loss: 0.1179, Train MAE: 0.1179,
                            Val Loss: 0.1299, Val MAE: 0.1297
2023-02-08 03:36:23,524:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-08 03:47:23,637:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.11s, LR: 0.00070, Train Loss: 0.1172, Train MAE: 0.1172,
                            Val Loss: 0.1305, Val MAE: 0.1303
2023-02-08 03:47:23,658:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-08 03:58:29,687:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.03s, LR: 0.00070, Train Loss: 0.1169, Train MAE: 0.1169,
                            Val Loss: 0.1290, Val MAE: 0.1287
2023-02-08 03:58:29,688:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-08 04:09:32,191:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.50s, LR: 0.00070, Train Loss: 0.1167, Train MAE: 0.1167,
                            Val Loss: 0.1298, Val MAE: 0.1295
2023-02-08 04:09:32,192:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-08 04:20:33,598:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.40s, LR: 0.00070, Train Loss: 0.1158, Train MAE: 0.1158,
                            Val Loss: 0.1285, Val MAE: 0.1283
2023-02-08 04:20:33,599:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-08 04:31:35,349:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12484096735715866
2023-02-08 04:31:35,351:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.75s, LR: 0.00070, Train Loss: 0.1160, Train MAE: 0.1160,
                            Val Loss: 0.1251, Val MAE: 0.1248
2023-02-08 04:31:35,352:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-08 04:43:36,552:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.20s, LR: 0.00070, Train Loss: 0.1151, Train MAE: 0.1151,
                            Val Loss: 0.1269, Val MAE: 0.1267
2023-02-08 04:43:36,553:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-08 04:54:37,859:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.30s, LR: 0.00070, Train Loss: 0.1147, Train MAE: 0.1147,
                            Val Loss: 0.1255, Val MAE: 0.1253
2023-02-08 04:54:37,860:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-08 05:05:45,591:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12426769733428955
2023-02-08 05:05:45,592:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.73s, LR: 0.00070, Train Loss: 0.1145, Train MAE: 0.1145,
                            Val Loss: 0.1245, Val MAE: 0.1243
2023-02-08 05:05:45,593:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-08 05:16:52,933:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.34s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1307, Val MAE: 0.1305
2023-02-08 05:16:52,934:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-08 05:28:00,125:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.19s, LR: 0.00070, Train Loss: 0.1136, Train MAE: 0.1136,
                            Val Loss: 0.1247, Val MAE: 0.1244
2023-02-08 05:28:00,127:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-08 05:39:07,895:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1230151504278183
2023-02-08 05:39:07,897:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.77s, LR: 0.00070, Train Loss: 0.1126, Train MAE: 0.1126,
                            Val Loss: 0.1232, Val MAE: 0.1230
2023-02-08 05:39:07,898:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-08 05:50:17,977:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.08s, LR: 0.00070, Train Loss: 0.1124, Train MAE: 0.1124,
                            Val Loss: 0.1292, Val MAE: 0.1290
2023-02-08 05:50:17,978:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-08 06:01:23,944:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12251252681016922
2023-02-08 06:01:23,946:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.97s, LR: 0.00070, Train Loss: 0.1123, Train MAE: 0.1123,
                            Val Loss: 0.1227, Val MAE: 0.1225
2023-02-08 06:01:23,946:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-08 06:12:32,829:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.88s, LR: 0.00070, Train Loss: 0.1119, Train MAE: 0.1119,
                            Val Loss: 0.1252, Val MAE: 0.1250
2023-02-08 06:12:32,830:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-08 06:23:39,810:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.98s, LR: 0.00070, Train Loss: 0.1116, Train MAE: 0.1116,
                            Val Loss: 0.1246, Val MAE: 0.1244
2023-02-08 06:23:39,811:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-08 06:34:43,864:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.05s, LR: 0.00070, Train Loss: 0.1117, Train MAE: 0.1117,
                            Val Loss: 0.1293, Val MAE: 0.1291
2023-02-08 06:34:43,866:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-08 06:45:51,836:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12154936045408249
2023-02-08 06:45:51,838:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 667.97s, LR: 0.00070, Train Loss: 0.1109, Train MAE: 0.1109,
                            Val Loss: 0.1218, Val MAE: 0.1215
2023-02-08 06:45:51,839:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-08 06:56:54,666:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.83s, LR: 0.00070, Train Loss: 0.1105, Train MAE: 0.1105,
                            Val Loss: 0.1232, Val MAE: 0.1230
2023-02-08 06:56:54,667:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-08 07:07:59,853:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.18s, LR: 0.00070, Train Loss: 0.1100, Train MAE: 0.1100,
                            Val Loss: 0.1220, Val MAE: 0.1218
2023-02-08 07:07:59,854:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-08 07:19:04,533:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.68s, LR: 0.00070, Train Loss: 0.1099, Train MAE: 0.1099,
                            Val Loss: 0.1230, Val MAE: 0.1228
2023-02-08 07:19:04,534:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-08 07:30:08,866:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12102659046649933
2023-02-08 07:30:08,868:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.33s, LR: 0.00070, Train Loss: 0.1090, Train MAE: 0.1090,
                            Val Loss: 0.1213, Val MAE: 0.1210
2023-02-08 07:30:08,869:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-08 07:41:14,696:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.83s, LR: 0.00070, Train Loss: 0.1092, Train MAE: 0.1092,
                            Val Loss: 0.1290, Val MAE: 0.1288
2023-02-08 07:41:14,697:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-08 07:53:16,079:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11973316222429276
2023-02-08 07:53:16,106:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.41s, LR: 0.00070, Train Loss: 0.1090, Train MAE: 0.1090,
                            Val Loss: 0.1200, Val MAE: 0.1197
2023-02-08 07:53:16,107:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-08 08:04:22,222:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.11s, LR: 0.00070, Train Loss: 0.1089, Train MAE: 0.1089,
                            Val Loss: 0.1275, Val MAE: 0.1273
2023-02-08 08:04:22,223:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-08 08:15:31,234:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11848030239343643
2023-02-08 08:15:31,236:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.01s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1187, Val MAE: 0.1185
2023-02-08 08:15:31,236:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-02-08 08:26:37,417:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.18s, LR: 0.00070, Train Loss: 0.1084, Train MAE: 0.1084,
                            Val Loss: 0.1226, Val MAE: 0.1224
2023-02-08 08:26:37,419:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-02-08 08:37:40,960:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.54s, LR: 0.00070, Train Loss: 0.1077, Train MAE: 0.1077,
                            Val Loss: 0.1233, Val MAE: 0.1231
2023-02-08 08:37:40,961:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-02-08 08:48:46,092:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.13s, LR: 0.00070, Train Loss: 0.1078, Train MAE: 0.1078,
                            Val Loss: 0.1240, Val MAE: 0.1237
2023-02-08 08:48:46,093:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-02-08 08:59:50,164:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.07s, LR: 0.00070, Train Loss: 0.1069, Train MAE: 0.1069,
                            Val Loss: 0.1241, Val MAE: 0.1239
2023-02-08 08:59:50,165:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-02-08 09:10:54,423:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.26s, LR: 0.00070, Train Loss: 0.1070, Train MAE: 0.1070,
                            Val Loss: 0.1236, Val MAE: 0.1233
2023-02-08 09:10:54,424:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-02-08 09:21:58,934:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 664.51s, LR: 0.00070, Train Loss: 0.1066, Train MAE: 0.1066,
                            Val Loss: 0.1192, Val MAE: 0.1190
2023-02-08 09:21:58,935:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-02-08 09:33:02,844:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.91s, LR: 0.00070, Train Loss: 0.1066, Train MAE: 0.1066,
                            Val Loss: 0.1215, Val MAE: 0.1213
2023-02-08 09:33:02,844:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-02-08 09:44:06,102:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.26s, LR: 0.00070, Train Loss: 0.1059, Train MAE: 0.1059,
                            Val Loss: 0.1193, Val MAE: 0.1191
2023-02-08 09:44:06,103:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 69/1000
2023-02-08 09:55:09,718:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.61s, LR: 0.00070, Train Loss: 0.1060, Train MAE: 0.1060,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-08 09:55:09,719:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 70/1000
2023-02-08 10:06:23,562:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.84s, LR: 0.00070, Train Loss: 0.1056, Train MAE: 0.1056,
                            Val Loss: 0.1191, Val MAE: 0.1188
2023-02-08 10:06:23,564:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 71/1000
2023-02-08 10:17:31,621:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.06s, LR: 0.00070, Train Loss: 0.1061, Train MAE: 0.1061,
                            Val Loss: 0.1202, Val MAE: 0.1200
2023-02-08 10:17:31,622:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 72/1000
2023-02-08 10:28:42,381:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.76s, LR: 0.00070, Train Loss: 0.1054, Train MAE: 0.1054,
                            Val Loss: 0.1192, Val MAE: 0.1190
2023-02-08 10:28:42,382:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 73/1000
2023-02-08 10:39:45,789:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 663.41s, LR: 0.00070, Train Loss: 0.1054, Train MAE: 0.1054,
                            Val Loss: 0.1191, Val MAE: 0.1189
2023-02-08 10:39:45,791:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 74/1000
2023-02-08 10:50:57,648:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.86s, LR: 0.00070, Train Loss: 0.1053, Train MAE: 0.1053,
                            Val Loss: 0.1213, Val MAE: 0.1211
2023-02-08 10:50:57,649:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 75/1000
2023-02-08 11:03:11,018:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11674182116985321
2023-02-08 11:03:11,020:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 733.37s, LR: 0.00070, Train Loss: 0.1054, Train MAE: 0.1054,
                            Val Loss: 0.1170, Val MAE: 0.1167
2023-02-08 11:03:11,020:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 76/1000
2023-02-08 11:14:23,894:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.87s, LR: 0.00070, Train Loss: 0.1055, Train MAE: 0.1055,
                            Val Loss: 0.1182, Val MAE: 0.1180
2023-02-08 11:14:23,895:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 77/1000
2023-02-08 11:25:37,688:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.79s, LR: 0.00070, Train Loss: 0.1048, Train MAE: 0.1048,
                            Val Loss: 0.1193, Val MAE: 0.1190
2023-02-08 11:25:37,688:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 78/1000
2023-02-08 11:36:51,129:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.44s, LR: 0.00070, Train Loss: 0.1047, Train MAE: 0.1047,
                            Val Loss: 0.1209, Val MAE: 0.1207
2023-02-08 11:36:51,131:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 79/1000
2023-02-08 11:48:05,043:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.91s, LR: 0.00070, Train Loss: 0.1048, Train MAE: 0.1048,
                            Val Loss: 0.1208, Val MAE: 0.1206
2023-02-08 11:48:05,044:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 80/1000
2023-02-08 11:59:19,147:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.10s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1202, Val MAE: 0.1199
2023-02-08 11:59:19,148:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 81/1000
2023-02-08 12:10:32,951:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.80s, LR: 0.00070, Train Loss: 0.1042, Train MAE: 0.1042,
                            Val Loss: 0.1199, Val MAE: 0.1197
2023-02-08 12:10:32,952:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 82/1000
2023-02-08 12:21:47,649:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.70s, LR: 0.00070, Train Loss: 0.1040, Train MAE: 0.1040,
                            Val Loss: 0.1202, Val MAE: 0.1200
2023-02-08 12:21:47,650:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 83/1000
2023-02-08 12:33:01,250:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.60s, LR: 0.00070, Train Loss: 0.1038, Train MAE: 0.1038,
                            Val Loss: 0.1193, Val MAE: 0.1190
2023-02-08 12:33:01,251:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 84/1000
2023-02-08 12:44:14,002:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.75s, LR: 0.00070, Train Loss: 0.1037, Train MAE: 0.1037,
                            Val Loss: 0.1193, Val MAE: 0.1190
2023-02-08 12:44:14,003:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 85/1000
2023-02-08 12:55:26,581:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.58s, LR: 0.00070, Train Loss: 0.1035, Train MAE: 0.1035,
                            Val Loss: 0.1194, Val MAE: 0.1192
2023-02-08 12:55:26,582:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 86/1000
2023-02-08 13:06:38,221:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.64s, LR: 0.00070, Train Loss: 0.1034, Train MAE: 0.1034,
                            Val Loss: 0.1190, Val MAE: 0.1188
2023-02-08 13:06:38,222:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 87/1000
2023-02-08 13:17:52,320:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.10s, LR: 0.00070, Train Loss: 0.1031, Train MAE: 0.1031,
                            Val Loss: 0.1224, Val MAE: 0.1221
2023-02-08 13:17:52,321:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 88/1000
2023-02-08 13:29:06,393:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.07s, LR: 0.00070, Train Loss: 0.1031, Train MAE: 0.1031,
                            Val Loss: 0.1186, Val MAE: 0.1183
2023-02-08 13:29:06,394:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 89/1000
2023-02-08 13:40:20,405:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.01s, LR: 0.00070, Train Loss: 0.1032, Train MAE: 0.1032,
                            Val Loss: 0.1185, Val MAE: 0.1182
2023-02-08 13:40:20,405:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 90/1000
2023-02-08 13:51:33,650:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 673.24s, LR: 0.00070, Train Loss: 0.1026, Train MAE: 0.1026,
                            Val Loss: 0.1191, Val MAE: 0.1189
2023-02-08 13:51:33,652:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 91/1000
2023-02-08 14:02:48,263:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 674.61s, LR: 0.00070, Train Loss: 0.1030, Train MAE: 0.1030,
                            Val Loss: 0.1183, Val MAE: 0.1180
2023-02-08 14:02:48,264:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 92/1000
2023-02-08 14:15:00,584:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11241983622312546
2023-02-08 14:15:00,586:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 732.32s, LR: 0.00035, Train Loss: 0.0960, Train MAE: 0.0960,
                            Val Loss: 0.1127, Val MAE: 0.1124
2023-02-08 14:15:00,587:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 93/1000
2023-02-08 14:26:11,356:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11130733042955399
2023-02-08 14:26:11,358:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.77s, LR: 0.00035, Train Loss: 0.0954, Train MAE: 0.0954,
                            Val Loss: 0.1116, Val MAE: 0.1113
2023-02-08 14:26:11,359:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 94/1000
2023-02-08 14:37:23,460:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.10s, LR: 0.00035, Train Loss: 0.0946, Train MAE: 0.0946,
                            Val Loss: 0.1123, Val MAE: 0.1121
2023-02-08 14:37:23,461:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 95/1000
2023-02-08 14:48:34,597:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11114586144685745
2023-02-08 14:48:34,600:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.14s, LR: 0.00035, Train Loss: 0.0946, Train MAE: 0.0946,
                            Val Loss: 0.1114, Val MAE: 0.1111
2023-02-08 14:48:34,601:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 96/1000
2023-02-08 14:59:45,502:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.90s, LR: 0.00035, Train Loss: 0.0943, Train MAE: 0.0943,
                            Val Loss: 0.1118, Val MAE: 0.1116
2023-02-08 14:59:45,503:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 97/1000
2023-02-08 15:10:57,554:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 672.05s, LR: 0.00035, Train Loss: 0.0940, Train MAE: 0.0940,
                            Val Loss: 0.1116, Val MAE: 0.1113
2023-02-08 15:10:57,555:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 98/1000
2023-02-08 15:22:07,691:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11102965474128723
2023-02-08 15:22:07,693:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 670.14s, LR: 0.00035, Train Loss: 0.0939, Train MAE: 0.0939,
                            Val Loss: 0.1113, Val MAE: 0.1110
2023-02-08 15:22:07,694:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 99/1000
2023-02-08 15:33:12,901:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 665.21s, LR: 0.00035, Train Loss: 0.0935, Train MAE: 0.0935,
                            Val Loss: 0.1131, Val MAE: 0.1129
2023-02-08 15:33:12,903:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 100/1000
2023-02-08 15:44:23,970:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.07s, LR: 0.00035, Train Loss: 0.0935, Train MAE: 0.0935,
                            Val Loss: 0.1130, Val MAE: 0.1127
2023-02-08 15:44:23,971:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 101/1000
2023-02-08 15:55:30,643:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11046165227890015
2023-02-08 15:55:30,645:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 666.67s, LR: 0.00035, Train Loss: 0.0934, Train MAE: 0.0934,
                            Val Loss: 0.1107, Val MAE: 0.1105
2023-02-08 15:55:30,646:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 102/1000
2023-02-08 16:06:39,431:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.78s, LR: 0.00035, Train Loss: 0.0932, Train MAE: 0.0932,
                            Val Loss: 0.1111, Val MAE: 0.1108
2023-02-08 16:06:39,432:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 103/1000
2023-02-08 16:17:51,136:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.70s, LR: 0.00035, Train Loss: 0.0931, Train MAE: 0.0931,
                            Val Loss: 0.1123, Val MAE: 0.1121
2023-02-08 16:17:51,137:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 104/1000
2023-02-08 16:28:59,275:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 668.14s, LR: 0.00035, Train Loss: 0.0929, Train MAE: 0.0929,
                            Val Loss: 0.1119, Val MAE: 0.1117
2023-02-08 16:28:59,276:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 105/1000
2023-02-08 16:40:08,617:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 669.34s, LR: 0.00035, Train Loss: 0.0928, Train MAE: 0.0928,
                            Val Loss: 0.1111, Val MAE: 0.1109
2023-02-08 16:40:08,618:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 106/1000
2023-02-08 16:51:19,876:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.26s, LR: 0.00035, Train Loss: 0.0928, Train MAE: 0.0928,
                            Val Loss: 0.1110, Val MAE: 0.1108
2023-02-08 16:51:19,877:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 107/1000
