2023-02-07 05:16:03,996:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-02-07 05:16:03,997:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:30:05,908:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:30:42,871:ogbdata.py:332 -             __init__(): Time taken: 878.8746s
2023-02-07 05:30:42,871:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:30:42,871:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:30:42,872:ogbdata.py:348 -             __init__(): [I] Data load time: 878.8750s
2023-02-07 05:30:42,872:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_41.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [41]}
2023-02-07 05:30:42,872:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:30:42,877:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:42,966:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:42,966:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:42,966:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:42,985:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:30:43,002:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:30:43,002:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 41 in [41]...
2023-02-07 05:30:43,002:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:43,002:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:43,003:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:43,003:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:48,831:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 07:31:25,974:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7242.972257614136
2023-02-07 07:31:26,067:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 07:31:26,067:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 07:31:26,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 07:52:18,355:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22745250165462494
2023-02-07 07:52:18,697:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 1252.61s, LR: 0.00070, Train Loss: 0.3380, Train MAE: 0.3380,
                            Val Loss: 0.2277, Val MAE: 0.2275
2023-02-07 07:52:18,747:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 08:03:57,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.04s, LR: 0.00070, Train Loss: 0.1851, Train MAE: 0.1851,
                            Val Loss: 0.2285, Val MAE: 0.2281
2023-02-07 08:03:57,819:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 08:15:48,668:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.20425397157669067
2023-02-07 08:15:48,726:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.91s, LR: 0.00070, Train Loss: 0.1700, Train MAE: 0.1700,
                            Val Loss: 0.2046, Val MAE: 0.2043
2023-02-07 08:15:48,727:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 08:26:44,708:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1942027062177658
2023-02-07 08:26:44,731:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.00s, LR: 0.00070, Train Loss: 0.1627, Train MAE: 0.1627,
                            Val Loss: 0.1945, Val MAE: 0.1942
2023-02-07 08:26:44,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 08:37:46,163:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.43s, LR: 0.00070, Train Loss: 0.1577, Train MAE: 0.1577,
                            Val Loss: 0.1949, Val MAE: 0.1946
2023-02-07 08:37:46,185:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 08:48:46,915:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16881392896175385
2023-02-07 08:48:46,993:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.81s, LR: 0.00070, Train Loss: 0.1545, Train MAE: 0.1545,
                            Val Loss: 0.1691, Val MAE: 0.1688
2023-02-07 08:48:46,994:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 08:59:47,897:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1579914093017578
2023-02-07 08:59:47,900:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.90s, LR: 0.00070, Train Loss: 0.1511, Train MAE: 0.1511,
                            Val Loss: 0.1584, Val MAE: 0.1580
2023-02-07 08:59:47,900:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 09:11:50,863:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.96s, LR: 0.00070, Train Loss: 0.1484, Train MAE: 0.1484,
                            Val Loss: 0.1780, Val MAE: 0.1777
2023-02-07 09:11:50,894:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 09:22:51,160:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.27s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.2053, Val MAE: 0.2050
2023-02-07 09:22:51,200:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 09:33:52,250:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.05s, LR: 0.00070, Train Loss: 0.1452, Train MAE: 0.1452,
                            Val Loss: 0.1717, Val MAE: 0.1713
2023-02-07 09:33:52,264:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 09:44:52,410:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.15s, LR: 0.00070, Train Loss: 0.1442, Train MAE: 0.1442,
                            Val Loss: 0.1648, Val MAE: 0.1645
2023-02-07 09:44:52,412:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 09:55:52,459:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1487656533718109
2023-02-07 09:55:52,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.12s, LR: 0.00070, Train Loss: 0.1444, Train MAE: 0.1444,
                            Val Loss: 0.1491, Val MAE: 0.1488
2023-02-07 09:55:52,537:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 10:06:52,735:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.20s, LR: 0.00070, Train Loss: 0.1403, Train MAE: 0.1403,
                            Val Loss: 0.1612, Val MAE: 0.1609
2023-02-07 10:06:52,754:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 10:17:53,132:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 660.38s, LR: 0.00070, Train Loss: 0.1547, Train MAE: 0.1547,
                            Val Loss: 0.1614, Val MAE: 0.1611
2023-02-07 10:17:53,134:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 10:28:48,660:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.53s, LR: 0.00070, Train Loss: 0.1532, Train MAE: 0.1532,
                            Val Loss: 0.1731, Val MAE: 0.1727
2023-02-07 10:28:48,682:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-07 10:39:41,735:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 653.05s, LR: 0.00070, Train Loss: 0.1428, Train MAE: 0.1428,
                            Val Loss: 0.1651, Val MAE: 0.1648
2023-02-07 10:39:41,736:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-07 10:50:37,990:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.25s, LR: 0.00070, Train Loss: 0.1427, Train MAE: 0.1427,
                            Val Loss: 0.1578, Val MAE: 0.1575
2023-02-07 10:50:37,991:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-07 11:01:33,680:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.69s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1506, Val MAE: 0.1503
2023-02-07 11:01:33,692:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-07 11:12:29,318:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.63s, LR: 0.00070, Train Loss: 0.1417, Train MAE: 0.1417,
                            Val Loss: 0.1888, Val MAE: 0.1884
2023-02-07 11:12:29,319:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-07 11:23:25,210:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14843352138996124
2023-02-07 11:23:25,221:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.90s, LR: 0.00070, Train Loss: 0.1456, Train MAE: 0.1456,
                            Val Loss: 0.1487, Val MAE: 0.1484
2023-02-07 11:23:25,222:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-07 11:34:21,227:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1433950662612915
2023-02-07 11:34:21,244:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.02s, LR: 0.00070, Train Loss: 0.1363, Train MAE: 0.1363,
                            Val Loss: 0.1436, Val MAE: 0.1434
2023-02-07 11:34:21,245:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-07 11:45:17,295:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13962598145008087
2023-02-07 11:45:17,327:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.08s, LR: 0.00070, Train Loss: 0.1364, Train MAE: 0.1364,
                            Val Loss: 0.1398, Val MAE: 0.1396
2023-02-07 11:45:17,328:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-07 11:56:13,264:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.93s, LR: 0.00070, Train Loss: 0.1327, Train MAE: 0.1327,
                            Val Loss: 0.1459, Val MAE: 0.1457
2023-02-07 11:56:13,265:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-07 12:08:09,746:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13751888275146484
2023-02-07 12:08:09,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 716.51s, LR: 0.00070, Train Loss: 0.1302, Train MAE: 0.1302,
                            Val Loss: 0.1377, Val MAE: 0.1375
2023-02-07 12:08:09,773:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-07 12:19:11,197:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13397635519504547
2023-02-07 12:19:11,217:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.44s, LR: 0.00070, Train Loss: 0.1290, Train MAE: 0.1290,
                            Val Loss: 0.1342, Val MAE: 0.1340
2023-02-07 12:19:11,217:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-07 12:30:13,420:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 662.20s, LR: 0.00070, Train Loss: 0.1274, Train MAE: 0.1274,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-02-07 12:30:13,438:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-07 12:41:14,998:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13236258924007416
2023-02-07 12:41:15,033:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.59s, LR: 0.00070, Train Loss: 0.1260, Train MAE: 0.1260,
                            Val Loss: 0.1326, Val MAE: 0.1324
2023-02-07 12:41:15,033:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-07 12:52:16,553:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.52s, LR: 0.00070, Train Loss: 0.1252, Train MAE: 0.1252,
                            Val Loss: 0.1425, Val MAE: 0.1423
2023-02-07 12:52:16,554:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-07 13:03:17,985:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.43s, LR: 0.00070, Train Loss: 0.1245, Train MAE: 0.1245,
                            Val Loss: 0.1402, Val MAE: 0.1400
2023-02-07 13:03:17,998:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-07 13:14:19,487:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12985074520111084
2023-02-07 13:14:19,496:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.50s, LR: 0.00070, Train Loss: 0.1238, Train MAE: 0.1238,
                            Val Loss: 0.1301, Val MAE: 0.1299
2023-02-07 13:14:19,496:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-07 13:25:13,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 653.94s, LR: 0.00070, Train Loss: 0.1224, Train MAE: 0.1224,
                            Val Loss: 0.1354, Val MAE: 0.1351
2023-02-07 13:25:13,439:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-07 13:36:10,068:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12776879966259003
2023-02-07 13:36:10,079:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.64s, LR: 0.00070, Train Loss: 0.1221, Train MAE: 0.1221,
                            Val Loss: 0.1280, Val MAE: 0.1278
2023-02-07 13:36:10,079:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-07 13:47:06,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1212, Train MAE: 0.1212,
                            Val Loss: 0.1307, Val MAE: 0.1304
2023-02-07 13:47:06,455:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-07 13:58:02,730:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.28s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1316, Val MAE: 0.1314
2023-02-07 13:58:02,732:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-07 14:08:59,096:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1202, Train MAE: 0.1202,
                            Val Loss: 0.1572, Val MAE: 0.1570
2023-02-07 14:08:59,097:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-07 14:19:55,308:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12637346982955933
2023-02-07 14:19:55,324:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.23s, LR: 0.00070, Train Loss: 0.1195, Train MAE: 0.1195,
                            Val Loss: 0.1266, Val MAE: 0.1264
2023-02-07 14:19:55,324:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-07 14:30:51,982:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.66s, LR: 0.00070, Train Loss: 0.1192, Train MAE: 0.1192,
                            Val Loss: 0.1478, Val MAE: 0.1476
2023-02-07 14:30:51,995:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-07 14:41:48,801:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.81s, LR: 0.00070, Train Loss: 0.1183, Train MAE: 0.1183,
                            Val Loss: 0.1486, Val MAE: 0.1485
2023-02-07 14:41:48,810:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-07 14:52:45,439:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.63s, LR: 0.00070, Train Loss: 0.1182, Train MAE: 0.1182,
                            Val Loss: 0.1321, Val MAE: 0.1319
2023-02-07 14:52:45,440:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-07 15:03:41,946:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.51s, LR: 0.00070, Train Loss: 0.1175, Train MAE: 0.1175,
                            Val Loss: 0.1295, Val MAE: 0.1292
2023-02-07 15:03:41,963:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-07 15:15:39,969:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12334844470024109
2023-02-07 15:15:39,986:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.02s, LR: 0.00070, Train Loss: 0.1173, Train MAE: 0.1173,
                            Val Loss: 0.1236, Val MAE: 0.1233
2023-02-07 15:15:39,986:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-07 15:26:41,521:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.53s, LR: 0.00070, Train Loss: 0.1165, Train MAE: 0.1165,
                            Val Loss: 0.1260, Val MAE: 0.1258
2023-02-07 15:26:41,532:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-07 15:37:43,008:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 661.48s, LR: 0.00070, Train Loss: 0.1169, Train MAE: 0.1169,
                            Val Loss: 0.1304, Val MAE: 0.1302
2023-02-07 15:37:43,009:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-07 15:48:38,638:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 655.63s, LR: 0.00070, Train Loss: 0.1187, Train MAE: 0.1187,
                            Val Loss: 0.1262, Val MAE: 0.1260
2023-02-07 15:48:38,639:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-07 15:59:34,740:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.10s, LR: 0.00070, Train Loss: 0.1156, Train MAE: 0.1156,
                            Val Loss: 0.1347, Val MAE: 0.1344
2023-02-07 15:59:34,741:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-07 16:10:31,098:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 656.36s, LR: 0.00070, Train Loss: 0.1160, Train MAE: 0.1160,
                            Val Loss: 0.1244, Val MAE: 0.1242
2023-02-07 16:10:31,100:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-07 16:21:28,192:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 657.09s, LR: 0.00070, Train Loss: 0.1153, Train MAE: 0.1153,
                            Val Loss: 0.1305, Val MAE: 0.1302
2023-02-07 16:21:28,204:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
