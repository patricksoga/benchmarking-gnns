2023-02-07 05:15:55,599:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 05:15:55,599:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:29:37,816:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:30:09,318:ogbdata.py:332 -             __init__(): Time taken: 853.7185s
2023-02-07 05:30:09,318:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:30:09,318:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:30:09,318:ogbdata.py:348 -             __init__(): [I] Data load time: 853.7191s
2023-02-07 05:30:09,318:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_35.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [35]}
2023-02-07 05:30:09,318:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [35], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:30:09,353:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:09,557:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:09,557:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:09,557:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:09,660:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:30:09,675:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:30:09,675:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 35 in [35]...
2023-02-07 05:30:09,676:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:09,676:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:09,676:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:09,676:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:16,557:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 07:29:30,129:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7160.453615903854
2023-02-07 07:29:30,166:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 07:29:30,166:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 07:29:30,172:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 07:45:11,821:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.348657488822937
2023-02-07 07:45:11,852:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 941.68s, LR: 0.00070, Train Loss: 0.3286, Train MAE: 0.3286,
                            Val Loss: 0.3490, Val MAE: 0.3487
2023-02-07 07:45:11,852:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 07:57:12,875:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22880108654499054
2023-02-07 07:57:12,877:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.02s, LR: 0.00070, Train Loss: 0.1891, Train MAE: 0.1891,
                            Val Loss: 0.2292, Val MAE: 0.2288
2023-02-07 07:57:12,878:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 08:09:14,442:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18549363315105438
2023-02-07 08:09:14,445:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.57s, LR: 0.00070, Train Loss: 0.1727, Train MAE: 0.1727,
                            Val Loss: 0.1858, Val MAE: 0.1855
2023-02-07 08:09:14,445:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 08:21:04,785:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16657811403274536
2023-02-07 08:21:04,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.34s, LR: 0.00070, Train Loss: 0.1623, Train MAE: 0.1623,
                            Val Loss: 0.1669, Val MAE: 0.1666
2023-02-07 08:21:04,788:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 08:33:05,889:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.10s, LR: 0.00070, Train Loss: 0.1544, Train MAE: 0.1544,
                            Val Loss: 0.1674, Val MAE: 0.1670
2023-02-07 08:33:05,890:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 08:45:07,230:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1658417135477066
2023-02-07 08:45:07,232:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.34s, LR: 0.00070, Train Loss: 0.1502, Train MAE: 0.1502,
                            Val Loss: 0.1662, Val MAE: 0.1658
2023-02-07 08:45:07,233:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 08:57:09,467:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.23s, LR: 0.00070, Train Loss: 0.1464, Train MAE: 0.1464,
                            Val Loss: 0.1787, Val MAE: 0.1784
2023-02-07 08:57:09,469:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 09:10:14,100:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.63s, LR: 0.00070, Train Loss: 0.1430, Train MAE: 0.1430,
                            Val Loss: 0.2027, Val MAE: 0.2025
2023-02-07 09:10:14,101:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 09:22:16,387:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1598876714706421
2023-02-07 09:22:16,390:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.29s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1603, Val MAE: 0.1599
2023-02-07 09:22:16,390:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 09:34:16,550:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.16s, LR: 0.00070, Train Loss: 0.1379, Train MAE: 0.1379,
                            Val Loss: 0.1623, Val MAE: 0.1620
2023-02-07 09:34:16,552:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 09:46:16,838:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1445818394422531
2023-02-07 09:46:16,840:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.29s, LR: 0.00070, Train Loss: 0.1362, Train MAE: 0.1362,
                            Val Loss: 0.1449, Val MAE: 0.1446
2023-02-07 09:46:16,841:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 09:58:17,759:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.92s, LR: 0.00070, Train Loss: 0.1342, Train MAE: 0.1342,
                            Val Loss: 0.1498, Val MAE: 0.1495
2023-02-07 09:58:17,759:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 10:10:18,560:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.80s, LR: 0.00070, Train Loss: 0.1325, Train MAE: 0.1325,
                            Val Loss: 0.1626, Val MAE: 0.1623
2023-02-07 10:10:18,561:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 10:22:19,006:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13928979635238647
2023-02-07 10:22:19,008:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.45s, LR: 0.00070, Train Loss: 0.1315, Train MAE: 0.1315,
                            Val Loss: 0.1396, Val MAE: 0.1393
2023-02-07 10:22:19,009:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 10:34:19,853:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.84s, LR: 0.00070, Train Loss: 0.1303, Train MAE: 0.1303,
                            Val Loss: 0.1415, Val MAE: 0.1411
2023-02-07 10:34:19,854:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-07 10:46:20,337:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.48s, LR: 0.00070, Train Loss: 0.1292, Train MAE: 0.1292,
                            Val Loss: 0.1585, Val MAE: 0.1582
2023-02-07 10:46:20,337:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-07 10:58:22,961:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.62s, LR: 0.00070, Train Loss: 0.1282, Train MAE: 0.1282,
                            Val Loss: 0.1583, Val MAE: 0.1580
2023-02-07 10:58:22,962:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-07 11:10:23,277:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.31s, LR: 0.00070, Train Loss: 0.1264, Train MAE: 0.1264,
                            Val Loss: 0.1489, Val MAE: 0.1486
2023-02-07 11:10:23,278:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-07 11:22:23,996:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13716134428977966
2023-02-07 11:22:23,999:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.72s, LR: 0.00070, Train Loss: 0.1262, Train MAE: 0.1262,
                            Val Loss: 0.1375, Val MAE: 0.1372
2023-02-07 11:22:23,999:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-07 11:34:24,634:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13260483741760254
2023-02-07 11:34:24,636:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.64s, LR: 0.00070, Train Loss: 0.1250, Train MAE: 0.1250,
                            Val Loss: 0.1329, Val MAE: 0.1326
2023-02-07 11:34:24,636:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-07 11:46:25,412:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.77s, LR: 0.00070, Train Loss: 0.1241, Train MAE: 0.1241,
                            Val Loss: 0.1393, Val MAE: 0.1390
2023-02-07 11:46:25,413:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-07 11:58:26,460:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.05s, LR: 0.00070, Train Loss: 0.1233, Train MAE: 0.1233,
                            Val Loss: 0.1480, Val MAE: 0.1477
2023-02-07 11:58:26,461:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-07 12:10:27,011:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.55s, LR: 0.00070, Train Loss: 0.1226, Train MAE: 0.1226,
                            Val Loss: 0.1526, Val MAE: 0.1523
2023-02-07 12:10:27,011:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-07 12:23:33,104:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.09s, LR: 0.00070, Train Loss: 0.1223, Train MAE: 0.1223,
                            Val Loss: 0.1333, Val MAE: 0.1330
2023-02-07 12:23:33,106:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-07 12:35:32,074:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.97s, LR: 0.00070, Train Loss: 0.1212, Train MAE: 0.1212,
                            Val Loss: 0.1657, Val MAE: 0.1655
2023-02-07 12:35:32,075:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-07 12:47:32,844:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.77s, LR: 0.00070, Train Loss: 0.1203, Train MAE: 0.1203,
                            Val Loss: 0.1372, Val MAE: 0.1368
2023-02-07 12:47:32,845:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-07 12:59:33,272:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.43s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1378, Val MAE: 0.1375
2023-02-07 12:59:33,274:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-07 13:11:33,983:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.71s, LR: 0.00070, Train Loss: 0.1196, Train MAE: 0.1196,
                            Val Loss: 0.1365, Val MAE: 0.1362
2023-02-07 13:11:33,984:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-07 13:23:34,991:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.01s, LR: 0.00070, Train Loss: 0.1186, Train MAE: 0.1186,
                            Val Loss: 0.1335, Val MAE: 0.1332
2023-02-07 13:23:34,992:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-07 13:35:35,193:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.20s, LR: 0.00070, Train Loss: 0.1183, Train MAE: 0.1183,
                            Val Loss: 0.1429, Val MAE: 0.1426
2023-02-07 13:35:35,194:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-07 13:47:35,535:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13205976784229279
2023-02-07 13:47:35,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.34s, LR: 0.00070, Train Loss: 0.1178, Train MAE: 0.1178,
                            Val Loss: 0.1324, Val MAE: 0.1321
2023-02-07 13:47:35,537:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-07 13:59:35,788:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12850193679332733
2023-02-07 13:59:35,790:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.25s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1288, Val MAE: 0.1285
2023-02-07 13:59:35,791:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-07 14:11:35,891:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.10s, LR: 0.00070, Train Loss: 0.1166, Train MAE: 0.1166,
                            Val Loss: 0.1294, Val MAE: 0.1291
2023-02-07 14:11:35,892:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-07 14:23:35,956:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.06s, LR: 0.00070, Train Loss: 0.1167, Train MAE: 0.1167,
                            Val Loss: 0.1436, Val MAE: 0.1433
2023-02-07 14:23:35,957:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-07 14:35:35,385:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.43s, LR: 0.00070, Train Loss: 0.1162, Train MAE: 0.1162,
                            Val Loss: 0.1291, Val MAE: 0.1288
2023-02-07 14:35:35,386:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-07 14:47:34,978:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.59s, LR: 0.00070, Train Loss: 0.1154, Train MAE: 0.1154,
                            Val Loss: 0.1338, Val MAE: 0.1336
2023-02-07 14:47:34,979:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-07 14:59:35,094:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12599895894527435
2023-02-07 14:59:35,096:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.12s, LR: 0.00070, Train Loss: 0.1149, Train MAE: 0.1149,
                            Val Loss: 0.1263, Val MAE: 0.1260
2023-02-07 14:59:35,096:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-07 15:11:35,613:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.52s, LR: 0.00070, Train Loss: 0.1152, Train MAE: 0.1152,
                            Val Loss: 0.1270, Val MAE: 0.1267
2023-02-07 15:11:35,614:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-07 15:23:36,484:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.87s, LR: 0.00070, Train Loss: 0.1142, Train MAE: 0.1142,
                            Val Loss: 0.1270, Val MAE: 0.1267
2023-02-07 15:23:36,485:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-07 15:35:36,269:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.78s, LR: 0.00070, Train Loss: 0.1142, Train MAE: 0.1142,
                            Val Loss: 0.1391, Val MAE: 0.1389
2023-02-07 15:35:36,270:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-07 15:48:39,527:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12464947253465652
2023-02-07 15:48:39,529:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.26s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1249, Val MAE: 0.1246
2023-02-07 15:48:39,530:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-07 16:00:39,695:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.16s, LR: 0.00070, Train Loss: 0.1136, Train MAE: 0.1136,
                            Val Loss: 0.1294, Val MAE: 0.1291
2023-02-07 16:00:39,696:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-07 16:12:39,354:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.66s, LR: 0.00070, Train Loss: 0.1132, Train MAE: 0.1132,
                            Val Loss: 0.1314, Val MAE: 0.1311
2023-02-07 16:12:39,354:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-07 16:24:39,143:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.79s, LR: 0.00070, Train Loss: 0.1132, Train MAE: 0.1132,
                            Val Loss: 0.1262, Val MAE: 0.1259
2023-02-07 16:24:39,144:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-07 16:36:39,390:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.24s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.1323, Val MAE: 0.1321
2023-02-07 16:36:39,391:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-07 16:48:40,037:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.64s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.1297, Val MAE: 0.1294
2023-02-07 16:48:40,037:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-07 17:00:40,904:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.87s, LR: 0.00070, Train Loss: 0.1117, Train MAE: 0.1117,
                            Val Loss: 0.1281, Val MAE: 0.1278
2023-02-07 17:00:40,905:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-07 17:12:19,946:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.04s, LR: 0.00070, Train Loss: 0.1119, Train MAE: 0.1119,
                            Val Loss: 0.1275, Val MAE: 0.1272
2023-02-07 17:12:19,947:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-07 17:23:10,836:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.89s, LR: 0.00070, Train Loss: 0.1113, Train MAE: 0.1113,
                            Val Loss: 0.1293, Val MAE: 0.1290
2023-02-07 17:23:10,837:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-07 17:34:50,149:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12121716141700745
2023-02-07 17:34:50,151:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.31s, LR: 0.00070, Train Loss: 0.1108, Train MAE: 0.1108,
                            Val Loss: 0.1215, Val MAE: 0.1212
2023-02-07 17:34:50,151:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-07 17:45:42,115:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 651.96s, LR: 0.00070, Train Loss: 0.1106, Train MAE: 0.1106,
                            Val Loss: 0.1249, Val MAE: 0.1246
2023-02-07 17:45:42,116:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-07 17:56:33,279:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 651.16s, LR: 0.00070, Train Loss: 0.1103, Train MAE: 0.1103,
                            Val Loss: 0.1330, Val MAE: 0.1327
2023-02-07 17:56:33,280:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-07 18:07:24,152:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.87s, LR: 0.00070, Train Loss: 0.1102, Train MAE: 0.1102,
                            Val Loss: 0.1231, Val MAE: 0.1229
2023-02-07 18:07:24,153:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-07 18:18:15,140:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.99s, LR: 0.00070, Train Loss: 0.1101, Train MAE: 0.1101,
                            Val Loss: 0.1234, Val MAE: 0.1231
2023-02-07 18:18:15,141:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-07 18:29:38,531:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 683.39s, LR: 0.00070, Train Loss: 0.1097, Train MAE: 0.1097,
                            Val Loss: 0.1292, Val MAE: 0.1290
2023-02-07 18:29:38,532:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-07 18:41:38,153:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.62s, LR: 0.00070, Train Loss: 0.1100, Train MAE: 0.1100,
                            Val Loss: 0.1257, Val MAE: 0.1254
2023-02-07 18:41:38,154:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-07 18:53:38,034:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12081406265497208
2023-02-07 18:53:38,036:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.88s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1211, Val MAE: 0.1208
2023-02-07 18:53:38,037:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-07 19:06:44,461:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.42s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1223, Val MAE: 0.1221
2023-02-07 19:06:44,534:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-07 19:18:43,296:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.76s, LR: 0.00070, Train Loss: 0.1086, Train MAE: 0.1086,
                            Val Loss: 0.1230, Val MAE: 0.1228
2023-02-07 19:18:43,324:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-07 19:30:42,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.19s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1246, Val MAE: 0.1243
2023-02-07 19:30:42,566:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-02-07 19:42:43,151:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.58s, LR: 0.00070, Train Loss: 0.1089, Train MAE: 0.1089,
                            Val Loss: 0.1287, Val MAE: 0.1284
2023-02-07 19:42:43,165:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-02-07 19:54:42,521:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.35s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1234, Val MAE: 0.1232
2023-02-07 19:54:42,522:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-02-07 20:06:25,027:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12062329798936844
2023-02-07 20:06:25,056:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 702.53s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1209, Val MAE: 0.1206
2023-02-07 20:06:25,057:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-02-07 20:17:13,098:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 648.04s, LR: 0.00070, Train Loss: 0.1082, Train MAE: 0.1082,
                            Val Loss: 0.1268, Val MAE: 0.1266
2023-02-07 20:17:13,120:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-02-07 20:27:55,905:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 642.78s, LR: 0.00070, Train Loss: 0.1075, Train MAE: 0.1075,
                            Val Loss: 0.1218, Val MAE: 0.1216
2023-02-07 20:27:55,928:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-02-07 20:38:53,604:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 657.68s, LR: 0.00070, Train Loss: 0.1075, Train MAE: 0.1075,
                            Val Loss: 0.1233, Val MAE: 0.1231
2023-02-07 20:38:53,627:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-02-07 20:49:42,294:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11931020021438599
2023-02-07 20:49:42,354:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 648.73s, LR: 0.00070, Train Loss: 0.1075, Train MAE: 0.1075,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-07 20:49:42,355:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-02-07 21:12:49,510:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 21:12:49,511:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:13:25,000:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 21:13:25,001:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:14:29,192:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 21:14:29,193:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:14:44,040:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 21:14:44,041:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:14:58,274:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 21:14:58,275:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:15:15,684:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-02-07 21:15:15,685:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 21:25:42,670:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 21:26:10,250:ogbdata.py:332 -             __init__(): Time taken: 654.5650s
2023-02-07 21:26:10,250:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 21:26:10,250:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 21:26:10,250:ogbdata.py:348 -             __init__(): [I] Data load time: 654.5657s
2023-02-07 21:26:10,251:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_35.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [35]}
2023-02-07 21:26:10,251:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [35], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 21:26:10,261:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 21:26:10,315:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 21:26:10,315:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 21:26:10,315:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 21:26:10,343:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 21:26:10,354:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 21:26:10,355:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 35 in [35]...
2023-02-07 21:26:10,355:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 21:26:10,355:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 21:26:10,355:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 21:26:10,355:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 21:26:17,855:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 23:23:49,775:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7059.402974367142
2023-02-07 23:23:50,240:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 23:23:50,240:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 23:23:50,318:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 23:38:01,100:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22758416831493378
2023-02-07 23:38:01,203:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 850.89s, LR: 0.00070, Train Loss: 0.3360, Train MAE: 0.3360,
                            Val Loss: 0.2280, Val MAE: 0.2276
2023-02-07 23:38:01,204:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 23:50:26,380:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.19936609268188477
2023-02-07 23:50:26,383:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 745.18s, LR: 0.00070, Train Loss: 0.1904, Train MAE: 0.1904,
                            Val Loss: 0.1998, Val MAE: 0.1994
2023-02-07 23:50:26,384:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-08 00:02:58,555:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.17s, LR: 0.00070, Train Loss: 0.1755, Train MAE: 0.1755,
                            Val Loss: 0.2498, Val MAE: 0.2494
2023-02-08 00:02:58,557:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-08 00:15:25,497:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 746.94s, LR: 0.00070, Train Loss: 0.1672, Train MAE: 0.1672,
                            Val Loss: 0.2170, Val MAE: 0.2167
2023-02-08 00:15:25,498:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-08 00:28:12,725:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16103127598762512
2023-02-08 00:28:12,852:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 767.35s, LR: 0.00070, Train Loss: 0.1603, Train MAE: 0.1603,
                            Val Loss: 0.1614, Val MAE: 0.1610
2023-02-08 00:28:12,854:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-08 00:41:03,512:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 770.65s, LR: 0.00070, Train Loss: 0.1568, Train MAE: 0.1568,
                            Val Loss: 0.2177, Val MAE: 0.2174
2023-02-08 00:41:03,531:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-08 00:53:27,651:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 744.11s, LR: 0.00070, Train Loss: 0.1538, Train MAE: 0.1538,
                            Val Loss: 0.1633, Val MAE: 0.1629
2023-02-08 00:53:27,658:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-08 01:06:40,861:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 793.20s, LR: 0.00070, Train Loss: 0.1497, Train MAE: 0.1497,
                            Val Loss: 0.1794, Val MAE: 0.1790
2023-02-08 01:06:40,863:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-08 01:19:19,078:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 758.20s, LR: 0.00070, Train Loss: 0.1472, Train MAE: 0.1472,
                            Val Loss: 0.1826, Val MAE: 0.1823
2023-02-08 01:19:19,105:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-08 01:32:02,607:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 763.49s, LR: 0.00070, Train Loss: 0.1444, Train MAE: 0.1444,
                            Val Loss: 0.1839, Val MAE: 0.1836
2023-02-08 01:32:02,623:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-08 01:44:45,617:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 762.98s, LR: 0.00070, Train Loss: 0.1436, Train MAE: 0.1436,
                            Val Loss: 0.1614, Val MAE: 0.1611
2023-02-08 01:44:45,620:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-08 01:56:25,413:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.79s, LR: 0.00070, Train Loss: 0.1431, Train MAE: 0.1431,
                            Val Loss: 0.2113, Val MAE: 0.2111
2023-02-08 01:56:25,415:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-08 02:08:33,235:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1522238850593567
2023-02-08 02:08:33,263:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 727.85s, LR: 0.00070, Train Loss: 0.1399, Train MAE: 0.1399,
                            Val Loss: 0.1525, Val MAE: 0.1522
2023-02-08 02:08:33,264:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-08 02:21:24,857:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 771.58s, LR: 0.00070, Train Loss: 0.1399, Train MAE: 0.1399,
                            Val Loss: 0.2405, Val MAE: 0.2403
2023-02-08 02:21:24,904:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-08 02:34:15,762:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 770.86s, LR: 0.00070, Train Loss: 0.1378, Train MAE: 0.1378,
                            Val Loss: 0.1543, Val MAE: 0.1540
2023-02-08 02:34:15,763:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-08 02:46:49,132:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.35s, LR: 0.00070, Train Loss: 0.1373, Train MAE: 0.1373,
                            Val Loss: 0.2281, Val MAE: 0.2279
2023-02-08 02:46:49,213:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-08 02:58:27,371:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 698.14s, LR: 0.00070, Train Loss: 0.1359, Train MAE: 0.1359,
                            Val Loss: 0.1731, Val MAE: 0.1728
2023-02-08 02:58:27,393:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-08 03:10:09,093:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14153026044368744
2023-02-08 03:10:09,108:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 701.71s, LR: 0.00070, Train Loss: 0.1348, Train MAE: 0.1348,
                            Val Loss: 0.1418, Val MAE: 0.1415
2023-02-08 03:10:09,109:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-08 03:22:09,448:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.31s, LR: 0.00070, Train Loss: 0.1384, Train MAE: 0.1384,
                            Val Loss: 0.1726, Val MAE: 0.1722
2023-02-08 03:22:09,495:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-08 03:34:49,654:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 760.16s, LR: 0.00070, Train Loss: 0.1344, Train MAE: 0.1344,
                            Val Loss: 0.1838, Val MAE: 0.1835
2023-02-08 03:34:49,682:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-08 03:47:38,309:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 768.63s, LR: 0.00070, Train Loss: 0.1387, Train MAE: 0.1387,
                            Val Loss: 0.1811, Val MAE: 0.1809
2023-02-08 03:47:38,310:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-08 03:59:23,757:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 705.45s, LR: 0.00070, Train Loss: 0.1460, Train MAE: 0.1460,
                            Val Loss: 0.1505, Val MAE: 0.1502
2023-02-08 03:59:23,759:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-08 04:11:01,342:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.57s, LR: 0.00070, Train Loss: 0.1389, Train MAE: 0.1389,
                            Val Loss: 0.1466, Val MAE: 0.1463
2023-02-08 04:11:01,397:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-08 04:23:45,254:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.138550266623497
2023-02-08 04:23:45,256:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 763.86s, LR: 0.00070, Train Loss: 0.1326, Train MAE: 0.1326,
                            Val Loss: 0.1388, Val MAE: 0.1386
2023-02-08 04:23:45,257:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-08 04:36:23,900:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 758.63s, LR: 0.00070, Train Loss: 0.1290, Train MAE: 0.1290,
                            Val Loss: 0.1410, Val MAE: 0.1407
2023-02-08 04:36:23,925:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-08 04:48:46,012:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 742.09s, LR: 0.00070, Train Loss: 0.1275, Train MAE: 0.1275,
                            Val Loss: 0.1447, Val MAE: 0.1445
2023-02-08 04:48:46,026:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-08 05:01:28,484:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1361069530248642
2023-02-08 05:01:28,498:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 762.47s, LR: 0.00070, Train Loss: 0.1255, Train MAE: 0.1255,
                            Val Loss: 0.1363, Val MAE: 0.1361
2023-02-08 05:01:28,500:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-08 05:14:16,457:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13316814601421356
2023-02-08 05:14:16,469:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 767.97s, LR: 0.00070, Train Loss: 0.1246, Train MAE: 0.1246,
                            Val Loss: 0.1334, Val MAE: 0.1332
2023-02-08 05:14:16,471:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-08 05:27:04,135:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13267947733402252
2023-02-08 05:27:04,169:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 767.70s, LR: 0.00070, Train Loss: 0.1234, Train MAE: 0.1234,
                            Val Loss: 0.1329, Val MAE: 0.1327
2023-02-08 05:27:04,170:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-08 05:39:54,454:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 770.28s, LR: 0.00070, Train Loss: 0.1222, Train MAE: 0.1222,
                            Val Loss: 0.1358, Val MAE: 0.1356
2023-02-08 05:39:54,466:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-08 05:52:25,013:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13193610310554504
2023-02-08 05:52:25,033:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.57s, LR: 0.00070, Train Loss: 0.1213, Train MAE: 0.1213,
                            Val Loss: 0.1322, Val MAE: 0.1319
2023-02-08 05:52:25,034:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-08 06:04:38,322:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 733.26s, LR: 0.00070, Train Loss: 0.1207, Train MAE: 0.1207,
                            Val Loss: 0.1360, Val MAE: 0.1357
2023-02-08 06:04:38,361:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-08 06:16:51,570:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 733.21s, LR: 0.00070, Train Loss: 0.1194, Train MAE: 0.1194,
                            Val Loss: 0.1329, Val MAE: 0.1327
2023-02-08 06:16:51,599:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-08 06:29:13,727:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 742.13s, LR: 0.00070, Train Loss: 0.1189, Train MAE: 0.1189,
                            Val Loss: 0.1378, Val MAE: 0.1376
2023-02-08 06:29:13,762:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-08 06:41:57,028:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1250191330909729
2023-02-08 06:41:57,031:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 763.27s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1253, Val MAE: 0.1250
2023-02-08 06:41:57,032:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-08 06:54:48,233:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 771.18s, LR: 0.00070, Train Loss: 0.1171, Train MAE: 0.1171,
                            Val Loss: 0.1340, Val MAE: 0.1337
2023-02-08 06:54:48,295:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-08 07:07:36,738:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 768.44s, LR: 0.00070, Train Loss: 0.1155, Train MAE: 0.1155,
                            Val Loss: 0.1295, Val MAE: 0.1292
2023-02-08 07:07:36,740:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-08 07:20:21,466:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 764.72s, LR: 0.00070, Train Loss: 0.1152, Train MAE: 0.1152,
                            Val Loss: 0.1282, Val MAE: 0.1279
2023-02-08 07:20:21,487:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-08 07:31:51,607:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12499081343412399
2023-02-08 07:31:51,610:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.12s, LR: 0.00070, Train Loss: 0.1147, Train MAE: 0.1147,
                            Val Loss: 0.1252, Val MAE: 0.1250
2023-02-08 07:31:51,611:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-08 07:43:53,342:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.73s, LR: 0.00070, Train Loss: 0.1141, Train MAE: 0.1141,
                            Val Loss: 0.1265, Val MAE: 0.1262
2023-02-08 07:43:53,356:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-08 07:57:43,821:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.46s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1269, Val MAE: 0.1267
2023-02-08 07:57:43,842:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-08 08:10:00,605:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12463770061731339
2023-02-08 08:10:00,608:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 736.76s, LR: 0.00070, Train Loss: 0.1134, Train MAE: 0.1134,
                            Val Loss: 0.1249, Val MAE: 0.1246
2023-02-08 08:10:00,609:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-08 08:22:20,631:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12326174974441528
2023-02-08 08:22:20,653:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 740.04s, LR: 0.00070, Train Loss: 0.1129, Train MAE: 0.1129,
                            Val Loss: 0.1235, Val MAE: 0.1233
2023-02-08 08:22:20,654:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-08 08:34:40,047:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12324011325836182
2023-02-08 08:34:40,066:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 739.41s, LR: 0.00070, Train Loss: 0.1126, Train MAE: 0.1126,
                            Val Loss: 0.1235, Val MAE: 0.1232
2023-02-08 08:34:40,067:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-08 08:47:28,165:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 768.10s, LR: 0.00070, Train Loss: 0.1116, Train MAE: 0.1116,
                            Val Loss: 0.1243, Val MAE: 0.1241
2023-02-08 08:47:28,166:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-08 09:00:16,824:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 768.66s, LR: 0.00070, Train Loss: 0.1115, Train MAE: 0.1115,
                            Val Loss: 0.1261, Val MAE: 0.1259
2023-02-08 09:00:16,825:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-08 09:12:37,694:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12156260013580322
2023-02-08 09:12:37,712:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 740.89s, LR: 0.00070, Train Loss: 0.1112, Train MAE: 0.1112,
                            Val Loss: 0.1218, Val MAE: 0.1216
2023-02-08 09:12:37,713:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-08 09:25:27,951:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 770.24s, LR: 0.00070, Train Loss: 0.1106, Train MAE: 0.1106,
                            Val Loss: 0.1283, Val MAE: 0.1281
2023-02-08 09:25:27,965:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-08 09:37:40,210:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 732.24s, LR: 0.00070, Train Loss: 0.1104, Train MAE: 0.1104,
                            Val Loss: 0.1228, Val MAE: 0.1225
2023-02-08 09:37:40,228:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-08 09:49:40,765:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.54s, LR: 0.00070, Train Loss: 0.1103, Train MAE: 0.1103,
                            Val Loss: 0.1260, Val MAE: 0.1258
2023-02-08 09:49:40,783:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-08 10:02:24,587:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 763.80s, LR: 0.00070, Train Loss: 0.1100, Train MAE: 0.1100,
                            Val Loss: 0.1371, Val MAE: 0.1369
2023-02-08 10:02:24,588:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-08 10:15:16,699:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 772.11s, LR: 0.00070, Train Loss: 0.1095, Train MAE: 0.1095,
                            Val Loss: 0.1234, Val MAE: 0.1232
2023-02-08 10:15:16,708:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-08 10:27:47,465:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.76s, LR: 0.00070, Train Loss: 0.1095, Train MAE: 0.1095,
                            Val Loss: 0.1260, Val MAE: 0.1258
2023-02-08 10:27:47,466:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-08 10:40:36,319:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 768.85s, LR: 0.00070, Train Loss: 0.1089, Train MAE: 0.1089,
                            Val Loss: 0.1220, Val MAE: 0.1217
2023-02-08 10:40:36,334:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-08 10:53:03,860:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 747.53s, LR: 0.00070, Train Loss: 0.1089, Train MAE: 0.1089,
                            Val Loss: 0.1263, Val MAE: 0.1261
2023-02-08 10:53:03,871:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-08 11:05:00,284:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11968132108449936
2023-02-08 11:05:00,303:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 716.43s, LR: 0.00070, Train Loss: 0.1087, Train MAE: 0.1087,
                            Val Loss: 0.1199, Val MAE: 0.1197
2023-02-08 11:05:00,304:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-08 11:16:37,865:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.56s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1208, Val MAE: 0.1206
2023-02-08 11:16:37,882:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-08 11:29:18,232:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 760.35s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1216, Val MAE: 0.1213
2023-02-08 11:29:18,233:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-08 11:40:47,277:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 689.04s, LR: 0.00070, Train Loss: 0.1080, Train MAE: 0.1080,
                            Val Loss: 0.1219, Val MAE: 0.1216
2023-02-08 11:40:47,279:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-08 11:52:18,206:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.93s, LR: 0.00070, Train Loss: 0.1079, Train MAE: 0.1079,
                            Val Loss: 0.1246, Val MAE: 0.1243
2023-02-08 11:52:18,213:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-02-08 12:03:48,224:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.01s, LR: 0.00070, Train Loss: 0.1079, Train MAE: 0.1079,
                            Val Loss: 0.1232, Val MAE: 0.1230
2023-02-08 12:03:48,225:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-02-08 12:15:16,194:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.97s, LR: 0.00070, Train Loss: 0.1073, Train MAE: 0.1073,
                            Val Loss: 0.1210, Val MAE: 0.1207
2023-02-08 12:15:16,208:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-02-08 12:26:45,482:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11964917927980423
2023-02-08 12:26:45,484:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 689.28s, LR: 0.00070, Train Loss: 0.1070, Train MAE: 0.1070,
                            Val Loss: 0.1199, Val MAE: 0.1196
2023-02-08 12:26:45,486:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-02-08 12:38:12,746:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.26s, LR: 0.00070, Train Loss: 0.1071, Train MAE: 0.1071,
                            Val Loss: 0.1202, Val MAE: 0.1199
2023-02-08 12:38:12,760:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-02-08 12:49:40,438:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.68s, LR: 0.00070, Train Loss: 0.1072, Train MAE: 0.1072,
                            Val Loss: 0.1208, Val MAE: 0.1205
2023-02-08 12:49:40,457:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-02-08 13:01:08,556:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11930222064256668
2023-02-08 13:01:08,572:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.11s, LR: 0.00070, Train Loss: 0.1066, Train MAE: 0.1066,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-08 13:01:08,573:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-02-08 13:12:30,828:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 682.25s, LR: 0.00070, Train Loss: 0.1066, Train MAE: 0.1066,
                            Val Loss: 0.1210, Val MAE: 0.1208
2023-02-08 13:12:30,829:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-02-08 13:23:58,874:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11851296573877335
2023-02-08 13:23:58,876:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.05s, LR: 0.00070, Train Loss: 0.1063, Train MAE: 0.1063,
                            Val Loss: 0.1188, Val MAE: 0.1185
2023-02-08 13:23:58,876:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 69/1000
2023-02-08 13:35:26,241:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.36s, LR: 0.00070, Train Loss: 0.1063, Train MAE: 0.1063,
                            Val Loss: 0.1209, Val MAE: 0.1207
2023-02-08 13:35:26,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 70/1000
2023-02-08 13:46:54,411:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.15s, LR: 0.00070, Train Loss: 0.1060, Train MAE: 0.1060,
                            Val Loss: 0.1298, Val MAE: 0.1296
2023-02-08 13:46:54,412:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 71/1000
2023-02-08 13:58:21,422:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.01s, LR: 0.00070, Train Loss: 0.1061, Train MAE: 0.1061,
                            Val Loss: 0.1245, Val MAE: 0.1243
2023-02-08 13:58:21,432:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 72/1000
2023-02-08 14:09:47,944:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.51s, LR: 0.00070, Train Loss: 0.1060, Train MAE: 0.1060,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-08 14:09:47,954:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 73/1000
2023-02-08 14:21:14,972:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.02s, LR: 0.00070, Train Loss: 0.1056, Train MAE: 0.1056,
                            Val Loss: 0.1223, Val MAE: 0.1221
2023-02-08 14:21:14,985:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 74/1000
2023-02-08 14:32:41,658:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.67s, LR: 0.00070, Train Loss: 0.1055, Train MAE: 0.1055,
                            Val Loss: 0.1241, Val MAE: 0.1238
2023-02-08 14:32:41,673:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 75/1000
2023-02-08 14:45:07,085:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11811549961566925
2023-02-08 14:45:07,088:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 745.41s, LR: 0.00070, Train Loss: 0.1052, Train MAE: 0.1052,
                            Val Loss: 0.1184, Val MAE: 0.1181
2023-02-08 14:45:07,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 76/1000
2023-02-08 14:56:29,257:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 682.17s, LR: 0.00070, Train Loss: 0.1051, Train MAE: 0.1051,
                            Val Loss: 0.1236, Val MAE: 0.1233
2023-02-08 14:56:29,275:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 77/1000
2023-02-08 15:07:57,230:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11811530590057373
2023-02-08 15:07:57,249:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.97s, LR: 0.00070, Train Loss: 0.1050, Train MAE: 0.1050,
                            Val Loss: 0.1184, Val MAE: 0.1181
2023-02-08 15:07:57,249:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 78/1000
2023-02-08 15:19:24,906:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.66s, LR: 0.00070, Train Loss: 0.1049, Train MAE: 0.1049,
                            Val Loss: 0.1192, Val MAE: 0.1189
2023-02-08 15:19:24,919:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 79/1000
2023-02-08 15:30:53,448:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1177835538983345
2023-02-08 15:30:53,450:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.53s, LR: 0.00070, Train Loss: 0.1049, Train MAE: 0.1049,
                            Val Loss: 0.1181, Val MAE: 0.1178
2023-02-08 15:30:53,451:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 80/1000
2023-02-08 15:42:20,421:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11764228343963623
2023-02-08 15:42:20,423:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.97s, LR: 0.00070, Train Loss: 0.1050, Train MAE: 0.1050,
                            Val Loss: 0.1179, Val MAE: 0.1176
2023-02-08 15:42:20,424:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 81/1000
2023-02-08 15:53:48,459:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.03s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1236, Val MAE: 0.1234
2023-02-08 15:53:48,469:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 82/1000
2023-02-08 16:05:14,648:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.18s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1194, Val MAE: 0.1191
2023-02-08 16:05:14,666:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 83/1000
2023-02-08 16:16:42,175:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.51s, LR: 0.00070, Train Loss: 0.1046, Train MAE: 0.1046,
                            Val Loss: 0.1215, Val MAE: 0.1212
2023-02-08 16:16:42,188:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 84/1000
2023-02-08 16:28:04,558:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 682.37s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1192, Val MAE: 0.1189
2023-02-08 16:28:04,569:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 85/1000
