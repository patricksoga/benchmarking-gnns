2023-02-07 05:15:55,599:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 05:15:55,599:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:29:37,816:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:30:09,318:ogbdata.py:332 -             __init__(): Time taken: 853.7185s
2023-02-07 05:30:09,318:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:30:09,318:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:30:09,318:ogbdata.py:348 -             __init__(): [I] Data load time: 853.7191s
2023-02-07 05:30:09,318:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_35.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [35]}
2023-02-07 05:30:09,318:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [35], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:30:09,353:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:09,557:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:09,557:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:09,557:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:09,660:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:30:09,675:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:30:09,675:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 35 in [35]...
2023-02-07 05:30:09,676:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:30:09,676:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:30:09,676:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:30:09,676:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:30:16,557:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 07:29:30,129:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7160.453615903854
2023-02-07 07:29:30,166:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 07:29:30,166:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 07:29:30,172:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 07:45:11,821:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.348657488822937
2023-02-07 07:45:11,852:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 941.68s, LR: 0.00070, Train Loss: 0.3286, Train MAE: 0.3286,
                            Val Loss: 0.3490, Val MAE: 0.3487
2023-02-07 07:45:11,852:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 07:57:12,875:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22880108654499054
2023-02-07 07:57:12,877:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.02s, LR: 0.00070, Train Loss: 0.1891, Train MAE: 0.1891,
                            Val Loss: 0.2292, Val MAE: 0.2288
2023-02-07 07:57:12,878:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 08:09:14,442:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.18549363315105438
2023-02-07 08:09:14,445:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.57s, LR: 0.00070, Train Loss: 0.1727, Train MAE: 0.1727,
                            Val Loss: 0.1858, Val MAE: 0.1855
2023-02-07 08:09:14,445:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 08:21:04,785:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16657811403274536
2023-02-07 08:21:04,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.34s, LR: 0.00070, Train Loss: 0.1623, Train MAE: 0.1623,
                            Val Loss: 0.1669, Val MAE: 0.1666
2023-02-07 08:21:04,788:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 08:33:05,889:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.10s, LR: 0.00070, Train Loss: 0.1544, Train MAE: 0.1544,
                            Val Loss: 0.1674, Val MAE: 0.1670
2023-02-07 08:33:05,890:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 08:45:07,230:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1658417135477066
2023-02-07 08:45:07,232:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.34s, LR: 0.00070, Train Loss: 0.1502, Train MAE: 0.1502,
                            Val Loss: 0.1662, Val MAE: 0.1658
2023-02-07 08:45:07,233:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 08:57:09,467:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.23s, LR: 0.00070, Train Loss: 0.1464, Train MAE: 0.1464,
                            Val Loss: 0.1787, Val MAE: 0.1784
2023-02-07 08:57:09,469:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 09:10:14,100:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.63s, LR: 0.00070, Train Loss: 0.1430, Train MAE: 0.1430,
                            Val Loss: 0.2027, Val MAE: 0.2025
2023-02-07 09:10:14,101:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 09:22:16,387:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1598876714706421
2023-02-07 09:22:16,390:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.29s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1603, Val MAE: 0.1599
2023-02-07 09:22:16,390:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 09:34:16,550:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.16s, LR: 0.00070, Train Loss: 0.1379, Train MAE: 0.1379,
                            Val Loss: 0.1623, Val MAE: 0.1620
2023-02-07 09:34:16,552:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 09:46:16,838:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1445818394422531
2023-02-07 09:46:16,840:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.29s, LR: 0.00070, Train Loss: 0.1362, Train MAE: 0.1362,
                            Val Loss: 0.1449, Val MAE: 0.1446
2023-02-07 09:46:16,841:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 09:58:17,759:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.92s, LR: 0.00070, Train Loss: 0.1342, Train MAE: 0.1342,
                            Val Loss: 0.1498, Val MAE: 0.1495
2023-02-07 09:58:17,759:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 10:10:18,560:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.80s, LR: 0.00070, Train Loss: 0.1325, Train MAE: 0.1325,
                            Val Loss: 0.1626, Val MAE: 0.1623
2023-02-07 10:10:18,561:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 10:22:19,006:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13928979635238647
2023-02-07 10:22:19,008:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.45s, LR: 0.00070, Train Loss: 0.1315, Train MAE: 0.1315,
                            Val Loss: 0.1396, Val MAE: 0.1393
2023-02-07 10:22:19,009:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 10:34:19,853:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.84s, LR: 0.00070, Train Loss: 0.1303, Train MAE: 0.1303,
                            Val Loss: 0.1415, Val MAE: 0.1411
2023-02-07 10:34:19,854:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-07 10:46:20,337:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.48s, LR: 0.00070, Train Loss: 0.1292, Train MAE: 0.1292,
                            Val Loss: 0.1585, Val MAE: 0.1582
2023-02-07 10:46:20,337:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-07 10:58:22,961:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.62s, LR: 0.00070, Train Loss: 0.1282, Train MAE: 0.1282,
                            Val Loss: 0.1583, Val MAE: 0.1580
2023-02-07 10:58:22,962:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-07 11:10:23,277:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.31s, LR: 0.00070, Train Loss: 0.1264, Train MAE: 0.1264,
                            Val Loss: 0.1489, Val MAE: 0.1486
2023-02-07 11:10:23,278:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-07 11:22:23,996:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13716134428977966
2023-02-07 11:22:23,999:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.72s, LR: 0.00070, Train Loss: 0.1262, Train MAE: 0.1262,
                            Val Loss: 0.1375, Val MAE: 0.1372
2023-02-07 11:22:23,999:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-07 11:34:24,634:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13260483741760254
2023-02-07 11:34:24,636:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.64s, LR: 0.00070, Train Loss: 0.1250, Train MAE: 0.1250,
                            Val Loss: 0.1329, Val MAE: 0.1326
2023-02-07 11:34:24,636:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-07 11:46:25,412:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.77s, LR: 0.00070, Train Loss: 0.1241, Train MAE: 0.1241,
                            Val Loss: 0.1393, Val MAE: 0.1390
2023-02-07 11:46:25,413:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-07 11:58:26,460:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.05s, LR: 0.00070, Train Loss: 0.1233, Train MAE: 0.1233,
                            Val Loss: 0.1480, Val MAE: 0.1477
2023-02-07 11:58:26,461:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-07 12:10:27,011:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.55s, LR: 0.00070, Train Loss: 0.1226, Train MAE: 0.1226,
                            Val Loss: 0.1526, Val MAE: 0.1523
2023-02-07 12:10:27,011:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-07 12:23:33,104:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.09s, LR: 0.00070, Train Loss: 0.1223, Train MAE: 0.1223,
                            Val Loss: 0.1333, Val MAE: 0.1330
2023-02-07 12:23:33,106:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-07 12:35:32,074:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.97s, LR: 0.00070, Train Loss: 0.1212, Train MAE: 0.1212,
                            Val Loss: 0.1657, Val MAE: 0.1655
2023-02-07 12:35:32,075:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-07 12:47:32,844:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.77s, LR: 0.00070, Train Loss: 0.1203, Train MAE: 0.1203,
                            Val Loss: 0.1372, Val MAE: 0.1368
2023-02-07 12:47:32,845:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-07 12:59:33,272:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.43s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1378, Val MAE: 0.1375
2023-02-07 12:59:33,274:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-07 13:11:33,983:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.71s, LR: 0.00070, Train Loss: 0.1196, Train MAE: 0.1196,
                            Val Loss: 0.1365, Val MAE: 0.1362
2023-02-07 13:11:33,984:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-07 13:23:34,991:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.01s, LR: 0.00070, Train Loss: 0.1186, Train MAE: 0.1186,
                            Val Loss: 0.1335, Val MAE: 0.1332
2023-02-07 13:23:34,992:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-07 13:35:35,193:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.20s, LR: 0.00070, Train Loss: 0.1183, Train MAE: 0.1183,
                            Val Loss: 0.1429, Val MAE: 0.1426
2023-02-07 13:35:35,194:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-07 13:47:35,535:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13205976784229279
2023-02-07 13:47:35,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.34s, LR: 0.00070, Train Loss: 0.1178, Train MAE: 0.1178,
                            Val Loss: 0.1324, Val MAE: 0.1321
2023-02-07 13:47:35,537:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-07 13:59:35,788:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12850193679332733
2023-02-07 13:59:35,790:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.25s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1288, Val MAE: 0.1285
2023-02-07 13:59:35,791:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-07 14:11:35,891:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.10s, LR: 0.00070, Train Loss: 0.1166, Train MAE: 0.1166,
                            Val Loss: 0.1294, Val MAE: 0.1291
2023-02-07 14:11:35,892:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-07 14:23:35,956:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.06s, LR: 0.00070, Train Loss: 0.1167, Train MAE: 0.1167,
                            Val Loss: 0.1436, Val MAE: 0.1433
2023-02-07 14:23:35,957:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-07 14:35:35,385:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.43s, LR: 0.00070, Train Loss: 0.1162, Train MAE: 0.1162,
                            Val Loss: 0.1291, Val MAE: 0.1288
2023-02-07 14:35:35,386:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-07 14:47:34,978:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.59s, LR: 0.00070, Train Loss: 0.1154, Train MAE: 0.1154,
                            Val Loss: 0.1338, Val MAE: 0.1336
2023-02-07 14:47:34,979:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-07 14:59:35,094:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12599895894527435
2023-02-07 14:59:35,096:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.12s, LR: 0.00070, Train Loss: 0.1149, Train MAE: 0.1149,
                            Val Loss: 0.1263, Val MAE: 0.1260
2023-02-07 14:59:35,096:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-07 15:11:35,613:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.52s, LR: 0.00070, Train Loss: 0.1152, Train MAE: 0.1152,
                            Val Loss: 0.1270, Val MAE: 0.1267
2023-02-07 15:11:35,614:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-07 15:23:36,484:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.87s, LR: 0.00070, Train Loss: 0.1142, Train MAE: 0.1142,
                            Val Loss: 0.1270, Val MAE: 0.1267
2023-02-07 15:23:36,485:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-07 15:35:36,269:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.78s, LR: 0.00070, Train Loss: 0.1142, Train MAE: 0.1142,
                            Val Loss: 0.1391, Val MAE: 0.1389
2023-02-07 15:35:36,270:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-07 15:48:39,527:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12464947253465652
2023-02-07 15:48:39,529:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.26s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1249, Val MAE: 0.1246
2023-02-07 15:48:39,530:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-07 16:00:39,695:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.16s, LR: 0.00070, Train Loss: 0.1136, Train MAE: 0.1136,
                            Val Loss: 0.1294, Val MAE: 0.1291
2023-02-07 16:00:39,696:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-07 16:12:39,354:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.66s, LR: 0.00070, Train Loss: 0.1132, Train MAE: 0.1132,
                            Val Loss: 0.1314, Val MAE: 0.1311
2023-02-07 16:12:39,354:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-07 16:24:39,143:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.79s, LR: 0.00070, Train Loss: 0.1132, Train MAE: 0.1132,
                            Val Loss: 0.1262, Val MAE: 0.1259
2023-02-07 16:24:39,144:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-07 16:36:39,390:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.24s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.1323, Val MAE: 0.1321
2023-02-07 16:36:39,391:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-07 16:48:40,037:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.64s, LR: 0.00070, Train Loss: 0.1122, Train MAE: 0.1122,
                            Val Loss: 0.1297, Val MAE: 0.1294
2023-02-07 16:48:40,037:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-07 17:00:40,904:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.87s, LR: 0.00070, Train Loss: 0.1117, Train MAE: 0.1117,
                            Val Loss: 0.1281, Val MAE: 0.1278
2023-02-07 17:00:40,905:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-07 17:12:19,946:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.04s, LR: 0.00070, Train Loss: 0.1119, Train MAE: 0.1119,
                            Val Loss: 0.1275, Val MAE: 0.1272
2023-02-07 17:12:19,947:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-07 17:23:10,836:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.89s, LR: 0.00070, Train Loss: 0.1113, Train MAE: 0.1113,
                            Val Loss: 0.1293, Val MAE: 0.1290
2023-02-07 17:23:10,837:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-07 17:34:50,149:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12121716141700745
2023-02-07 17:34:50,151:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 699.31s, LR: 0.00070, Train Loss: 0.1108, Train MAE: 0.1108,
                            Val Loss: 0.1215, Val MAE: 0.1212
2023-02-07 17:34:50,151:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-07 17:45:42,115:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 651.96s, LR: 0.00070, Train Loss: 0.1106, Train MAE: 0.1106,
                            Val Loss: 0.1249, Val MAE: 0.1246
2023-02-07 17:45:42,116:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-07 17:56:33,279:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 651.16s, LR: 0.00070, Train Loss: 0.1103, Train MAE: 0.1103,
                            Val Loss: 0.1330, Val MAE: 0.1327
2023-02-07 17:56:33,280:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-07 18:07:24,152:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.87s, LR: 0.00070, Train Loss: 0.1102, Train MAE: 0.1102,
                            Val Loss: 0.1231, Val MAE: 0.1229
2023-02-07 18:07:24,153:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-07 18:18:15,140:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 650.99s, LR: 0.00070, Train Loss: 0.1101, Train MAE: 0.1101,
                            Val Loss: 0.1234, Val MAE: 0.1231
2023-02-07 18:18:15,141:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-07 18:29:38,531:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 683.39s, LR: 0.00070, Train Loss: 0.1097, Train MAE: 0.1097,
                            Val Loss: 0.1292, Val MAE: 0.1290
2023-02-07 18:29:38,532:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-07 18:41:38,153:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.62s, LR: 0.00070, Train Loss: 0.1100, Train MAE: 0.1100,
                            Val Loss: 0.1257, Val MAE: 0.1254
2023-02-07 18:41:38,154:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-07 18:53:38,034:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12081406265497208
2023-02-07 18:53:38,036:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.88s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1211, Val MAE: 0.1208
2023-02-07 18:53:38,037:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-07 19:06:44,461:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.42s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1223, Val MAE: 0.1221
2023-02-07 19:06:44,534:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-07 19:18:43,296:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 718.76s, LR: 0.00070, Train Loss: 0.1086, Train MAE: 0.1086,
                            Val Loss: 0.1230, Val MAE: 0.1228
2023-02-07 19:18:43,324:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-07 19:30:42,536:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.19s, LR: 0.00070, Train Loss: 0.1091, Train MAE: 0.1091,
                            Val Loss: 0.1246, Val MAE: 0.1243
2023-02-07 19:30:42,566:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
