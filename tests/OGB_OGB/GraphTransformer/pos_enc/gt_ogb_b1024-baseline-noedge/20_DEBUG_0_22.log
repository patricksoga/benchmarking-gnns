2023-02-07 05:23:14,385:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-02-07 05:23:14,385:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:35:05,067:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:35:34,122:ogbdata.py:332 -             __init__(): Time taken: 739.7370s
2023-02-07 05:35:34,122:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:35:34,122:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:35:34,123:ogbdata.py:348 -             __init__(): [I] Data load time: 739.7376s
2023-02-07 05:35:34,123:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_22.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [22]}
2023-02-07 05:35:34,123:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [22], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:35:34,124:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:35:34,137:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:35:34,138:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:35:34,138:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:35:34,154:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:35:34,156:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:35:34,157:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 22 in [22]...
2023-02-07 05:35:34,157:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:35:34,157:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:35:34,157:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:35:34,157:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:35:35,852:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 07:25:50,189:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 6616.032599687576
2023-02-07 07:25:50,615:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 07:25:50,615:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 07:25:50,909:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 16:30:00,125:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 16:30:00,125:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 16:31:25,075:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 16:31:25,076:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 16:32:54,266:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-02-07 16:32:54,283:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 16:45:04,356:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 16:45:33,914:ogbdata.py:332 -             __init__(): Time taken: 759.6305s
2023-02-07 16:45:33,914:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 16:45:33,914:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 16:45:33,914:ogbdata.py:348 -             __init__(): [I] Data load time: 759.6309s
2023-02-07 16:45:33,914:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_22.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [22]}
2023-02-07 16:45:33,914:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [22], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 16:45:33,926:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 16:45:33,947:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 16:45:33,947:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 16:45:33,947:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 16:45:34,023:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 16:45:34,040:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 16:45:34,040:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 22 in [22]...
2023-02-07 16:45:34,040:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 16:45:34,041:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 16:45:34,041:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 16:45:34,041:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 16:45:41,638:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 18:50:17,448:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7483.407398939133
2023-02-07 18:50:18,022:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 18:50:18,022:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 18:50:18,068:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 19:04:08,676:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2556954622268677
2023-02-07 19:04:08,757:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.69s, LR: 0.00070, Train Loss: 0.3453, Train MAE: 0.3453,
                            Val Loss: 0.2561, Val MAE: 0.2557
2023-02-07 19:04:08,787:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 19:15:33,183:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.25076863169670105
2023-02-07 19:15:33,185:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 684.40s, LR: 0.00070, Train Loss: 0.1926, Train MAE: 0.1926,
                            Val Loss: 0.2512, Val MAE: 0.2508
2023-02-07 19:15:33,186:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 19:26:59,929:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17551659047603607
2023-02-07 19:26:59,931:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.74s, LR: 0.00070, Train Loss: 0.1743, Train MAE: 0.1743,
                            Val Loss: 0.1759, Val MAE: 0.1755
2023-02-07 19:26:59,931:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 19:38:28,904:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.97s, LR: 0.00070, Train Loss: 0.1649, Train MAE: 0.1649,
                            Val Loss: 0.1921, Val MAE: 0.1918
2023-02-07 19:38:28,905:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 19:50:01,225:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 692.32s, LR: 0.00070, Train Loss: 0.1591, Train MAE: 0.1591,
                            Val Loss: 0.1974, Val MAE: 0.1971
2023-02-07 19:50:01,226:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 20:01:28,676:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 687.45s, LR: 0.00070, Train Loss: 0.1540, Train MAE: 0.1540,
                            Val Loss: 0.1974, Val MAE: 0.1972
2023-02-07 20:01:28,677:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 20:12:55,600:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.92s, LR: 0.00070, Train Loss: 0.1510, Train MAE: 0.1510,
                            Val Loss: 0.1847, Val MAE: 0.1843
2023-02-07 20:12:55,602:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 20:25:23,287:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 747.68s, LR: 0.00070, Train Loss: 0.1476, Train MAE: 0.1476,
                            Val Loss: 0.1893, Val MAE: 0.1891
2023-02-07 20:25:23,288:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 20:36:47,712:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1643667221069336
2023-02-07 20:36:47,714:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 684.42s, LR: 0.00070, Train Loss: 0.1445, Train MAE: 0.1445,
                            Val Loss: 0.1647, Val MAE: 0.1644
2023-02-07 20:36:47,715:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-07 20:48:06,132:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1584208607673645
2023-02-07 20:48:06,135:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.42s, LR: 0.00070, Train Loss: 0.1437, Train MAE: 0.1437,
                            Val Loss: 0.1588, Val MAE: 0.1584
2023-02-07 20:48:06,136:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-07 20:59:33,123:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 686.99s, LR: 0.00070, Train Loss: 0.1420, Train MAE: 0.1420,
                            Val Loss: 0.1715, Val MAE: 0.1712
2023-02-07 20:59:33,154:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-07 21:11:01,663:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1472446173429489
2023-02-07 21:11:01,666:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.51s, LR: 0.00070, Train Loss: 0.1382, Train MAE: 0.1382,
                            Val Loss: 0.1475, Val MAE: 0.1472
2023-02-07 21:11:01,666:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-07 21:22:32,482:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.78s, LR: 0.00070, Train Loss: 0.1385, Train MAE: 0.1385,
                            Val Loss: 0.1756, Val MAE: 0.1754
2023-02-07 21:22:32,534:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-07 21:34:09,625:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.09s, LR: 0.00070, Train Loss: 0.1369, Train MAE: 0.1369,
                            Val Loss: 0.1640, Val MAE: 0.1638
2023-02-07 21:34:09,626:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-07 21:45:37,961:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.33s, LR: 0.00070, Train Loss: 0.1345, Train MAE: 0.1345,
                            Val Loss: 0.1553, Val MAE: 0.1550
2023-02-07 21:45:37,963:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-07 21:57:08,538:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.57s, LR: 0.00070, Train Loss: 0.1332, Train MAE: 0.1332,
                            Val Loss: 0.1553, Val MAE: 0.1550
2023-02-07 21:57:08,539:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-07 22:08:46,732:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14516890048980713
2023-02-07 22:08:46,734:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 698.19s, LR: 0.00070, Train Loss: 0.1323, Train MAE: 0.1323,
                            Val Loss: 0.1454, Val MAE: 0.1452
2023-02-07 22:08:46,735:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-07 22:20:21,908:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13832201063632965
2023-02-07 22:20:21,926:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 695.19s, LR: 0.00070, Train Loss: 0.1332, Train MAE: 0.1332,
                            Val Loss: 0.1386, Val MAE: 0.1383
2023-02-07 22:20:21,926:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-07 22:31:55,685:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 693.76s, LR: 0.00070, Train Loss: 0.1313, Train MAE: 0.1313,
                            Val Loss: 0.1523, Val MAE: 0.1520
2023-02-07 22:31:55,686:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-07 22:43:32,920:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13312959671020508
2023-02-07 22:43:32,936:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.25s, LR: 0.00070, Train Loss: 0.1301, Train MAE: 0.1301,
                            Val Loss: 0.1334, Val MAE: 0.1331
2023-02-07 22:43:32,937:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
