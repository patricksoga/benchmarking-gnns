2023-02-07 05:22:37,297:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 05:22:37,297:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:33:33,063:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:34:02,101:ogbdata.py:332 -             __init__(): Time taken: 684.8040s
2023-02-07 05:34:02,102:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:34:02,102:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:34:02,102:ogbdata.py:348 -             __init__(): [I] Data load time: 684.8047s
2023-02-07 05:34:02,102:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_95.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [95]}
2023-02-07 05:34:02,102:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:34:02,104:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:34:02,117:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:34:02,117:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:34:02,117:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:34:02,133:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:34:02,135:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:34:02,135:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 95 in [95]...
2023-02-07 05:34:02,136:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:34:02,136:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:34:02,136:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:34:02,136:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:34:03,498:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 19:40:38,818:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 19:40:38,818:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:41:55,717:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:41:55,728:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:42:40,350:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 19:42:40,351:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:43:01,384:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:43:01,384:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:45:20,206:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:45:20,207:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:56:57,286:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 19:57:31,190:ogbdata.py:332 -             __init__(): Time taken: 730.9831s
2023-02-07 19:57:31,190:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 19:57:31,190:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 19:57:31,190:ogbdata.py:348 -             __init__(): [I] Data load time: 730.9835s
2023-02-07 19:57:31,190:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_95.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [95]}
2023-02-07 19:57:31,191:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 19:57:31,237:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:57:31,469:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:57:31,469:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:57:31,469:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:57:31,553:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 19:57:31,556:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 19:57:31,556:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 95 in [95]...
2023-02-07 19:57:31,557:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:57:31,557:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:57:31,557:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:57:31,557:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:57:38,919:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 21:59:24,636:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7313.0789539813995
2023-02-07 21:59:24,677:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 21:59:24,677:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 21:59:24,878:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 22:15:57,011:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2445916086435318
2023-02-07 22:15:57,013:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 992.13s, LR: 0.00070, Train Loss: 0.3279, Train MAE: 0.3279,
                            Val Loss: 0.2449, Val MAE: 0.2446
2023-02-07 22:15:57,013:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 22:27:58,018:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1899033635854721
2023-02-07 22:27:58,020:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.01s, LR: 0.00070, Train Loss: 0.1891, Train MAE: 0.1891,
                            Val Loss: 0.1902, Val MAE: 0.1899
2023-02-07 22:27:58,021:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 22:39:42,261:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 704.24s, LR: 0.00070, Train Loss: 0.1712, Train MAE: 0.1712,
                            Val Loss: 0.2133, Val MAE: 0.2130
2023-02-07 22:39:42,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 22:51:02,236:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.97s, LR: 0.00070, Train Loss: 0.1621, Train MAE: 0.1621,
                            Val Loss: 0.2736, Val MAE: 0.2734
2023-02-07 22:51:02,238:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 23:03:18,281:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16666047275066376
2023-02-07 23:03:18,284:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 736.04s, LR: 0.00070, Train Loss: 0.1571, Train MAE: 0.1571,
                            Val Loss: 0.1670, Val MAE: 0.1667
2023-02-07 23:03:18,285:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 23:15:50,696:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.41s, LR: 0.00070, Train Loss: 0.1521, Train MAE: 0.1521,
                            Val Loss: 0.1689, Val MAE: 0.1686
2023-02-07 23:15:50,697:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 23:28:23,289:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.59s, LR: 0.00070, Train Loss: 0.1480, Train MAE: 0.1480,
                            Val Loss: 0.2036, Val MAE: 0.2034
2023-02-07 23:28:23,291:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 23:42:02,292:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14834369719028473
2023-02-07 23:42:02,295:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 819.00s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.1487, Val MAE: 0.1483
2023-02-07 23:42:02,296:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 23:53:39,633:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.34s, LR: 0.00070, Train Loss: 0.1428, Train MAE: 0.1428,
                            Val Loss: 0.1506, Val MAE: 0.1503
2023-02-07 23:53:39,635:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-08 00:06:03,840:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 744.20s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1628, Val MAE: 0.1625
2023-02-08 00:06:03,841:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-08 00:18:34,583:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.74s, LR: 0.00070, Train Loss: 0.1384, Train MAE: 0.1384,
                            Val Loss: 0.1537, Val MAE: 0.1534
2023-02-08 00:18:34,584:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-08 00:30:27,226:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 712.64s, LR: 0.00070, Train Loss: 0.1367, Train MAE: 0.1367,
                            Val Loss: 0.1674, Val MAE: 0.1672
2023-02-08 00:30:27,228:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-08 00:42:46,485:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 739.26s, LR: 0.00070, Train Loss: 0.1344, Train MAE: 0.1344,
                            Val Loss: 0.1487, Val MAE: 0.1484
2023-02-08 00:42:46,486:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-08 00:54:34,292:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14653326570987701
2023-02-08 00:54:34,295:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 707.81s, LR: 0.00070, Train Loss: 0.1339, Train MAE: 0.1339,
                            Val Loss: 0.1468, Val MAE: 0.1465
2023-02-08 00:54:34,296:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-08 01:07:07,637:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.34s, LR: 0.00070, Train Loss: 0.1313, Train MAE: 0.1313,
                            Val Loss: 0.1468, Val MAE: 0.1466
2023-02-08 01:07:07,637:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-08 01:19:40,387:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.75s, LR: 0.00070, Train Loss: 0.1295, Train MAE: 0.1295,
                            Val Loss: 0.1553, Val MAE: 0.1551
2023-02-08 01:19:40,388:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-02-08 01:31:49,355:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1429806500673294
2023-02-08 01:31:49,357:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 728.97s, LR: 0.00070, Train Loss: 0.1283, Train MAE: 0.1283,
                            Val Loss: 0.1433, Val MAE: 0.1430
2023-02-08 01:31:49,358:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-02-08 01:44:18,838:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.48s, LR: 0.00070, Train Loss: 0.1277, Train MAE: 0.1277,
                            Val Loss: 0.1511, Val MAE: 0.1509
2023-02-08 01:44:18,839:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-02-08 01:56:50,644:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1304655820131302
2023-02-08 01:56:50,646:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.81s, LR: 0.00070, Train Loss: 0.1261, Train MAE: 0.1261,
                            Val Loss: 0.1307, Val MAE: 0.1305
2023-02-08 01:56:50,647:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-02-08 02:09:23,122:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.47s, LR: 0.00070, Train Loss: 0.1256, Train MAE: 0.1256,
                            Val Loss: 0.1311, Val MAE: 0.1308
2023-02-08 02:09:23,123:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-02-08 02:21:55,684:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.56s, LR: 0.00070, Train Loss: 0.1239, Train MAE: 0.1239,
                            Val Loss: 0.1324, Val MAE: 0.1322
2023-02-08 02:21:55,684:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-02-08 02:34:27,953:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.27s, LR: 0.00070, Train Loss: 0.1235, Train MAE: 0.1235,
                            Val Loss: 0.1309, Val MAE: 0.1306
2023-02-08 02:34:27,954:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-02-08 02:46:58,867:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.91s, LR: 0.00070, Train Loss: 0.1223, Train MAE: 0.1223,
                            Val Loss: 0.1321, Val MAE: 0.1319
2023-02-08 02:46:58,868:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-02-08 03:00:21,747:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 802.88s, LR: 0.00070, Train Loss: 0.1221, Train MAE: 0.1221,
                            Val Loss: 0.1433, Val MAE: 0.1431
2023-02-08 03:00:21,749:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-02-08 03:12:21,389:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 719.64s, LR: 0.00070, Train Loss: 0.1214, Train MAE: 0.1214,
                            Val Loss: 0.1328, Val MAE: 0.1325
2023-02-08 03:12:21,390:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-02-08 03:24:50,824:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12889522314071655
2023-02-08 03:24:50,827:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.43s, LR: 0.00070, Train Loss: 0.1210, Train MAE: 0.1210,
                            Val Loss: 0.1291, Val MAE: 0.1289
2023-02-08 03:24:50,827:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-02-08 03:37:23,024:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.20s, LR: 0.00070, Train Loss: 0.1202, Train MAE: 0.1202,
                            Val Loss: 0.1412, Val MAE: 0.1409
2023-02-08 03:37:23,026:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-02-08 03:49:07,263:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1256818175315857
2023-02-08 03:49:07,266:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 704.24s, LR: 0.00070, Train Loss: 0.1199, Train MAE: 0.1199,
                            Val Loss: 0.1259, Val MAE: 0.1257
2023-02-08 03:49:07,267:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-02-08 04:01:08,539:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.27s, LR: 0.00070, Train Loss: 0.1187, Train MAE: 0.1187,
                            Val Loss: 0.1261, Val MAE: 0.1258
2023-02-08 04:01:08,540:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-02-08 04:13:38,831:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.29s, LR: 0.00070, Train Loss: 0.1181, Train MAE: 0.1181,
                            Val Loss: 0.1262, Val MAE: 0.1260
2023-02-08 04:13:38,832:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-02-08 04:26:11,114:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.28s, LR: 0.00070, Train Loss: 0.1176, Train MAE: 0.1176,
                            Val Loss: 0.1578, Val MAE: 0.1576
2023-02-08 04:26:11,115:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-02-08 04:38:42,115:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.00s, LR: 0.00070, Train Loss: 0.1164, Train MAE: 0.1164,
                            Val Loss: 0.1308, Val MAE: 0.1305
2023-02-08 04:38:42,115:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-02-08 04:50:20,113:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 698.00s, LR: 0.00070, Train Loss: 0.1162, Train MAE: 0.1162,
                            Val Loss: 0.1316, Val MAE: 0.1313
2023-02-08 04:50:20,114:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-02-08 05:02:48,510:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12520265579223633
2023-02-08 05:02:48,511:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 748.40s, LR: 0.00070, Train Loss: 0.1159, Train MAE: 0.1159,
                            Val Loss: 0.1254, Val MAE: 0.1252
2023-02-08 05:02:48,512:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-02-08 05:15:21,968:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.46s, LR: 0.00070, Train Loss: 0.1154, Train MAE: 0.1154,
                            Val Loss: 0.1307, Val MAE: 0.1305
2023-02-08 05:15:21,969:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-02-08 05:27:54,003:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.03s, LR: 0.00070, Train Loss: 0.1152, Train MAE: 0.1152,
                            Val Loss: 0.1269, Val MAE: 0.1267
2023-02-08 05:27:54,004:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-02-08 05:39:45,153:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1241261214017868
2023-02-08 05:39:45,155:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 711.15s, LR: 0.00070, Train Loss: 0.1146, Train MAE: 0.1146,
                            Val Loss: 0.1244, Val MAE: 0.1241
2023-02-08 05:39:45,155:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-02-08 05:52:18,732:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.58s, LR: 0.00070, Train Loss: 0.1143, Train MAE: 0.1143,
                            Val Loss: 0.1326, Val MAE: 0.1324
2023-02-08 05:52:18,734:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-02-08 06:04:51,643:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.91s, LR: 0.00070, Train Loss: 0.1136, Train MAE: 0.1136,
                            Val Loss: 0.1248, Val MAE: 0.1245
2023-02-08 06:04:51,644:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-02-08 06:17:03,741:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 732.10s, LR: 0.00070, Train Loss: 0.1133, Train MAE: 0.1133,
                            Val Loss: 0.1270, Val MAE: 0.1267
2023-02-08 06:17:03,743:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-02-08 06:30:09,810:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12401512265205383
2023-02-08 06:30:09,812:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.07s, LR: 0.00070, Train Loss: 0.1129, Train MAE: 0.1129,
                            Val Loss: 0.1243, Val MAE: 0.1240
2023-02-08 06:30:09,813:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-02-08 06:41:43,111:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 693.30s, LR: 0.00070, Train Loss: 0.1125, Train MAE: 0.1125,
                            Val Loss: 0.1268, Val MAE: 0.1266
2023-02-08 06:41:43,112:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-02-08 06:52:58,759:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 675.64s, LR: 0.00070, Train Loss: 0.1123, Train MAE: 0.1123,
                            Val Loss: 0.1269, Val MAE: 0.1267
2023-02-08 06:52:58,759:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-02-08 07:05:24,837:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 746.08s, LR: 0.00070, Train Loss: 0.1119, Train MAE: 0.1119,
                            Val Loss: 0.1315, Val MAE: 0.1313
2023-02-08 07:05:24,838:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-02-08 07:17:56,649:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12285348773002625
2023-02-08 07:17:56,651:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.81s, LR: 0.00070, Train Loss: 0.1111, Train MAE: 0.1111,
                            Val Loss: 0.1231, Val MAE: 0.1229
2023-02-08 07:17:56,652:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-02-08 07:30:28,861:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12231316417455673
2023-02-08 07:30:28,863:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.21s, LR: 0.00070, Train Loss: 0.1114, Train MAE: 0.1114,
                            Val Loss: 0.1225, Val MAE: 0.1223
2023-02-08 07:30:28,864:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-02-08 07:42:05,429:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12000560760498047
2023-02-08 07:42:05,431:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 696.57s, LR: 0.00070, Train Loss: 0.1105, Train MAE: 0.1105,
                            Val Loss: 0.1202, Val MAE: 0.1200
2023-02-08 07:42:05,432:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-02-08 07:54:37,546:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.11s, LR: 0.00070, Train Loss: 0.1103, Train MAE: 0.1103,
                            Val Loss: 0.1244, Val MAE: 0.1242
2023-02-08 07:54:37,547:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-02-08 08:07:11,120:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.57s, LR: 0.00070, Train Loss: 0.1098, Train MAE: 0.1098,
                            Val Loss: 0.1232, Val MAE: 0.1230
2023-02-08 08:07:11,121:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-02-08 08:19:43,636:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.51s, LR: 0.00070, Train Loss: 0.1098, Train MAE: 0.1098,
                            Val Loss: 0.1239, Val MAE: 0.1237
2023-02-08 08:19:43,637:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-02-08 08:32:13,316:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.68s, LR: 0.00070, Train Loss: 0.1095, Train MAE: 0.1095,
                            Val Loss: 0.1281, Val MAE: 0.1279
2023-02-08 08:32:13,318:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-02-08 08:44:45,689:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.37s, LR: 0.00070, Train Loss: 0.1088, Train MAE: 0.1088,
                            Val Loss: 0.1257, Val MAE: 0.1254
2023-02-08 08:44:45,690:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-02-08 08:57:05,717:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 740.03s, LR: 0.00070, Train Loss: 0.1088, Train MAE: 0.1088,
                            Val Loss: 0.1258, Val MAE: 0.1255
2023-02-08 08:57:05,717:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-02-08 09:09:38,787:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11946190893650055
2023-02-08 09:09:38,789:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.07s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1197, Val MAE: 0.1195
2023-02-08 09:09:38,790:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-02-08 09:22:12,917:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 754.13s, LR: 0.00070, Train Loss: 0.1083, Train MAE: 0.1083,
                            Val Loss: 0.1236, Val MAE: 0.1233
2023-02-08 09:22:12,918:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-02-08 09:34:42,856:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.94s, LR: 0.00070, Train Loss: 0.1080, Train MAE: 0.1080,
                            Val Loss: 0.1210, Val MAE: 0.1208
2023-02-08 09:34:42,857:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-02-08 09:46:44,568:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.71s, LR: 0.00070, Train Loss: 0.1078, Train MAE: 0.1078,
                            Val Loss: 0.1230, Val MAE: 0.1228
2023-02-08 09:46:44,569:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-02-08 09:59:29,093:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 764.52s, LR: 0.00070, Train Loss: 0.1073, Train MAE: 0.1073,
                            Val Loss: 0.1222, Val MAE: 0.1220
2023-02-08 09:59:29,095:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-02-08 10:11:20,052:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.96s, LR: 0.00070, Train Loss: 0.1073, Train MAE: 0.1073,
                            Val Loss: 0.1208, Val MAE: 0.1206
2023-02-08 10:11:20,053:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-02-08 10:22:44,027:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 683.97s, LR: 0.00070, Train Loss: 0.1073, Train MAE: 0.1073,
                            Val Loss: 0.1255, Val MAE: 0.1252
2023-02-08 10:22:44,028:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-02-08 10:34:56,453:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11920174211263657
2023-02-08 10:34:56,455:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 732.43s, LR: 0.00070, Train Loss: 0.1069, Train MAE: 0.1069,
                            Val Loss: 0.1194, Val MAE: 0.1192
2023-02-08 10:34:56,456:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-02-08 10:46:58,024:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.57s, LR: 0.00070, Train Loss: 0.1067, Train MAE: 0.1067,
                            Val Loss: 0.1207, Val MAE: 0.1205
2023-02-08 10:46:58,025:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-02-08 10:58:16,784:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11859872937202454
2023-02-08 10:58:16,787:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.76s, LR: 0.00070, Train Loss: 0.1063, Train MAE: 0.1063,
                            Val Loss: 0.1188, Val MAE: 0.1186
2023-02-08 10:58:16,788:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-02-08 11:09:35,473:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.68s, LR: 0.00070, Train Loss: 0.1061, Train MAE: 0.1061,
                            Val Loss: 0.1220, Val MAE: 0.1217
2023-02-08 11:09:35,474:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-02-08 11:20:54,810:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.33s, LR: 0.00070, Train Loss: 0.1057, Train MAE: 0.1057,
                            Val Loss: 0.1206, Val MAE: 0.1203
2023-02-08 11:20:54,811:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-02-08 11:32:14,083:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11815683543682098
2023-02-08 11:32:14,085:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.27s, LR: 0.00070, Train Loss: 0.1058, Train MAE: 0.1058,
                            Val Loss: 0.1184, Val MAE: 0.1182
2023-02-08 11:32:14,086:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-02-08 11:43:29,871:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 675.78s, LR: 0.00070, Train Loss: 0.1058, Train MAE: 0.1058,
                            Val Loss: 0.1198, Val MAE: 0.1195
2023-02-08 11:43:29,872:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-02-08 11:54:48,284:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.41s, LR: 0.00070, Train Loss: 0.1056, Train MAE: 0.1056,
                            Val Loss: 0.1197, Val MAE: 0.1194
2023-02-08 11:54:48,285:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 69/1000
2023-02-08 12:06:09,200:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.91s, LR: 0.00070, Train Loss: 0.1055, Train MAE: 0.1055,
                            Val Loss: 0.1236, Val MAE: 0.1234
2023-02-08 12:06:09,201:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 70/1000
2023-02-08 12:17:29,134:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.93s, LR: 0.00070, Train Loss: 0.1050, Train MAE: 0.1050,
                            Val Loss: 0.1190, Val MAE: 0.1188
2023-02-08 12:17:29,135:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 71/1000
2023-02-08 12:28:49,616:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.48s, LR: 0.00070, Train Loss: 0.1049, Train MAE: 0.1049,
                            Val Loss: 0.1293, Val MAE: 0.1290
2023-02-08 12:28:49,617:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 72/1000
2023-02-08 12:40:09,863:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.25s, LR: 0.00070, Train Loss: 0.1047, Train MAE: 0.1047,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-08 12:40:09,864:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 73/1000
2023-02-08 12:51:29,037:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.17s, LR: 0.00070, Train Loss: 0.1045, Train MAE: 0.1045,
                            Val Loss: 0.1215, Val MAE: 0.1213
2023-02-08 12:51:29,038:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 74/1000
2023-02-08 13:02:48,140:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.10s, LR: 0.00070, Train Loss: 0.1046, Train MAE: 0.1046,
                            Val Loss: 0.1202, Val MAE: 0.1199
2023-02-08 13:02:48,141:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 75/1000
2023-02-08 13:15:13,013:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 744.87s, LR: 0.00070, Train Loss: 0.1041, Train MAE: 0.1041,
                            Val Loss: 0.1242, Val MAE: 0.1240
2023-02-08 13:15:13,014:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 76/1000
2023-02-08 13:26:29,185:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.17s, LR: 0.00070, Train Loss: 0.1042, Train MAE: 0.1042,
                            Val Loss: 0.1191, Val MAE: 0.1188
2023-02-08 13:26:29,186:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 77/1000
2023-02-08 13:37:48,737:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11790286004543304
2023-02-08 13:37:48,742:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.55s, LR: 0.00070, Train Loss: 0.1041, Train MAE: 0.1041,
                            Val Loss: 0.1181, Val MAE: 0.1179
2023-02-08 13:37:48,743:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 78/1000
2023-02-08 13:49:08,150:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.41s, LR: 0.00070, Train Loss: 0.1040, Train MAE: 0.1040,
                            Val Loss: 0.1230, Val MAE: 0.1228
2023-02-08 13:49:08,151:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 79/1000
2023-02-08 14:00:28,383:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.23s, LR: 0.00070, Train Loss: 0.1036, Train MAE: 0.1036,
                            Val Loss: 0.1209, Val MAE: 0.1206
2023-02-08 14:00:28,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 80/1000
2023-02-08 14:11:48,701:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11789894849061966
2023-02-08 14:11:48,703:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.32s, LR: 0.00070, Train Loss: 0.1039, Train MAE: 0.1039,
                            Val Loss: 0.1182, Val MAE: 0.1179
2023-02-08 14:11:48,704:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 81/1000
2023-02-08 14:23:08,140:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.43s, LR: 0.00070, Train Loss: 0.1035, Train MAE: 0.1035,
                            Val Loss: 0.1225, Val MAE: 0.1222
2023-02-08 14:23:08,141:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 82/1000
2023-02-08 14:34:27,870:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.73s, LR: 0.00070, Train Loss: 0.1037, Train MAE: 0.1037,
                            Val Loss: 0.1188, Val MAE: 0.1186
2023-02-08 14:34:27,872:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 83/1000
2023-02-08 14:45:47,993:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.12s, LR: 0.00070, Train Loss: 0.1035, Train MAE: 0.1035,
                            Val Loss: 0.1285, Val MAE: 0.1282
2023-02-08 14:45:47,994:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 84/1000
2023-02-08 14:57:08,363:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.37s, LR: 0.00070, Train Loss: 0.1033, Train MAE: 0.1033,
                            Val Loss: 0.1192, Val MAE: 0.1189
2023-02-08 14:57:08,363:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 85/1000
2023-02-08 15:08:24,857:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.49s, LR: 0.00070, Train Loss: 0.1030, Train MAE: 0.1030,
                            Val Loss: 0.1203, Val MAE: 0.1200
2023-02-08 15:08:24,858:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 86/1000
2023-02-08 15:19:44,527:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11773943156003952
2023-02-08 15:19:44,530:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.67s, LR: 0.00070, Train Loss: 0.1028, Train MAE: 0.1028,
                            Val Loss: 0.1180, Val MAE: 0.1177
2023-02-08 15:19:44,531:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 87/1000
2023-02-08 15:31:04,119:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.59s, LR: 0.00070, Train Loss: 0.1029, Train MAE: 0.1029,
                            Val Loss: 0.1193, Val MAE: 0.1191
2023-02-08 15:31:04,121:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 88/1000
2023-02-08 15:42:25,666:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11693502217531204
2023-02-08 15:42:25,668:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 681.55s, LR: 0.00070, Train Loss: 0.1024, Train MAE: 0.1024,
                            Val Loss: 0.1172, Val MAE: 0.1169
2023-02-08 15:42:25,669:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 89/1000
2023-02-08 15:53:46,682:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 681.01s, LR: 0.00070, Train Loss: 0.1028, Train MAE: 0.1028,
                            Val Loss: 0.1216, Val MAE: 0.1214
2023-02-08 15:53:46,684:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 90/1000
2023-02-08 16:05:06,162:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.48s, LR: 0.00070, Train Loss: 0.1025, Train MAE: 0.1025,
                            Val Loss: 0.1188, Val MAE: 0.1186
2023-02-08 16:05:06,163:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 91/1000
2023-02-08 16:16:27,340:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 681.18s, LR: 0.00070, Train Loss: 0.1024, Train MAE: 0.1024,
                            Val Loss: 0.1185, Val MAE: 0.1182
2023-02-08 16:16:27,341:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 92/1000
2023-02-08 16:28:56,425:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.08s, LR: 0.00070, Train Loss: 0.1022, Train MAE: 0.1022,
                            Val Loss: 0.1186, Val MAE: 0.1183
2023-02-08 16:28:56,427:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 93/1000
2023-02-08 16:40:16,608:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.18s, LR: 0.00070, Train Loss: 0.1022, Train MAE: 0.1022,
                            Val Loss: 0.1182, Val MAE: 0.1179
2023-02-08 16:40:16,609:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 94/1000
2023-02-08 16:51:35,011:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.40s, LR: 0.00070, Train Loss: 0.1020, Train MAE: 0.1020,
                            Val Loss: 0.1180, Val MAE: 0.1178
2023-02-08 16:51:35,012:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 95/1000
2023-02-08 17:03:37,029:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 722.02s, LR: 0.00070, Train Loss: 0.1016, Train MAE: 0.1016,
                            Val Loss: 0.1173, Val MAE: 0.1170
2023-02-08 17:03:37,031:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 96/1000
2023-02-08 17:15:08,740:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11665055900812149
2023-02-08 17:15:08,743:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 691.71s, LR: 0.00070, Train Loss: 0.1018, Train MAE: 0.1018,
                            Val Loss: 0.1169, Val MAE: 0.1167
2023-02-08 17:15:08,744:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 97/1000
2023-02-08 17:27:20,197:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 731.45s, LR: 0.00070, Train Loss: 0.1018, Train MAE: 0.1018,
                            Val Loss: 0.1178, Val MAE: 0.1175
2023-02-08 17:27:20,199:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 98/1000
2023-02-08 17:38:40,913:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.71s, LR: 0.00070, Train Loss: 0.1017, Train MAE: 0.1017,
                            Val Loss: 0.1179, Val MAE: 0.1176
2023-02-08 17:38:40,915:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 99/1000
2023-02-08 17:50:01,328:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11663032323122025
2023-02-08 17:50:01,330:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.41s, LR: 0.00070, Train Loss: 0.1019, Train MAE: 0.1019,
                            Val Loss: 0.1169, Val MAE: 0.1166
2023-02-08 17:50:01,331:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 100/1000
2023-02-08 18:01:19,923:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.59s, LR: 0.00070, Train Loss: 0.1013, Train MAE: 0.1013,
                            Val Loss: 0.1175, Val MAE: 0.1172
2023-02-08 18:01:19,924:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 101/1000
2023-02-08 18:12:37,534:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.61s, LR: 0.00070, Train Loss: 0.1012, Train MAE: 0.1012,
                            Val Loss: 0.1173, Val MAE: 0.1171
2023-02-08 18:12:37,535:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 102/1000
2023-02-08 18:23:49,154:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 671.62s, LR: 0.00070, Train Loss: 0.1012, Train MAE: 0.1012,
                            Val Loss: 0.1174, Val MAE: 0.1172
2023-02-08 18:23:49,155:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 103/1000
2023-02-08 18:35:06,683:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.53s, LR: 0.00070, Train Loss: 0.1008, Train MAE: 0.1008,
                            Val Loss: 0.1177, Val MAE: 0.1174
2023-02-08 18:35:06,685:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 104/1000
2023-02-08 18:46:24,531:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.84s, LR: 0.00070, Train Loss: 0.1011, Train MAE: 0.1011,
                            Val Loss: 0.1197, Val MAE: 0.1194
2023-02-08 18:46:24,532:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 105/1000
2023-02-08 18:57:40,856:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11643291264772415
2023-02-08 18:57:40,858:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.32s, LR: 0.00070, Train Loss: 0.1009, Train MAE: 0.1009,
                            Val Loss: 0.1167, Val MAE: 0.1164
2023-02-08 18:57:40,859:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 106/1000
2023-02-08 19:08:57,122:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.26s, LR: 0.00070, Train Loss: 0.1008, Train MAE: 0.1008,
                            Val Loss: 0.1173, Val MAE: 0.1171
2023-02-08 19:08:57,123:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 107/1000
2023-02-08 19:20:16,377:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11583801358938217
2023-02-08 19:20:16,379:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.26s, LR: 0.00070, Train Loss: 0.1008, Train MAE: 0.1008,
                            Val Loss: 0.1161, Val MAE: 0.1158
2023-02-08 19:20:16,380:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 108/1000
2023-02-08 19:32:40,687:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 744.31s, LR: 0.00070, Train Loss: 0.1007, Train MAE: 0.1007,
                            Val Loss: 0.1164, Val MAE: 0.1161
2023-02-08 19:32:40,688:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 109/1000
2023-02-08 19:43:59,343:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 678.65s, LR: 0.00070, Train Loss: 0.1007, Train MAE: 0.1007,
                            Val Loss: 0.1196, Val MAE: 0.1193
2023-02-08 19:43:59,345:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 110/1000
2023-02-08 19:56:23,007:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 743.66s, LR: 0.00070, Train Loss: 0.1006, Train MAE: 0.1006,
                            Val Loss: 0.1164, Val MAE: 0.1162
2023-02-08 19:56:23,009:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 111/1000
2023-02-08 20:07:46,510:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 683.50s, LR: 0.00070, Train Loss: 0.1003, Train MAE: 0.1003,
                            Val Loss: 0.1200, Val MAE: 0.1197
2023-02-08 20:07:46,512:main_OGB_graph_regression.py:231 -   train_val_pipeline(): -----------------------------------------------------------------------------------------
2023-02-08 20:07:46,513:main_OGB_graph_regression.py:232 -   train_val_pipeline(): Max_time for training elapsed 24.00 hours, so stopping
2023-02-08 20:15:15,611:main_OGB_graph_regression.py:248 -   train_val_pipeline(): Val MAE: 0.1197
2023-02-08 20:15:15,613:main_OGB_graph_regression.py:249 -   train_val_pipeline(): Train MAE: 0.1006
2023-02-08 20:15:15,615:main_OGB_graph_regression.py:250 -   train_val_pipeline(): Best Train MAE Corresponding to Best Val MAE: inf
2023-02-08 20:15:15,616:main_OGB_graph_regression.py:251 -   train_val_pipeline(): Convergence Time (Epochs): 110.0000
2023-02-08 20:15:15,617:main_OGB_graph_regression.py:252 -   train_val_pipeline(): TOTAL TIME TAKEN: 87464.0603s
2023-02-08 20:15:15,617:main_OGB_graph_regression.py:253 -   train_val_pipeline(): AVG TIME PER EPOCH: 718.0321s
2023-02-08 20:15:15,702:main_OGB_graph_regression.py:257 -   train_val_pipeline(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-08 20:15:15,826:main_OGB_graph_regression.py:258 -   train_val_pipeline(): train history: [tensor(0.1006)]
2023-02-08 20:15:15,826:main_OGB_graph_regression.py:260 -   train_val_pipeline(): val history: [tensor(0.1197)]
