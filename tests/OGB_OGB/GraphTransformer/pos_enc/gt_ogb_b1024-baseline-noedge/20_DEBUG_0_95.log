2023-02-07 05:22:37,297:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 05:22:37,297:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 05:33:33,063:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 05:34:02,101:ogbdata.py:332 -             __init__(): Time taken: 684.8040s
2023-02-07 05:34:02,102:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 05:34:02,102:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 05:34:02,102:ogbdata.py:348 -             __init__(): [I] Data load time: 684.8047s
2023-02-07 05:34:02,102:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_95.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [95]}
2023-02-07 05:34:02,102:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 05:34:02,104:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:34:02,117:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:34:02,117:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:34:02,117:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:34:02,133:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 05:34:02,135:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 05:34:02,135:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 95 in [95]...
2023-02-07 05:34:02,136:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 05:34:02,136:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 05:34:02,136:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 05:34:02,136:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 05:34:03,498:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 19:40:38,818:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 19:40:38,818:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:41:55,717:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:41:55,728:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:42:40,350:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-02-07 19:42:40,351:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:43:01,384:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:43:01,384:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:45:20,206:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2023-02-07 19:45:20,207:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-02-07 19:56:57,286:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-02-07 19:57:31,190:ogbdata.py:332 -             __init__(): Time taken: 730.9831s
2023-02-07 19:57:31,190:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-02-07 19:57:31,190:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-02-07 19:57:31,190:ogbdata.py:348 -             __init__(): [I] Data load time: 730.9835s
2023-02-07 19:57:31,190:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'pos_enc': True, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/pos_enc/gt_ogb_b1024-baseline-noedge/20_DEBUG_0_95.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [95]}
2023-02-07 19:57:31,191:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-02-07 19:57:31,237:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:57:31,469:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:57:31,469:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:57:31,469:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:57:31,553:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-02-07 19:57:31,556:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-02-07 19:57:31,556:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 95 in [95]...
2023-02-07 19:57:31,557:pe_layer.py:99 -             __init__(): pos_enc
2023-02-07 19:57:31,557:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-02-07 19:57:31,557:pe_layer.py:196 -             __init__(): Using matrix: A
2023-02-07 19:57:31,557:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-02-07 19:57:38,919:main_OGB_graph_regression.py:53 -   train_val_pipeline(): [!] Adding Laplacian graph positional encoding.
2023-02-07 21:59:24,636:main_OGB_graph_regression.py:55 -   train_val_pipeline(): Time PE: 7313.0789539813995
2023-02-07 21:59:24,677:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-02-07 21:59:24,677:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-02-07 21:59:24,878:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-02-07 22:15:57,011:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2445916086435318
2023-02-07 22:15:57,013:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 992.13s, LR: 0.00070, Train Loss: 0.3279, Train MAE: 0.3279,
                            Val Loss: 0.2449, Val MAE: 0.2446
2023-02-07 22:15:57,013:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-02-07 22:27:58,018:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1899033635854721
2023-02-07 22:27:58,020:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 721.01s, LR: 0.00070, Train Loss: 0.1891, Train MAE: 0.1891,
                            Val Loss: 0.1902, Val MAE: 0.1899
2023-02-07 22:27:58,021:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-02-07 22:39:42,261:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 704.24s, LR: 0.00070, Train Loss: 0.1712, Train MAE: 0.1712,
                            Val Loss: 0.2133, Val MAE: 0.2130
2023-02-07 22:39:42,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-02-07 22:51:02,236:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.97s, LR: 0.00070, Train Loss: 0.1621, Train MAE: 0.1621,
                            Val Loss: 0.2736, Val MAE: 0.2734
2023-02-07 22:51:02,238:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-02-07 23:03:18,281:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16666047275066376
2023-02-07 23:03:18,284:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 736.04s, LR: 0.00070, Train Loss: 0.1571, Train MAE: 0.1571,
                            Val Loss: 0.1670, Val MAE: 0.1667
2023-02-07 23:03:18,285:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-02-07 23:15:50,696:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.41s, LR: 0.00070, Train Loss: 0.1521, Train MAE: 0.1521,
                            Val Loss: 0.1689, Val MAE: 0.1686
2023-02-07 23:15:50,697:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-02-07 23:28:23,289:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.59s, LR: 0.00070, Train Loss: 0.1480, Train MAE: 0.1480,
                            Val Loss: 0.2036, Val MAE: 0.2034
2023-02-07 23:28:23,291:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-02-07 23:42:02,292:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14834369719028473
2023-02-07 23:42:02,295:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 819.00s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.1487, Val MAE: 0.1483
2023-02-07 23:42:02,296:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-02-07 23:53:39,633:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 697.34s, LR: 0.00070, Train Loss: 0.1428, Train MAE: 0.1428,
                            Val Loss: 0.1506, Val MAE: 0.1503
2023-02-07 23:53:39,635:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-02-08 00:06:03,840:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 744.20s, LR: 0.00070, Train Loss: 0.1405, Train MAE: 0.1405,
                            Val Loss: 0.1628, Val MAE: 0.1625
2023-02-08 00:06:03,841:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-02-08 00:18:34,583:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.74s, LR: 0.00070, Train Loss: 0.1384, Train MAE: 0.1384,
                            Val Loss: 0.1537, Val MAE: 0.1534
2023-02-08 00:18:34,584:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-02-08 00:30:27,226:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 712.64s, LR: 0.00070, Train Loss: 0.1367, Train MAE: 0.1367,
                            Val Loss: 0.1674, Val MAE: 0.1672
2023-02-08 00:30:27,228:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-02-08 00:42:46,485:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 739.26s, LR: 0.00070, Train Loss: 0.1344, Train MAE: 0.1344,
                            Val Loss: 0.1487, Val MAE: 0.1484
2023-02-08 00:42:46,486:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-02-08 00:54:34,292:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14653326570987701
2023-02-08 00:54:34,295:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 707.81s, LR: 0.00070, Train Loss: 0.1339, Train MAE: 0.1339,
                            Val Loss: 0.1468, Val MAE: 0.1465
2023-02-08 00:54:34,296:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-02-08 01:07:07,637:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 753.34s, LR: 0.00070, Train Loss: 0.1313, Train MAE: 0.1313,
                            Val Loss: 0.1468, Val MAE: 0.1466
2023-02-08 01:07:07,637:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-02-08 01:19:40,387:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 752.75s, LR: 0.00070, Train Loss: 0.1295, Train MAE: 0.1295,
                            Val Loss: 0.1553, Val MAE: 0.1551
2023-02-08 01:19:40,388:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
