2023-01-23 06:47:17,779:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA TITAN X (Pascal)
2023-01-23 06:47:17,779:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-23 06:57:54,271:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-23 06:58:24,831:ogbdata.py:332 -             __init__(): Time taken: 667.0517s
2023-01-23 06:58:24,831:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-23 06:58:24,831:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-23 06:58:24,831:ogbdata.py:348 -             __init__(): [I] Data load time: 667.0520s
2023-01-23 06:58:24,832:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 32, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rand_pos_enc': True, 'rw_pos_enc': False, 'num_initials': 100, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'partial_rw_pos_enc': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': True, 'gape_softmax_init': True, 'gape_uniform_init': False, 'gape_stack_strat': '1', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/rand_pos_enc/b1024-bnorm-alt-noedge-500k-32-stoch-softinit-initials100-topn-trials/32_DEBUG_0_95.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [95]}
2023-01-23 06:58:24,832:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [95], 'save_name': 'b1024-bnorm-alt-noedge-500k-32-stoch-softinit-initials100-topn-trials', 'job_num': 32}
2023-01-23 06:58:24,839:pe_layer.py:99 -             __init__(): rand_pos_enc
2023-01-23 06:58:31,714:pe_layer.py:194 -             __init__(): Using 32 dimension positional encoding
2023-01-23 06:58:31,714:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-23 06:58:31,714:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-23 06:58:31,743:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-23 06:58:31,764:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 544241
2023-01-23 06:58:31,764:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 95 in [95]...
2023-01-23 06:58:31,765:pe_layer.py:99 -             __init__(): rand_pos_enc
2023-01-23 06:58:31,770:pe_layer.py:194 -             __init__(): Using 32 dimension positional encoding
2023-01-23 06:58:31,770:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-23 06:58:31,770:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-23 06:58:31,804:main_OGB_graph_regression.py:61 -   train_val_pipeline(): [!] Adding random automaton graph positional encoding (32).
2023-01-23 08:21:58,137:main_OGB_graph_regression.py:67 -   train_val_pipeline(): Time PE:5006.372086048126
2023-01-23 08:21:58,156:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-23 08:21:58,156:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-23 08:21:58,186:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-23 08:37:39,804:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.7887519001960754
2023-01-23 08:37:39,806:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 941.62s, LR: 0.00070, Train Loss: 0.5370, Train MAE: 0.5370,
                            Val Loss: 0.7890, Val MAE: 0.7888
2023-01-23 08:37:39,807:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-23 08:50:11,425:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.4953885078430176
2023-01-23 08:50:11,428:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.62s, LR: 0.00070, Train Loss: 0.3758, Train MAE: 0.3758,
                            Val Loss: 0.4957, Val MAE: 0.4954
2023-01-23 08:50:11,428:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-23 09:01:47,312:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 695.88s, LR: 0.00070, Train Loss: 0.3188, Train MAE: 0.3188,
                            Val Loss: 0.6097, Val MAE: 0.6097
2023-01-23 09:01:47,313:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-23 09:13:37,434:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.4392618238925934
2023-01-23 09:13:37,437:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 710.12s, LR: 0.00070, Train Loss: 0.3190, Train MAE: 0.3190,
                            Val Loss: 0.4395, Val MAE: 0.4393
2023-01-23 09:13:37,437:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-23 09:26:03,509:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.42018941044807434
2023-01-23 09:26:03,512:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 746.07s, LR: 0.00070, Train Loss: 0.2813, Train MAE: 0.2813,
                            Val Loss: 0.4204, Val MAE: 0.4202
2023-01-23 09:26:03,512:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-23 09:38:10,364:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 726.85s, LR: 0.00070, Train Loss: 0.2586, Train MAE: 0.2586,
                            Val Loss: 0.4278, Val MAE: 0.4276
2023-01-23 09:38:10,366:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-23 09:50:41,566:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.20s, LR: 0.00070, Train Loss: 0.2408, Train MAE: 0.2408,
                            Val Loss: 0.5453, Val MAE: 0.5451
2023-01-23 09:50:41,568:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-23 10:03:11,114:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.3281552493572235
2023-01-23 10:03:11,117:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.55s, LR: 0.00070, Train Loss: 0.2435, Train MAE: 0.2435,
                            Val Loss: 0.3285, Val MAE: 0.3282
2023-01-23 10:03:11,117:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-23 10:14:31,962:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.84s, LR: 0.00070, Train Loss: 0.2187, Train MAE: 0.2187,
                            Val Loss: 0.3702, Val MAE: 0.3700
2023-01-23 10:14:31,964:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-23 10:25:51,317:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.35s, LR: 0.00070, Train Loss: 0.2068, Train MAE: 0.2068,
                            Val Loss: 0.5160, Val MAE: 0.5159
2023-01-23 10:25:51,319:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-23 10:37:10,665:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.34s, LR: 0.00070, Train Loss: 0.1980, Train MAE: 0.1980,
                            Val Loss: 0.3965, Val MAE: 0.3964
2023-01-23 10:37:10,667:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-23 10:48:31,038:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 680.37s, LR: 0.00070, Train Loss: 0.1961, Train MAE: 0.1961,
                            Val Loss: 0.3388, Val MAE: 0.3386
2023-01-23 10:48:31,040:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-23 11:00:31,099:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2657831311225891
2023-01-23 11:00:31,102:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 720.06s, LR: 0.00070, Train Loss: 0.1898, Train MAE: 0.1898,
                            Val Loss: 0.2661, Val MAE: 0.2658
2023-01-23 11:00:31,103:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-23 11:13:00,985:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.88s, LR: 0.00070, Train Loss: 0.1825, Train MAE: 0.1825,
                            Val Loss: 0.3897, Val MAE: 0.3897
2023-01-23 11:13:00,986:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-23 11:25:32,371:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.38s, LR: 0.00070, Train Loss: 0.1805, Train MAE: 0.1805,
                            Val Loss: 0.3857, Val MAE: 0.3856
2023-01-23 11:25:32,372:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-23 11:38:02,299:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.93s, LR: 0.00070, Train Loss: 0.1750, Train MAE: 0.1750,
                            Val Loss: 0.3990, Val MAE: 0.3989
2023-01-23 11:38:02,301:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-23 11:50:31,923:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.62s, LR: 0.00070, Train Loss: 0.1706, Train MAE: 0.1706,
                            Val Loss: 0.3995, Val MAE: 0.3993
2023-01-23 11:50:31,925:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-23 12:03:03,261:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 751.34s, LR: 0.00070, Train Loss: 0.1711, Train MAE: 0.1711,
                            Val Loss: 0.3068, Val MAE: 0.3066
2023-01-23 12:03:03,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-23 12:15:33,819:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.55s, LR: 0.00070, Train Loss: 0.1655, Train MAE: 0.1655,
                            Val Loss: 0.2973, Val MAE: 0.2972
2023-01-23 12:15:33,820:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-23 12:28:03,615:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.79s, LR: 0.00070, Train Loss: 0.1647, Train MAE: 0.1647,
                            Val Loss: 0.3719, Val MAE: 0.3718
2023-01-23 12:28:03,616:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-23 12:40:33,384:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.77s, LR: 0.00070, Train Loss: 0.1626, Train MAE: 0.1626,
                            Val Loss: 0.2984, Val MAE: 0.2983
2023-01-23 12:40:33,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-23 12:53:04,331:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.23325836658477783
2023-01-23 12:53:04,333:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 750.95s, LR: 0.00070, Train Loss: 0.1593, Train MAE: 0.1593,
                            Val Loss: 0.2334, Val MAE: 0.2333
2023-01-23 12:53:04,334:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-23 13:05:33,972:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 749.64s, LR: 0.00070, Train Loss: 0.1585, Train MAE: 0.1585,
                            Val Loss: 0.2471, Val MAE: 0.2469
2023-01-23 13:05:33,973:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-23 13:18:01,259:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.22451944649219513
2023-01-23 13:18:01,263:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 747.29s, LR: 0.00070, Train Loss: 0.1587, Train MAE: 0.1587,
                            Val Loss: 0.2247, Val MAE: 0.2245
2023-01-23 13:18:01,263:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-23 13:29:20,894:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 679.63s, LR: 0.00070, Train Loss: 0.2088, Train MAE: 0.2088,
                            Val Loss: 0.3110, Val MAE: 0.3108
2023-01-23 13:29:20,894:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-23 13:40:38,728:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.83s, LR: 0.00070, Train Loss: 0.1565, Train MAE: 0.1565,
                            Val Loss: 0.3450, Val MAE: 0.3449
2023-01-23 13:40:38,729:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-23 13:51:55,292:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.56s, LR: 0.00070, Train Loss: 0.1531, Train MAE: 0.1531,
                            Val Loss: 0.3245, Val MAE: 0.3245
2023-01-23 13:51:55,294:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-23 14:03:29,458:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 694.16s, LR: 0.00070, Train Loss: 0.1497, Train MAE: 0.1497,
                            Val Loss: 0.3231, Val MAE: 0.3230
2023-01-23 14:03:29,459:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-23 14:14:59,855:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.39s, LR: 0.00070, Train Loss: 0.1489, Train MAE: 0.1489,
                            Val Loss: 0.4839, Val MAE: 0.4840
2023-01-23 14:14:59,857:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-23 14:26:16,146:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.29s, LR: 0.00070, Train Loss: 0.1465, Train MAE: 0.1465,
                            Val Loss: 0.3064, Val MAE: 0.3063
2023-01-23 14:26:16,147:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-23 14:37:33,441:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.21794018149375916
2023-01-23 14:37:33,444:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.30s, LR: 0.00070, Train Loss: 0.1787, Train MAE: 0.1787,
                            Val Loss: 0.2181, Val MAE: 0.2179
2023-01-23 14:37:33,444:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-23 14:48:51,109:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 677.66s, LR: 0.00070, Train Loss: 0.1591, Train MAE: 0.1591,
                            Val Loss: 0.2388, Val MAE: 0.2386
2023-01-23 14:48:51,111:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-23 15:00:07,189:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.200041264295578
2023-01-23 15:00:07,191:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.08s, LR: 0.00070, Train Loss: 0.1529, Train MAE: 0.1529,
                            Val Loss: 0.2003, Val MAE: 0.2000
2023-01-23 15:00:07,191:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-23 15:12:27,436:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 740.24s, LR: 0.00070, Train Loss: 0.1457, Train MAE: 0.1457,
                            Val Loss: 0.3087, Val MAE: 0.3086
2023-01-23 15:12:27,437:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-23 15:24:54,068:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 746.63s, LR: 0.00070, Train Loss: 0.1441, Train MAE: 0.1441,
                            Val Loss: 0.4645, Val MAE: 0.4646
2023-01-23 15:24:54,069:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-23 15:36:10,082:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1935218870639801
2023-01-23 15:36:10,084:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.01s, LR: 0.00070, Train Loss: 0.1408, Train MAE: 0.1408,
                            Val Loss: 0.1937, Val MAE: 0.1935
2023-01-23 15:36:10,084:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-23 15:47:26,604:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 676.52s, LR: 0.00070, Train Loss: 0.1401, Train MAE: 0.1401,
                            Val Loss: 0.2676, Val MAE: 0.2674
2023-01-23 15:47:26,606:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
