2023-01-23 06:47:09,326:main_utils.py:62 -            gpu_setup(): cuda available with GPU: Quadro RTX 6000
2023-01-23 06:47:09,326:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-23 06:57:00,226:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-23 06:57:35,162:ogbdata.py:332 -             __init__(): Time taken: 625.8358s
2023-01-23 06:57:35,163:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-23 06:57:35,163:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-23 06:57:35,163:ogbdata.py:348 -             __init__(): [I] Data load time: 625.8362s
2023-01-23 06:57:35,163:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 32, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rand_pos_enc': True, 'rw_pos_enc': False, 'num_initials': 100, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'partial_rw_pos_enc': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': True, 'gape_softmax_init': True, 'gape_uniform_init': False, 'gape_stack_strat': '1', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/rand_pos_enc/b1024-bnorm-alt-noedge-500k-32-stoch-softinit-initials100-topn-trials/32_DEBUG_0_41.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [41]}
2023-01-23 06:57:35,163:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'save_name': 'b1024-bnorm-alt-noedge-500k-32-stoch-softinit-initials100-topn-trials', 'job_num': 32}
2023-01-23 06:57:35,197:pe_layer.py:99 -             __init__(): rand_pos_enc
2023-01-23 06:57:37,212:pe_layer.py:194 -             __init__(): Using 32 dimension positional encoding
2023-01-23 06:57:37,212:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-23 06:57:37,212:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-23 06:57:37,230:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-23 06:57:37,237:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 544241
2023-01-23 06:57:37,237:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 41 in [41]...
2023-01-23 06:57:37,238:pe_layer.py:99 -             __init__(): rand_pos_enc
2023-01-23 06:57:37,241:pe_layer.py:194 -             __init__(): Using 32 dimension positional encoding
2023-01-23 06:57:37,242:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-23 06:57:37,242:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-23 06:57:37,258:main_OGB_graph_regression.py:61 -   train_val_pipeline(): [!] Adding random automaton graph positional encoding (32).
2023-01-23 08:07:23,177:main_OGB_graph_regression.py:67 -   train_val_pipeline(): Time PE:4185.939761161804
2023-01-23 08:07:23,280:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-23 08:07:23,280:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-23 08:07:23,291:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-23 08:18:51,533:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.5099469423294067
2023-01-23 08:18:51,535:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 688.24s, LR: 0.00070, Train Loss: 0.5008, Train MAE: 0.5008,
                            Val Loss: 0.5103, Val MAE: 0.5099
2023-01-23 08:18:51,535:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-23 08:28:17,075:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.46370866894721985
2023-01-23 08:28:17,077:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 565.54s, LR: 0.00070, Train Loss: 0.3160, Train MAE: 0.3160,
                            Val Loss: 0.4642, Val MAE: 0.4637
2023-01-23 08:28:17,078:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-23 08:38:16,933:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 599.85s, LR: 0.00070, Train Loss: 0.2665, Train MAE: 0.2665,
                            Val Loss: 0.5870, Val MAE: 0.5867
2023-01-23 08:38:16,935:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-23 08:48:07,778:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.3584299087524414
2023-01-23 08:48:07,780:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 590.84s, LR: 0.00070, Train Loss: 0.2300, Train MAE: 0.2300,
                            Val Loss: 0.3586, Val MAE: 0.3584
2023-01-23 08:48:07,781:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-23 08:57:33,374:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 565.59s, LR: 0.00070, Train Loss: 0.2118, Train MAE: 0.2118,
                            Val Loss: 0.5750, Val MAE: 0.5748
2023-01-23 08:57:33,376:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-23 09:07:39,303:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.346984326839447
2023-01-23 09:07:39,306:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 605.93s, LR: 0.00070, Train Loss: 0.1952, Train MAE: 0.1952,
                            Val Loss: 0.3473, Val MAE: 0.3470
2023-01-23 09:07:39,307:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-23 09:17:48,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 609.46s, LR: 0.00070, Train Loss: 0.1830, Train MAE: 0.1830,
                            Val Loss: 0.7032, Val MAE: 0.7029
2023-01-23 09:17:48,775:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-23 09:29:14,329:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 685.55s, LR: 0.00070, Train Loss: 0.1777, Train MAE: 0.1777,
                            Val Loss: 0.4225, Val MAE: 0.4224
2023-01-23 09:29:14,331:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-23 09:38:32,826:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.3241419494152069
2023-01-23 09:38:32,828:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 558.50s, LR: 0.00070, Train Loss: 0.1691, Train MAE: 0.1691,
                            Val Loss: 0.3243, Val MAE: 0.3241
2023-01-23 09:38:32,830:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-23 09:47:50,940:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.25634869933128357
2023-01-23 09:47:50,942:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 558.11s, LR: 0.00070, Train Loss: 0.1690, Train MAE: 0.1690,
                            Val Loss: 0.2566, Val MAE: 0.2563
2023-01-23 09:47:50,943:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-23 09:57:15,064:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 564.12s, LR: 0.00070, Train Loss: 0.1613, Train MAE: 0.1613,
                            Val Loss: 0.3345, Val MAE: 0.3343
2023-01-23 09:57:15,066:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-23 10:06:38,509:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.24903370440006256
2023-01-23 10:06:38,511:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 563.44s, LR: 0.00070, Train Loss: 0.1581, Train MAE: 0.1581,
                            Val Loss: 0.2492, Val MAE: 0.2490
2023-01-23 10:06:38,512:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-23 10:16:45,154:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 606.64s, LR: 0.00070, Train Loss: 0.1549, Train MAE: 0.1549,
                            Val Loss: 0.5614, Val MAE: 0.5613
2023-01-23 10:16:45,156:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-23 10:26:19,915:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 574.76s, LR: 0.00070, Train Loss: 0.1532, Train MAE: 0.1532,
                            Val Loss: 0.2885, Val MAE: 0.2884
2023-01-23 10:26:19,917:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-23 10:35:42,813:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 562.89s, LR: 0.00070, Train Loss: 0.1507, Train MAE: 0.1507,
                            Val Loss: 0.2567, Val MAE: 0.2565
2023-01-23 10:35:42,815:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-23 10:45:05,930:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 563.11s, LR: 0.00070, Train Loss: 0.1625, Train MAE: 0.1625,
                            Val Loss: 0.2747, Val MAE: 0.2746
2023-01-23 10:45:05,932:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-23 10:54:37,405:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1928628534078598
2023-01-23 10:54:37,407:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 571.47s, LR: 0.00070, Train Loss: 0.1456, Train MAE: 0.1456,
                            Val Loss: 0.1930, Val MAE: 0.1929
2023-01-23 10:54:37,409:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-23 11:03:58,571:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 561.16s, LR: 0.00070, Train Loss: 0.1435, Train MAE: 0.1435,
                            Val Loss: 0.2594, Val MAE: 0.2593
2023-01-23 11:03:58,572:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-23 11:13:11,006:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 552.43s, LR: 0.00070, Train Loss: 0.1412, Train MAE: 0.1412,
                            Val Loss: 0.1951, Val MAE: 0.1949
2023-01-23 11:13:11,007:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-23 11:22:14,477:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 543.47s, LR: 0.00070, Train Loss: 0.1391, Train MAE: 0.1391,
                            Val Loss: 0.2475, Val MAE: 0.2474
2023-01-23 11:22:14,477:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-23 11:31:35,512:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 561.03s, LR: 0.00070, Train Loss: 0.1393, Train MAE: 0.1393,
                            Val Loss: 0.3193, Val MAE: 0.3193
2023-01-23 11:31:35,512:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-23 11:40:56,840:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 561.33s, LR: 0.00070, Train Loss: 0.1358, Train MAE: 0.1358,
                            Val Loss: 0.2607, Val MAE: 0.2607
2023-01-23 11:40:56,842:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-23 11:50:18,592:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 561.75s, LR: 0.00070, Train Loss: 0.1347, Train MAE: 0.1347,
                            Val Loss: 0.2468, Val MAE: 0.2468
2023-01-23 11:50:18,593:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-23 12:01:10,407:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 651.81s, LR: 0.00070, Train Loss: 0.1346, Train MAE: 0.1346,
                            Val Loss: 0.2581, Val MAE: 0.2580
2023-01-23 12:01:10,408:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-23 12:10:31,642:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 561.23s, LR: 0.00070, Train Loss: 0.1316, Train MAE: 0.1316,
                            Val Loss: 0.1978, Val MAE: 0.1977
2023-01-23 12:10:31,644:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-23 12:19:42,734:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 551.09s, LR: 0.00070, Train Loss: 0.1300, Train MAE: 0.1300,
                            Val Loss: 0.1984, Val MAE: 0.1982
2023-01-23 12:19:42,735:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-23 12:28:49,476:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 546.74s, LR: 0.00070, Train Loss: 0.1299, Train MAE: 0.1299,
                            Val Loss: 0.2147, Val MAE: 0.2146
2023-01-23 12:28:49,478:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-23 12:38:04,095:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 554.62s, LR: 0.00070, Train Loss: 0.1276, Train MAE: 0.1276,
                            Val Loss: 0.2058, Val MAE: 0.2056
2023-01-23 12:38:04,097:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-23 12:47:24,099:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 560.00s, LR: 0.00070, Train Loss: 0.1269, Train MAE: 0.1269,
                            Val Loss: 0.1967, Val MAE: 0.1966
2023-01-23 12:47:24,101:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-23 12:56:44,931:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 560.83s, LR: 0.00070, Train Loss: 0.1256, Train MAE: 0.1256,
                            Val Loss: 0.2805, Val MAE: 0.2803
2023-01-23 12:56:44,931:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-23 13:05:58,453:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 553.52s, LR: 0.00070, Train Loss: 0.1253, Train MAE: 0.1253,
                            Val Loss: 0.2001, Val MAE: 0.2000
2023-01-23 13:05:58,453:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-23 13:15:17,604:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.14240127801895142
2023-01-23 13:15:17,605:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 559.15s, LR: 0.00070, Train Loss: 0.1237, Train MAE: 0.1237,
                            Val Loss: 0.1426, Val MAE: 0.1424
2023-01-23 13:15:17,605:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-23 13:24:37,372:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1380884349346161
2023-01-23 13:24:37,374:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 559.77s, LR: 0.00070, Train Loss: 0.1238, Train MAE: 0.1238,
                            Val Loss: 0.1383, Val MAE: 0.1381
2023-01-23 13:24:37,376:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-23 13:33:37,849:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 540.47s, LR: 0.00070, Train Loss: 0.1219, Train MAE: 0.1219,
                            Val Loss: 0.2626, Val MAE: 0.2625
2023-01-23 13:33:37,850:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-23 13:42:43,774:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 545.92s, LR: 0.00070, Train Loss: 0.1215, Train MAE: 0.1215,
                            Val Loss: 0.2701, Val MAE: 0.2700
2023-01-23 13:42:43,775:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-23 13:51:43,766:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 539.99s, LR: 0.00070, Train Loss: 0.1204, Train MAE: 0.1204,
                            Val Loss: 0.2091, Val MAE: 0.2090
2023-01-23 13:51:43,767:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-23 14:01:02,402:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 558.63s, LR: 0.00070, Train Loss: 0.1195, Train MAE: 0.1195,
                            Val Loss: 0.2026, Val MAE: 0.2025
2023-01-23 14:01:02,403:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-01-23 14:11:04,764:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 602.36s, LR: 0.00070, Train Loss: 0.1217, Train MAE: 0.1217,
                            Val Loss: 0.1650, Val MAE: 0.1649
2023-01-23 14:11:04,765:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-01-23 14:21:06,206:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 601.44s, LR: 0.00070, Train Loss: 0.1195, Train MAE: 0.1195,
                            Val Loss: 0.3964, Val MAE: 0.3965
2023-01-23 14:21:06,207:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-01-23 14:31:09,703:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 603.49s, LR: 0.00070, Train Loss: 0.1176, Train MAE: 0.1176,
                            Val Loss: 0.2185, Val MAE: 0.2184
2023-01-23 14:31:09,704:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-01-23 14:42:40,225:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 690.52s, LR: 0.00070, Train Loss: 0.1168, Train MAE: 0.1168,
                            Val Loss: 0.1691, Val MAE: 0.1690
2023-01-23 14:42:40,226:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-01-23 14:52:28,302:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 588.08s, LR: 0.00070, Train Loss: 0.1165, Train MAE: 0.1165,
                            Val Loss: 0.1834, Val MAE: 0.1833
2023-01-23 14:52:28,304:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-01-23 15:01:37,816:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 549.51s, LR: 0.00070, Train Loss: 0.1153, Train MAE: 0.1153,
                            Val Loss: 0.1862, Val MAE: 0.1860
2023-01-23 15:01:37,817:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-01-23 15:10:19,572:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 521.75s, LR: 0.00070, Train Loss: 0.1150, Train MAE: 0.1150,
                            Val Loss: 0.2829, Val MAE: 0.2829
2023-01-23 15:10:19,574:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-01-23 15:19:16,286:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 536.71s, LR: 0.00070, Train Loss: 0.1145, Train MAE: 0.1145,
                            Val Loss: 0.2674, Val MAE: 0.2674
2023-01-23 15:19:16,288:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-01-23 15:27:53,588:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 517.30s, LR: 0.00070, Train Loss: 0.1137, Train MAE: 0.1137,
                            Val Loss: 0.4329, Val MAE: 0.4329
2023-01-23 15:27:53,589:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-01-23 15:36:28,373:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 514.78s, LR: 0.00070, Train Loss: 0.1131, Train MAE: 0.1131,
                            Val Loss: 0.1571, Val MAE: 0.1569
2023-01-23 15:36:28,375:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-01-23 15:45:28,729:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 540.35s, LR: 0.00070, Train Loss: 0.1128, Train MAE: 0.1128,
                            Val Loss: 0.1485, Val MAE: 0.1483
2023-01-23 15:45:28,730:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
