2023-01-22 02:25:25,906:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-01-22 02:25:25,906:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-22 02:36:29,926:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-22 02:37:07,069:ogbdata.py:332 -             __init__(): Time taken: 701.1632s
2023-01-22 02:37:07,070:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-22 02:37:07,070:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-22 02:37:07,070:ogbdata.py:348 -             __init__(): [I] Data load time: 701.1636s
2023-01-22 02:37:07,070:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': True, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/partial_rw_pos_enc/gt_ogb_b1024-prwpe-20-noedge/20_DEBUG_0_22.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [22]}
2023-01-22 02:37:07,070:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [22], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-01-22 02:37:07,072:pe_layer.py:99 -             __init__(): partial_rw_pos_enc
2023-01-22 02:37:07,085:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-01-22 02:37:07,085:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:37:07,085:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:37:07,101:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-22 02:37:07,104:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-01-22 02:37:07,105:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 22 in [22]...
2023-01-22 02:37:07,105:pe_layer.py:99 -             __init__(): partial_rw_pos_enc
2023-01-22 02:37:07,105:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-01-22 02:37:07,105:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:37:07,105:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:37:08,394:main_OGB_graph_regression.py:78 -   train_val_pipeline(): [!] Adding partial random walk graph positional encoding (20).
2023-01-22 02:37:08,394:positional_encs.py:134 - add_rw_pos_encodings(): Adding PE to train graphs...
2023-01-22 06:26:48,139:positional_encs.py:136 - add_rw_pos_encodings(): Adding PE to val graphs...
2023-01-22 06:31:47,367:main_OGB_graph_regression.py:80 -   train_val_pipeline(): Time PE:14080.261974334717
2023-01-22 06:31:47,466:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-22 06:31:47,467:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-22 06:31:47,477:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-22 06:48:10,205:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2696549892425537
2023-01-22 06:48:10,207:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 982.73s, LR: 0.00070, Train Loss: 0.3193, Train MAE: 0.3193,
                            Val Loss: 0.2699, Val MAE: 0.2697
2023-01-22 06:48:10,207:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-22 07:01:31,349:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17844323813915253
2023-01-22 07:01:31,351:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 801.14s, LR: 0.00070, Train Loss: 0.1842, Train MAE: 0.1842,
                            Val Loss: 0.1788, Val MAE: 0.1784
2023-01-22 07:01:31,351:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-22 07:15:16,969:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 825.62s, LR: 0.00070, Train Loss: 0.1703, Train MAE: 0.1703,
                            Val Loss: 0.2299, Val MAE: 0.2295
2023-01-22 07:15:16,970:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-22 07:28:55,933:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17227831482887268
2023-01-22 07:28:55,935:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 818.96s, LR: 0.00070, Train Loss: 0.1627, Train MAE: 0.1627,
                            Val Loss: 0.1726, Val MAE: 0.1723
2023-01-22 07:28:55,935:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-22 07:42:46,168:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1665884107351303
2023-01-22 07:42:46,169:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.23s, LR: 0.00070, Train Loss: 0.1559, Train MAE: 0.1559,
                            Val Loss: 0.1669, Val MAE: 0.1666
2023-01-22 07:42:46,169:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-22 07:56:12,914:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 806.74s, LR: 0.00070, Train Loss: 0.1534, Train MAE: 0.1534,
                            Val Loss: 0.1906, Val MAE: 0.1904
2023-01-22 07:56:12,914:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-22 08:09:56,050:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16541467607021332
2023-01-22 08:09:56,051:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.14s, LR: 0.00070, Train Loss: 0.1510, Train MAE: 0.1510,
                            Val Loss: 0.1657, Val MAE: 0.1654
2023-01-22 08:09:56,051:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-22 08:24:31,772:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1601986587047577
2023-01-22 08:24:31,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 875.72s, LR: 0.00070, Train Loss: 0.1473, Train MAE: 0.1473,
                            Val Loss: 0.1605, Val MAE: 0.1602
2023-01-22 08:24:31,773:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-22 08:38:15,500:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.73s, LR: 0.00070, Train Loss: 0.1631, Train MAE: 0.1631,
                            Val Loss: 0.2829, Val MAE: 0.2827
2023-01-22 08:38:15,501:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-22 08:52:02,569:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 827.07s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.2147, Val MAE: 0.2145
2023-01-22 08:52:02,569:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-22 09:06:06,916:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 844.35s, LR: 0.00070, Train Loss: 0.1414, Train MAE: 0.1414,
                            Val Loss: 0.2075, Val MAE: 0.2073
2023-01-22 09:06:06,917:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-22 09:19:57,503:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13831113278865814
2023-01-22 09:19:57,504:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.59s, LR: 0.00070, Train Loss: 0.1412, Train MAE: 0.1412,
                            Val Loss: 0.1385, Val MAE: 0.1383
2023-01-22 09:19:57,505:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-22 09:34:03,141:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 845.64s, LR: 0.00070, Train Loss: 0.1467, Train MAE: 0.1467,
                            Val Loss: 0.1740, Val MAE: 0.1738
2023-01-22 09:34:03,142:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-22 09:47:48,583:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 825.44s, LR: 0.00070, Train Loss: 0.1508, Train MAE: 0.1508,
                            Val Loss: 0.1554, Val MAE: 0.1552
2023-01-22 09:47:48,583:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-22 10:01:05,416:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.83s, LR: 0.00070, Train Loss: 0.1449, Train MAE: 0.1449,
                            Val Loss: 0.1609, Val MAE: 0.1608
2023-01-22 10:01:05,416:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-22 10:14:48,843:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.43s, LR: 0.00070, Train Loss: 0.1409, Train MAE: 0.1409,
                            Val Loss: 0.1519, Val MAE: 0.1517
2023-01-22 10:14:48,844:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-22 10:28:42,365:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 833.52s, LR: 0.00070, Train Loss: 0.1371, Train MAE: 0.1371,
                            Val Loss: 0.1536, Val MAE: 0.1535
2023-01-22 10:28:42,366:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-22 10:42:38,001:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 835.64s, LR: 0.00070, Train Loss: 0.1345, Train MAE: 0.1345,
                            Val Loss: 0.1552, Val MAE: 0.1550
2023-01-22 10:42:38,002:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-22 10:56:04,113:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 806.11s, LR: 0.00070, Train Loss: 0.1326, Train MAE: 0.1326,
                            Val Loss: 0.1712, Val MAE: 0.1711
2023-01-22 10:56:04,114:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-22 11:10:03,609:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 839.49s, LR: 0.00070, Train Loss: 0.1304, Train MAE: 0.1304,
                            Val Loss: 0.1678, Val MAE: 0.1677
2023-01-22 11:10:03,610:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-22 11:24:03,960:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13680453598499298
2023-01-22 11:24:03,962:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.35s, LR: 0.00070, Train Loss: 0.1288, Train MAE: 0.1288,
                            Val Loss: 0.1370, Val MAE: 0.1368
2023-01-22 11:24:03,962:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-22 11:37:45,032:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 821.07s, LR: 0.00070, Train Loss: 0.1267, Train MAE: 0.1267,
                            Val Loss: 0.1541, Val MAE: 0.1540
2023-01-22 11:37:45,033:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-22 11:51:01,938:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.91s, LR: 0.00070, Train Loss: 0.1273, Train MAE: 0.1273,
                            Val Loss: 0.1377, Val MAE: 0.1375
2023-01-22 11:51:01,939:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-22 12:05:42,307:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13137175142765045
2023-01-22 12:05:42,308:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 880.37s, LR: 0.00070, Train Loss: 0.1245, Train MAE: 0.1245,
                            Val Loss: 0.1315, Val MAE: 0.1314
2023-01-22 12:05:42,308:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-22 12:19:42,586:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.28s, LR: 0.00070, Train Loss: 0.1240, Train MAE: 0.1240,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-01-22 12:19:42,587:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-22 12:33:42,596:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.01s, LR: 0.00070, Train Loss: 0.1235, Train MAE: 0.1235,
                            Val Loss: 0.1470, Val MAE: 0.1469
2023-01-22 12:33:42,597:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-22 12:47:12,709:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12779735028743744
2023-01-22 12:47:12,710:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 810.11s, LR: 0.00070, Train Loss: 0.1223, Train MAE: 0.1223,
                            Val Loss: 0.1280, Val MAE: 0.1278
2023-01-22 12:47:12,710:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-22 13:00:28,878:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.17s, LR: 0.00070, Train Loss: 0.1215, Train MAE: 0.1215,
                            Val Loss: 0.1354, Val MAE: 0.1352
2023-01-22 13:00:28,879:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-22 13:14:15,950:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 827.07s, LR: 0.00070, Train Loss: 0.1208, Train MAE: 0.1208,
                            Val Loss: 0.1344, Val MAE: 0.1343
2023-01-22 13:14:15,950:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-22 13:28:15,346:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 839.40s, LR: 0.00070, Train Loss: 0.1205, Train MAE: 0.1205,
                            Val Loss: 0.1362, Val MAE: 0.1361
2023-01-22 13:28:15,347:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-22 13:41:39,975:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12727580964565277
2023-01-22 13:41:39,976:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 804.63s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1274, Val MAE: 0.1273
2023-01-22 13:41:39,977:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-22 13:54:48,762:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.78s, LR: 0.00070, Train Loss: 0.1198, Train MAE: 0.1198,
                            Val Loss: 0.1333, Val MAE: 0.1332
2023-01-22 13:54:48,763:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-22 14:07:58,383:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1267566680908203
2023-01-22 14:07:58,384:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.62s, LR: 0.00070, Train Loss: 0.1192, Train MAE: 0.1192,
                            Val Loss: 0.1269, Val MAE: 0.1268
2023-01-22 14:07:58,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-22 14:21:07,302:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.92s, LR: 0.00070, Train Loss: 0.1185, Train MAE: 0.1185,
                            Val Loss: 0.1353, Val MAE: 0.1351
2023-01-22 14:21:07,303:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-22 14:34:16,286:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.98s, LR: 0.00070, Train Loss: 0.1189, Train MAE: 0.1189,
                            Val Loss: 0.1270, Val MAE: 0.1269
2023-01-22 14:34:16,287:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-22 14:47:26,154:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.87s, LR: 0.00070, Train Loss: 0.1180, Train MAE: 0.1180,
                            Val Loss: 0.1306, Val MAE: 0.1304
2023-01-22 14:47:26,154:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-22 15:00:50,682:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12349960207939148
2023-01-22 15:00:50,683:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 804.53s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1237, Val MAE: 0.1235
2023-01-22 15:00:50,684:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-01-22 15:14:08,021:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 797.34s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1362, Val MAE: 0.1360
2023-01-22 15:14:08,021:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-01-22 15:27:17,359:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.34s, LR: 0.00070, Train Loss: 0.1168, Train MAE: 0.1168,
                            Val Loss: 0.1243, Val MAE: 0.1241
2023-01-22 15:27:17,359:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-01-22 15:40:26,087:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11983447521924973
2023-01-22 15:40:26,089:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.73s, LR: 0.00070, Train Loss: 0.1163, Train MAE: 0.1163,
                            Val Loss: 0.1200, Val MAE: 0.1198
2023-01-22 15:40:26,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-01-22 15:54:35,880:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 849.79s, LR: 0.00070, Train Loss: 0.1167, Train MAE: 0.1167,
                            Val Loss: 0.1259, Val MAE: 0.1257
2023-01-22 15:54:35,880:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-01-22 16:07:45,173:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.29s, LR: 0.00070, Train Loss: 0.1154, Train MAE: 0.1154,
                            Val Loss: 0.1220, Val MAE: 0.1218
2023-01-22 16:07:45,174:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-01-22 16:20:53,226:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.05s, LR: 0.00070, Train Loss: 0.1156, Train MAE: 0.1156,
                            Val Loss: 0.1330, Val MAE: 0.1328
2023-01-22 16:20:53,227:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-01-22 16:34:01,767:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.54s, LR: 0.00070, Train Loss: 0.1150, Train MAE: 0.1150,
                            Val Loss: 0.1224, Val MAE: 0.1223
2023-01-22 16:34:01,767:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-01-22 16:47:10,103:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.34s, LR: 0.00070, Train Loss: 0.1143, Train MAE: 0.1143,
                            Val Loss: 0.1227, Val MAE: 0.1226
2023-01-22 16:47:10,104:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-01-22 17:00:17,095:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.99s, LR: 0.00070, Train Loss: 0.1139, Train MAE: 0.1139,
                            Val Loss: 0.1215, Val MAE: 0.1213
2023-01-22 17:00:17,095:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-01-22 17:13:52,847:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 815.75s, LR: 0.00070, Train Loss: 0.1133, Train MAE: 0.1133,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-01-22 17:13:52,848:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-01-22 17:27:14,431:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11962693184614182
2023-01-22 17:27:14,432:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 801.58s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1198, Val MAE: 0.1196
2023-01-22 17:27:14,433:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-01-22 17:40:21,883:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.45s, LR: 0.00070, Train Loss: 0.1131, Train MAE: 0.1131,
                            Val Loss: 0.1208, Val MAE: 0.1206
2023-01-22 17:40:21,884:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-01-22 17:53:29,594:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.71s, LR: 0.00070, Train Loss: 0.1128, Train MAE: 0.1128,
                            Val Loss: 0.1238, Val MAE: 0.1236
2023-01-22 17:53:29,595:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-01-22 18:06:37,344:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.75s, LR: 0.00070, Train Loss: 0.1124, Train MAE: 0.1124,
                            Val Loss: 0.1279, Val MAE: 0.1278
2023-01-22 18:06:37,345:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-01-22 18:19:44,392:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.05s, LR: 0.00070, Train Loss: 0.1113, Train MAE: 0.1113,
                            Val Loss: 0.1275, Val MAE: 0.1274
2023-01-22 18:19:44,393:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-01-22 18:32:51,499:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.11s, LR: 0.00070, Train Loss: 0.1112, Train MAE: 0.1112,
                            Val Loss: 0.1249, Val MAE: 0.1247
2023-01-22 18:32:51,500:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
