2023-01-22 02:25:25,906:main_utils.py:62 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX TITAN X
2023-01-22 02:25:25,906:ogbdata.py:297 -             __init__(): [I] Loading dataset OGB...
2023-01-22 02:36:29,926:ogbdata.py:313 -             __init__(): Splitting dataset...
2023-01-22 02:37:07,069:ogbdata.py:332 -             __init__(): Time taken: 701.1632s
2023-01-22 02:37:07,070:ogbdata.py:346 -             __init__(): train, val sizes: 3378606,73545
2023-01-22 02:37:07,070:ogbdata.py:347 -             __init__(): [I] Finished loading.
2023-01-22 02:37:07,070:ogbdata.py:348 -             __init__(): [I] Data load time: 701.1636s
2023-01-22 02:37:07,070:main_OGB_graph_regression.py:359 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'edge_feat': False, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'pos_enc_dim': 20, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 1024, 'rw_pos_enc': False, 'partial_rw_pos_enc': True, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'OGB', 'matrix_type': 'A', 'spectral_attn': False, 'cat_gape': False, 'learned_pos_enc': False, 'gape_softmax_after': False, 'gape_softmax_before': False, 'gape_individual': False, 'random_orientation': False, 'gape_clamp': False, 'rand_sketchy_pos_enc': False, 'eigen_bartels_stewart': False, 'gape_rand': False, 'experiment_1': False, 'gape_normalization': False, 'gape_squash': 'none', 'gape_div': False, 'gape_norm': False, 'gape_symmetric': False, 'gape_weight_gen': False, 'cycles_k': 6, 'gape_scale': '0', 'gape_per_layer': False, 'gape_scalar': False, 'gape_stoch': False, 'gape_softmax_init': False, 'gape_uniform_init': False, 'gape_stack_strat': '2', 'gape_normalize_mat': False, 'gape_tau': False, 'gape_tau_mat': False, 'gape_beta': 1.0, 'gape_weight_id': False, 'gape_break_batch': False, 'ngape_betas': [], 'gape_cond_lbl': False, 'ngape_agg': 'sum', 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/OGB_OGB/GraphTransformer/partial_rw_pos_enc/gt_ogb_b1024-prwpe-20-noedge/20_DEBUG_0_22.log', 'device': device(type='cuda'), 'num_atom_type': 14, 'seed_array': [22]}
2023-01-22 02:37:07,070:main_OGB_graph_regression.py:360 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [22], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-01-22 02:37:07,072:pe_layer.py:99 -             __init__(): partial_rw_pos_enc
2023-01-22 02:37:07,085:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-01-22 02:37:07,085:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:37:07,085:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:37:07,101:main_utils.py:76 -     view_model_param(): MODEL DETAILS:

2023-01-22 02:37:07,104:main_utils.py:81 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 538421
2023-01-22 02:37:07,105:main_OGB_graph_regression.py:40 -   train_val_pipeline(): [!] Starting seed: 22 in [22]...
2023-01-22 02:37:07,105:pe_layer.py:99 -             __init__(): partial_rw_pos_enc
2023-01-22 02:37:07,105:pe_layer.py:194 -             __init__(): Using 20 dimension positional encoding
2023-01-22 02:37:07,105:pe_layer.py:196 -             __init__(): Using matrix: A
2023-01-22 02:37:07,105:pe_layer.py:197 -             __init__(): Matrix power: 1
2023-01-22 02:37:08,394:main_OGB_graph_regression.py:78 -   train_val_pipeline(): [!] Adding partial random walk graph positional encoding (20).
2023-01-22 02:37:08,394:positional_encs.py:134 - add_rw_pos_encodings(): Adding PE to train graphs...
2023-01-22 06:26:48,139:positional_encs.py:136 - add_rw_pos_encodings(): Adding PE to val graphs...
2023-01-22 06:31:47,367:main_OGB_graph_regression.py:80 -   train_val_pipeline(): Time PE:14080.261974334717
2023-01-22 06:31:47,466:main_OGB_graph_regression.py:120 -   train_val_pipeline(): Training Graphs: 3378606
2023-01-22 06:31:47,467:main_OGB_graph_regression.py:121 -   train_val_pipeline(): Validation Graphs: 73545
2023-01-22 06:31:47,477:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 1/1000
2023-01-22 06:48:10,205:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.2696549892425537
2023-01-22 06:48:10,207:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 982.73s, LR: 0.00070, Train Loss: 0.3193, Train MAE: 0.3193,
                            Val Loss: 0.2699, Val MAE: 0.2697
2023-01-22 06:48:10,207:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 2/1000
2023-01-22 07:01:31,349:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17844323813915253
2023-01-22 07:01:31,351:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 801.14s, LR: 0.00070, Train Loss: 0.1842, Train MAE: 0.1842,
                            Val Loss: 0.1788, Val MAE: 0.1784
2023-01-22 07:01:31,351:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 3/1000
2023-01-22 07:15:16,969:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 825.62s, LR: 0.00070, Train Loss: 0.1703, Train MAE: 0.1703,
                            Val Loss: 0.2299, Val MAE: 0.2295
2023-01-22 07:15:16,970:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 4/1000
2023-01-22 07:28:55,933:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.17227831482887268
2023-01-22 07:28:55,935:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 818.96s, LR: 0.00070, Train Loss: 0.1627, Train MAE: 0.1627,
                            Val Loss: 0.1726, Val MAE: 0.1723
2023-01-22 07:28:55,935:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 5/1000
2023-01-22 07:42:46,168:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1665884107351303
2023-01-22 07:42:46,169:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.23s, LR: 0.00070, Train Loss: 0.1559, Train MAE: 0.1559,
                            Val Loss: 0.1669, Val MAE: 0.1666
2023-01-22 07:42:46,169:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 6/1000
2023-01-22 07:56:12,914:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 806.74s, LR: 0.00070, Train Loss: 0.1534, Train MAE: 0.1534,
                            Val Loss: 0.1906, Val MAE: 0.1904
2023-01-22 07:56:12,914:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 7/1000
2023-01-22 08:09:56,050:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.16541467607021332
2023-01-22 08:09:56,051:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.14s, LR: 0.00070, Train Loss: 0.1510, Train MAE: 0.1510,
                            Val Loss: 0.1657, Val MAE: 0.1654
2023-01-22 08:09:56,051:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 8/1000
2023-01-22 08:24:31,772:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1601986587047577
2023-01-22 08:24:31,773:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 875.72s, LR: 0.00070, Train Loss: 0.1473, Train MAE: 0.1473,
                            Val Loss: 0.1605, Val MAE: 0.1602
2023-01-22 08:24:31,773:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 9/1000
2023-01-22 08:38:15,500:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.73s, LR: 0.00070, Train Loss: 0.1631, Train MAE: 0.1631,
                            Val Loss: 0.2829, Val MAE: 0.2827
2023-01-22 08:38:15,501:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 10/1000
2023-01-22 08:52:02,569:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 827.07s, LR: 0.00070, Train Loss: 0.1463, Train MAE: 0.1463,
                            Val Loss: 0.2147, Val MAE: 0.2145
2023-01-22 08:52:02,569:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 11/1000
2023-01-22 09:06:06,916:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 844.35s, LR: 0.00070, Train Loss: 0.1414, Train MAE: 0.1414,
                            Val Loss: 0.2075, Val MAE: 0.2073
2023-01-22 09:06:06,917:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 12/1000
2023-01-22 09:19:57,503:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13831113278865814
2023-01-22 09:19:57,504:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 830.59s, LR: 0.00070, Train Loss: 0.1412, Train MAE: 0.1412,
                            Val Loss: 0.1385, Val MAE: 0.1383
2023-01-22 09:19:57,505:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 13/1000
2023-01-22 09:34:03,141:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 845.64s, LR: 0.00070, Train Loss: 0.1467, Train MAE: 0.1467,
                            Val Loss: 0.1740, Val MAE: 0.1738
2023-01-22 09:34:03,142:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 14/1000
2023-01-22 09:47:48,583:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 825.44s, LR: 0.00070, Train Loss: 0.1508, Train MAE: 0.1508,
                            Val Loss: 0.1554, Val MAE: 0.1552
2023-01-22 09:47:48,583:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 15/1000
2023-01-22 10:01:05,416:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.83s, LR: 0.00070, Train Loss: 0.1449, Train MAE: 0.1449,
                            Val Loss: 0.1609, Val MAE: 0.1608
2023-01-22 10:01:05,416:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 16/1000
2023-01-22 10:14:48,843:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.43s, LR: 0.00070, Train Loss: 0.1409, Train MAE: 0.1409,
                            Val Loss: 0.1519, Val MAE: 0.1517
2023-01-22 10:14:48,844:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 17/1000
2023-01-22 10:28:42,365:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 833.52s, LR: 0.00070, Train Loss: 0.1371, Train MAE: 0.1371,
                            Val Loss: 0.1536, Val MAE: 0.1535
2023-01-22 10:28:42,366:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 18/1000
2023-01-22 10:42:38,001:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 835.64s, LR: 0.00070, Train Loss: 0.1345, Train MAE: 0.1345,
                            Val Loss: 0.1552, Val MAE: 0.1550
2023-01-22 10:42:38,002:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 19/1000
2023-01-22 10:56:04,113:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 806.11s, LR: 0.00070, Train Loss: 0.1326, Train MAE: 0.1326,
                            Val Loss: 0.1712, Val MAE: 0.1711
2023-01-22 10:56:04,114:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 20/1000
2023-01-22 11:10:03,609:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 839.49s, LR: 0.00070, Train Loss: 0.1304, Train MAE: 0.1304,
                            Val Loss: 0.1678, Val MAE: 0.1677
2023-01-22 11:10:03,610:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 21/1000
2023-01-22 11:24:03,960:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13680453598499298
2023-01-22 11:24:03,962:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.35s, LR: 0.00070, Train Loss: 0.1288, Train MAE: 0.1288,
                            Val Loss: 0.1370, Val MAE: 0.1368
2023-01-22 11:24:03,962:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 22/1000
2023-01-22 11:37:45,032:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 821.07s, LR: 0.00070, Train Loss: 0.1267, Train MAE: 0.1267,
                            Val Loss: 0.1541, Val MAE: 0.1540
2023-01-22 11:37:45,033:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 23/1000
2023-01-22 11:51:01,938:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.91s, LR: 0.00070, Train Loss: 0.1273, Train MAE: 0.1273,
                            Val Loss: 0.1377, Val MAE: 0.1375
2023-01-22 11:51:01,939:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 24/1000
2023-01-22 12:05:42,307:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.13137175142765045
2023-01-22 12:05:42,308:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 880.37s, LR: 0.00070, Train Loss: 0.1245, Train MAE: 0.1245,
                            Val Loss: 0.1315, Val MAE: 0.1314
2023-01-22 12:05:42,308:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 25/1000
2023-01-22 12:19:42,586:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.28s, LR: 0.00070, Train Loss: 0.1240, Train MAE: 0.1240,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-01-22 12:19:42,587:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 26/1000
2023-01-22 12:33:42,596:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 840.01s, LR: 0.00070, Train Loss: 0.1235, Train MAE: 0.1235,
                            Val Loss: 0.1470, Val MAE: 0.1469
2023-01-22 12:33:42,597:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 27/1000
2023-01-22 12:47:12,709:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12779735028743744
2023-01-22 12:47:12,710:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 810.11s, LR: 0.00070, Train Loss: 0.1223, Train MAE: 0.1223,
                            Val Loss: 0.1280, Val MAE: 0.1278
2023-01-22 12:47:12,710:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 28/1000
2023-01-22 13:00:28,878:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.17s, LR: 0.00070, Train Loss: 0.1215, Train MAE: 0.1215,
                            Val Loss: 0.1354, Val MAE: 0.1352
2023-01-22 13:00:28,879:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 29/1000
2023-01-22 13:14:15,950:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 827.07s, LR: 0.00070, Train Loss: 0.1208, Train MAE: 0.1208,
                            Val Loss: 0.1344, Val MAE: 0.1343
2023-01-22 13:14:15,950:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 30/1000
2023-01-22 13:28:15,346:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 839.40s, LR: 0.00070, Train Loss: 0.1205, Train MAE: 0.1205,
                            Val Loss: 0.1362, Val MAE: 0.1361
2023-01-22 13:28:15,347:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 31/1000
2023-01-22 13:41:39,975:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12727580964565277
2023-01-22 13:41:39,976:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 804.63s, LR: 0.00070, Train Loss: 0.1200, Train MAE: 0.1200,
                            Val Loss: 0.1274, Val MAE: 0.1273
2023-01-22 13:41:39,977:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 32/1000
2023-01-22 13:54:48,762:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.78s, LR: 0.00070, Train Loss: 0.1198, Train MAE: 0.1198,
                            Val Loss: 0.1333, Val MAE: 0.1332
2023-01-22 13:54:48,763:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 33/1000
2023-01-22 14:07:58,383:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1267566680908203
2023-01-22 14:07:58,384:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.62s, LR: 0.00070, Train Loss: 0.1192, Train MAE: 0.1192,
                            Val Loss: 0.1269, Val MAE: 0.1268
2023-01-22 14:07:58,385:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 34/1000
2023-01-22 14:21:07,302:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.92s, LR: 0.00070, Train Loss: 0.1185, Train MAE: 0.1185,
                            Val Loss: 0.1353, Val MAE: 0.1351
2023-01-22 14:21:07,303:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 35/1000
2023-01-22 14:34:16,286:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.98s, LR: 0.00070, Train Loss: 0.1189, Train MAE: 0.1189,
                            Val Loss: 0.1270, Val MAE: 0.1269
2023-01-22 14:34:16,287:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 36/1000
2023-01-22 14:47:26,154:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.87s, LR: 0.00070, Train Loss: 0.1180, Train MAE: 0.1180,
                            Val Loss: 0.1306, Val MAE: 0.1304
2023-01-22 14:47:26,154:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 37/1000
2023-01-22 15:00:50,682:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.12349960207939148
2023-01-22 15:00:50,683:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 804.53s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1237, Val MAE: 0.1235
2023-01-22 15:00:50,684:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 38/1000
2023-01-22 15:14:08,021:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 797.34s, LR: 0.00070, Train Loss: 0.1177, Train MAE: 0.1177,
                            Val Loss: 0.1362, Val MAE: 0.1360
2023-01-22 15:14:08,021:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 39/1000
2023-01-22 15:27:17,359:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.34s, LR: 0.00070, Train Loss: 0.1168, Train MAE: 0.1168,
                            Val Loss: 0.1243, Val MAE: 0.1241
2023-01-22 15:27:17,359:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 40/1000
2023-01-22 15:40:26,087:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11983447521924973
2023-01-22 15:40:26,089:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.73s, LR: 0.00070, Train Loss: 0.1163, Train MAE: 0.1163,
                            Val Loss: 0.1200, Val MAE: 0.1198
2023-01-22 15:40:26,089:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 41/1000
2023-01-22 15:54:35,880:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 849.79s, LR: 0.00070, Train Loss: 0.1167, Train MAE: 0.1167,
                            Val Loss: 0.1259, Val MAE: 0.1257
2023-01-22 15:54:35,880:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 42/1000
2023-01-22 16:07:45,173:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 789.29s, LR: 0.00070, Train Loss: 0.1154, Train MAE: 0.1154,
                            Val Loss: 0.1220, Val MAE: 0.1218
2023-01-22 16:07:45,174:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 43/1000
2023-01-22 16:20:53,226:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.05s, LR: 0.00070, Train Loss: 0.1156, Train MAE: 0.1156,
                            Val Loss: 0.1330, Val MAE: 0.1328
2023-01-22 16:20:53,227:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 44/1000
2023-01-22 16:34:01,767:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.54s, LR: 0.00070, Train Loss: 0.1150, Train MAE: 0.1150,
                            Val Loss: 0.1224, Val MAE: 0.1223
2023-01-22 16:34:01,767:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 45/1000
2023-01-22 16:47:10,103:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 788.34s, LR: 0.00070, Train Loss: 0.1143, Train MAE: 0.1143,
                            Val Loss: 0.1227, Val MAE: 0.1226
2023-01-22 16:47:10,104:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 46/1000
2023-01-22 17:00:17,095:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 786.99s, LR: 0.00070, Train Loss: 0.1139, Train MAE: 0.1139,
                            Val Loss: 0.1215, Val MAE: 0.1213
2023-01-22 17:00:17,095:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 47/1000
2023-01-22 17:13:52,847:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 815.75s, LR: 0.00070, Train Loss: 0.1133, Train MAE: 0.1133,
                            Val Loss: 0.1392, Val MAE: 0.1390
2023-01-22 17:13:52,848:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 48/1000
2023-01-22 17:27:14,431:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11962693184614182
2023-01-22 17:27:14,432:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 801.58s, LR: 0.00070, Train Loss: 0.1138, Train MAE: 0.1138,
                            Val Loss: 0.1198, Val MAE: 0.1196
2023-01-22 17:27:14,433:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 49/1000
2023-01-22 17:40:21,883:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.45s, LR: 0.00070, Train Loss: 0.1131, Train MAE: 0.1131,
                            Val Loss: 0.1208, Val MAE: 0.1206
2023-01-22 17:40:21,884:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 50/1000
2023-01-22 17:53:29,594:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.71s, LR: 0.00070, Train Loss: 0.1128, Train MAE: 0.1128,
                            Val Loss: 0.1238, Val MAE: 0.1236
2023-01-22 17:53:29,595:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 51/1000
2023-01-22 18:06:37,344:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.75s, LR: 0.00070, Train Loss: 0.1124, Train MAE: 0.1124,
                            Val Loss: 0.1279, Val MAE: 0.1278
2023-01-22 18:06:37,345:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 52/1000
2023-01-22 18:19:44,392:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.05s, LR: 0.00070, Train Loss: 0.1113, Train MAE: 0.1113,
                            Val Loss: 0.1275, Val MAE: 0.1274
2023-01-22 18:19:44,393:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 53/1000
2023-01-22 18:32:51,499:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 787.11s, LR: 0.00070, Train Loss: 0.1112, Train MAE: 0.1112,
                            Val Loss: 0.1249, Val MAE: 0.1247
2023-01-22 18:32:51,500:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 54/1000
2023-01-22 18:46:38,418:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 826.92s, LR: 0.00070, Train Loss: 0.1108, Train MAE: 0.1108,
                            Val Loss: 0.1251, Val MAE: 0.1249
2023-01-22 18:46:38,419:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 55/1000
2023-01-22 19:00:14,301:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 815.88s, LR: 0.00070, Train Loss: 0.1102, Train MAE: 0.1102,
                            Val Loss: 0.1354, Val MAE: 0.1352
2023-01-22 19:00:14,301:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 56/1000
2023-01-22 19:13:57,814:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.51s, LR: 0.00070, Train Loss: 0.1096, Train MAE: 0.1096,
                            Val Loss: 0.1253, Val MAE: 0.1252
2023-01-22 19:13:57,814:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 57/1000
2023-01-22 19:27:40,393:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11729367077350616
2023-01-22 19:27:40,394:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 822.58s, LR: 0.00070, Train Loss: 0.1097, Train MAE: 0.1097,
                            Val Loss: 0.1175, Val MAE: 0.1173
2023-01-22 19:27:40,394:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 58/1000
2023-01-22 19:42:13,370:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11601172387599945
2023-01-22 19:42:13,371:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 872.98s, LR: 0.00070, Train Loss: 0.1090, Train MAE: 0.1090,
                            Val Loss: 0.1162, Val MAE: 0.1160
2023-01-22 19:42:13,371:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 59/1000
2023-01-22 19:55:30,271:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.90s, LR: 0.00070, Train Loss: 0.1086, Train MAE: 0.1086,
                            Val Loss: 0.1297, Val MAE: 0.1295
2023-01-22 19:55:30,271:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 60/1000
2023-01-22 20:08:59,583:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 809.31s, LR: 0.00070, Train Loss: 0.1084, Train MAE: 0.1084,
                            Val Loss: 0.1222, Val MAE: 0.1220
2023-01-22 20:08:59,584:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 61/1000
2023-01-22 20:22:38,434:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 818.85s, LR: 0.00070, Train Loss: 0.1076, Train MAE: 0.1076,
                            Val Loss: 0.1255, Val MAE: 0.1253
2023-01-22 20:22:38,434:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 62/1000
2023-01-22 20:36:30,572:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 832.14s, LR: 0.00070, Train Loss: 0.1078, Train MAE: 0.1078,
                            Val Loss: 0.1198, Val MAE: 0.1196
2023-01-22 20:36:30,572:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 63/1000
2023-01-22 20:50:12,707:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 822.13s, LR: 0.00070, Train Loss: 0.1070, Train MAE: 0.1070,
                            Val Loss: 0.1175, Val MAE: 0.1173
2023-01-22 20:50:12,707:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 64/1000
2023-01-22 21:03:46,369:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 813.66s, LR: 0.00070, Train Loss: 0.1067, Train MAE: 0.1067,
                            Val Loss: 0.1163, Val MAE: 0.1161
2023-01-22 21:03:46,369:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 65/1000
2023-01-22 21:17:29,932:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 823.56s, LR: 0.00070, Train Loss: 0.1064, Train MAE: 0.1064,
                            Val Loss: 0.1190, Val MAE: 0.1189
2023-01-22 21:17:29,932:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 66/1000
2023-01-22 21:31:12,894:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11582032591104507
2023-01-22 21:31:12,895:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 822.96s, LR: 0.00070, Train Loss: 0.1063, Train MAE: 0.1063,
                            Val Loss: 0.1160, Val MAE: 0.1158
2023-01-22 21:31:12,896:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 67/1000
2023-01-22 21:44:49,526:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 816.63s, LR: 0.00070, Train Loss: 0.1055, Train MAE: 0.1055,
                            Val Loss: 0.1179, Val MAE: 0.1177
2023-01-22 21:44:49,527:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 68/1000
2023-01-22 21:58:30,301:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 820.77s, LR: 0.00070, Train Loss: 0.1057, Train MAE: 0.1057,
                            Val Loss: 0.1184, Val MAE: 0.1182
2023-01-22 21:58:30,302:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 69/1000
2023-01-22 22:12:18,364:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 828.06s, LR: 0.00070, Train Loss: 0.1052, Train MAE: 0.1052,
                            Val Loss: 0.1181, Val MAE: 0.1179
2023-01-22 22:12:18,365:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 70/1000
2023-01-22 22:25:34,669:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 796.30s, LR: 0.00070, Train Loss: 0.1050, Train MAE: 0.1050,
                            Val Loss: 0.1174, Val MAE: 0.1172
2023-01-22 22:25:34,669:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 71/1000
2023-01-22 22:38:38,782:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.11s, LR: 0.00070, Train Loss: 0.1052, Train MAE: 0.1052,
                            Val Loss: 0.1192, Val MAE: 0.1190
2023-01-22 22:38:38,782:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 72/1000
2023-01-22 22:51:42,926:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.14s, LR: 0.00070, Train Loss: 0.1047, Train MAE: 0.1047,
                            Val Loss: 0.1178, Val MAE: 0.1176
2023-01-22 22:51:42,926:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 73/1000
2023-01-22 23:04:46,835:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.1143333911895752
2023-01-22 23:04:46,836:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.91s, LR: 0.00070, Train Loss: 0.1048, Train MAE: 0.1048,
                            Val Loss: 0.1145, Val MAE: 0.1143
2023-01-22 23:04:46,836:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 74/1000
2023-01-22 23:17:50,498:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.66s, LR: 0.00070, Train Loss: 0.1039, Train MAE: 0.1039,
                            Val Loss: 0.1162, Val MAE: 0.1160
2023-01-22 23:17:50,499:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 75/1000
2023-01-22 23:31:58,125:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 847.63s, LR: 0.00070, Train Loss: 0.1036, Train MAE: 0.1036,
                            Val Loss: 0.1155, Val MAE: 0.1153
2023-01-22 23:31:58,125:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 76/1000
2023-01-22 23:45:01,975:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.85s, LR: 0.00070, Train Loss: 0.1042, Train MAE: 0.1042,
                            Val Loss: 0.1201, Val MAE: 0.1199
2023-01-22 23:45:01,975:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 77/1000
2023-01-22 23:58:05,748:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11377230286598206
2023-01-22 23:58:05,749:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.77s, LR: 0.00070, Train Loss: 0.1036, Train MAE: 0.1036,
                            Val Loss: 0.1139, Val MAE: 0.1138
2023-01-22 23:58:05,749:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 78/1000
2023-01-23 00:11:09,268:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.52s, LR: 0.00070, Train Loss: 0.1033, Train MAE: 0.1033,
                            Val Loss: 0.1153, Val MAE: 0.1151
2023-01-23 00:11:09,268:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 79/1000
2023-01-23 00:24:13,548:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.28s, LR: 0.00070, Train Loss: 0.1033, Train MAE: 0.1033,
                            Val Loss: 0.1142, Val MAE: 0.1141
2023-01-23 00:24:13,549:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 80/1000
2023-01-23 00:37:17,026:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11328418552875519
2023-01-23 00:37:17,027:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.48s, LR: 0.00070, Train Loss: 0.1031, Train MAE: 0.1031,
                            Val Loss: 0.1135, Val MAE: 0.1133
2023-01-23 00:37:17,027:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 81/1000
2023-01-23 00:50:21,034:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 784.01s, LR: 0.00070, Train Loss: 0.1030, Train MAE: 0.1030,
                            Val Loss: 0.1190, Val MAE: 0.1189
2023-01-23 00:50:21,035:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 82/1000
2023-01-23 01:03:24,503:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 783.47s, LR: 0.00070, Train Loss: 0.1024, Train MAE: 0.1024,
                            Val Loss: 0.1137, Val MAE: 0.1136
2023-01-23 01:03:24,504:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 83/1000
2023-01-23 01:16:27,101:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 782.60s, LR: 0.00070, Train Loss: 0.1026, Train MAE: 0.1026,
                            Val Loss: 0.1158, Val MAE: 0.1156
2023-01-23 01:16:27,101:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 84/1000
2023-01-23 01:29:29,303:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 782.20s, LR: 0.00070, Train Loss: 0.1021, Train MAE: 0.1021,
                            Val Loss: 0.1159, Val MAE: 0.1157
2023-01-23 01:29:29,304:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 85/1000
2023-01-23 01:42:31,548:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 782.24s, LR: 0.00070, Train Loss: 0.1022, Train MAE: 0.1022,
                            Val Loss: 0.1147, Val MAE: 0.1145
2023-01-23 01:42:31,548:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 86/1000
2023-01-23 01:55:33,460:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 781.91s, LR: 0.00070, Train Loss: 0.1018, Train MAE: 0.1018,
                            Val Loss: 0.1188, Val MAE: 0.1186
2023-01-23 01:55:33,460:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 87/1000
2023-01-23 02:08:34,676:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 781.22s, LR: 0.00070, Train Loss: 0.1020, Train MAE: 0.1020,
                            Val Loss: 0.1166, Val MAE: 0.1164
2023-01-23 02:08:34,676:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 88/1000
2023-01-23 02:21:36,517:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 781.84s, LR: 0.00070, Train Loss: 0.1017, Train MAE: 0.1017,
                            Val Loss: 0.1137, Val MAE: 0.1136
2023-01-23 02:21:36,517:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 89/1000
2023-01-23 02:34:38,177:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 781.66s, LR: 0.00070, Train Loss: 0.1017, Train MAE: 0.1017,
                            Val Loss: 0.1144, Val MAE: 0.1142
2023-01-23 02:34:38,177:main_OGB_graph_regression.py:161 -   train_val_pipeline(): Epoch 90/1000
2023-01-23 02:47:39,808:main_OGB_graph_regression.py:179 -   train_val_pipeline(): Best model with val MAE 0.11067693680524826
2023-01-23 02:47:39,809:main_OGB_graph_regression.py:205 -   train_val_pipeline(): 	Time: 781.63s, LR: 0.00070, Train Loss: 0.1016, Train MAE: 0.1016,
                            Val Loss: 0.1108, Val MAE: 0.1107
2023-01-23 02:47:39,809:main_OGB_graph_regression.py:231 -   train_val_pipeline(): -----------------------------------------------------------------------------------------
2023-01-23 02:47:39,810:main_OGB_graph_regression.py:232 -   train_val_pipeline(): Max_time for training elapsed 24.00 hours, so stopping
2023-01-23 02:57:35,895:main_OGB_graph_regression.py:248 -   train_val_pipeline(): Val MAE: 0.1107
2023-01-23 02:57:35,896:main_OGB_graph_regression.py:249 -   train_val_pipeline(): Train MAE: 0.0965
2023-01-23 02:57:35,896:main_OGB_graph_regression.py:250 -   train_val_pipeline(): Best Train MAE Corresponding to Best Val MAE: inf
2023-01-23 02:57:35,896:main_OGB_graph_regression.py:251 -   train_val_pipeline(): Convergence Time (Epochs): 89.0000
2023-01-23 02:57:35,896:main_OGB_graph_regression.py:252 -   train_val_pipeline(): TOTAL TIME TAKEN: 87628.7915s
2023-01-23 02:57:35,896:main_OGB_graph_regression.py:253 -   train_val_pipeline(): AVG TIME PER EPOCH: 810.5812s
2023-01-23 02:57:35,898:main_OGB_graph_regression.py:257 -   train_val_pipeline(): {'seed': 41, 'epochs': 1000, 'batch_size': 1024, 'init_lr': 0.0007, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 15, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [22], 'save_name': 'b128-prwpe', 'job_num': 20}
2023-01-23 02:57:35,900:main_OGB_graph_regression.py:258 -   train_val_pipeline(): train history: [tensor(0.0965)]
2023-01-23 02:57:35,900:main_OGB_graph_regression.py:260 -   train_val_pipeline(): val history: [tensor(0.1107)]
