2022-09-05 11:13:33,245:main_utils.py:39 -            gpu_setup(): cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
2022-09-05 11:13:52,466:main_SBMs_node_classification.py:344 -                 main(): {'L': 10, 'n_heads': 8, 'hidden_dim': 80, 'out_dim': 80, 'residual': True, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'layer_norm': False, 'batch_norm': True, 'self_loop': False, 'wl_pos_enc': False, 'full_graph': False, 'gpu_id': 0, 'batch_size': 26, 'rand_pos_enc': True, 'rw_pos_enc': False, 'power_method': False, 'diag': False, 'pow_of_mat': 1, 'adj_enc': False, 'dataset': 'SBM_PATTERN', 'matrix_type': 'L', 'spectral_attn': False, 'cat_gape': False, 'n_gape': 8, 'gape_softmax_after': True, 'gape_softmax_before': True, 'gape_individual': False, 'log_file': '/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/tests/SBMs_SBM_PATTERN/GraphTransformer/rand_pos_enc/b26-bnorm-alt-ngape8-softmax-lap/16_DEBUG.log', 'device': device(type='cuda'), 'pos_enc_dim': 16, 'in_dim': 3, 'n_classes': 2}
2022-09-05 11:13:52,466:main_SBMs_node_classification.py:345 -                 main(): {'seed': 41, 'epochs': 1000, 'batch_size': 26, 'init_lr': 0.0005, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 10, 'min_lr': 1e-06, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 24, 'seed_array': [41], 'job_num': 16}
2022-09-05 11:13:52,470:pe_layer.py:68 -             __init__(): rand_pos_enc
2022-09-05 11:13:53,749:pe_layer.py:132 -             __init__(): Using 16 dimension positional encoding (# states if an automata enc, otherwise smallest k eigvecs)
2022-09-05 11:13:53,749:pe_layer.py:137 -             __init__(): Using matrix: L
2022-09-05 11:13:53,749:pe_layer.py:138 -             __init__(): Matrix power: 1
2022-09-05 11:13:53,765:main_utils.py:53 -     view_model_param(): MODEL DETAILS:

2022-09-05 11:13:53,769:main_utils.py:58 -     view_model_param(): MODEL/Total parameters: GraphTransformer, 530735
2022-09-05 11:13:53,773:pe_layer.py:68 -             __init__(): rand_pos_enc
2022-09-05 11:13:53,775:pe_layer.py:132 -             __init__(): Using 16 dimension positional encoding (# states if an automata enc, otherwise smallest k eigvecs)
2022-09-05 11:13:53,775:pe_layer.py:137 -             __init__(): Using matrix: L
2022-09-05 11:13:53,775:pe_layer.py:138 -             __init__(): Matrix power: 1
2022-09-05 11:13:53,802:main_SBMs_node_classification.py:89 -   train_val_pipeline(): [!] Adding random automaton graph positional encoding (16).
2022-09-05 11:13:53,802:main_SBMs_node_classification.py:93 -   train_val_pipeline(): [!] Using 8 random automata.
2022-09-05 11:30:58,980:main_SBMs_node_classification.py:97 -   train_val_pipeline(): Time PE:1025.178344964981
2022-09-05 11:30:59,004:main_SBMs_node_classification.py:133 -   train_val_pipeline(): Training Graphs: 10000
2022-09-05 11:30:59,004:main_SBMs_node_classification.py:134 -   train_val_pipeline(): Validation Graphs: 2000
2022-09-05 11:30:59,004:main_SBMs_node_classification.py:135 -   train_val_pipeline(): Test Graphs: 2000
2022-09-05 11:30:59,004:main_SBMs_node_classification.py:136 -   train_val_pipeline(): Number of Classes: 2
2022-09-05 11:30:59,008:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 1/1000
2022-09-05 11:33:24,869:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 81.4491 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:33:24,869:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 145.86s, LR: 0.00050, Train Loss: 0.4173, Train Acc: 80.3248,
                        Val Loss: 0.4112, Val Acc: 81.2335, Test Acc: 81.4491
2022-09-05 11:33:24,869:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 2/1000
2022-09-05 11:35:41,513:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.64s, LR: 0.00050, Train Loss: 0.3510, Train Acc: 84.5317,
                        Val Loss: 0.4130, Val Acc: 81.0004, Test Acc: 81.2408
2022-09-05 11:35:41,514:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 3/1000
2022-09-05 11:37:58,486:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 82.4116 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:37:58,486:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.97s, LR: 0.00050, Train Loss: 0.3411, Train Acc: 84.9827,
                        Val Loss: 0.3923, Val Acc: 82.2123, Test Acc: 82.4116
2022-09-05 11:37:58,486:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 4/1000
2022-09-05 11:40:23,133:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 83.4863 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:40:23,133:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 144.65s, LR: 0.00050, Train Loss: 0.3381, Train Acc: 85.0914,
                        Val Loss: 0.3738, Val Acc: 83.4516, Test Acc: 83.4863
2022-09-05 11:40:23,133:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 5/1000
2022-09-05 11:42:39,666:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 84.2570 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:42:39,667:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.53s, LR: 0.00050, Train Loss: 0.3358, Train Acc: 85.1872,
                        Val Loss: 0.3581, Val Acc: 84.1122, Test Acc: 84.2570
2022-09-05 11:42:39,667:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 6/1000
2022-09-05 11:44:56,568:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 84.5403 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:44:56,568:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.90s, LR: 0.00050, Train Loss: 0.3346, Train Acc: 85.2651,
                        Val Loss: 0.3523, Val Acc: 84.4383, Test Acc: 84.5403
2022-09-05 11:44:56,568:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 7/1000
2022-09-05 11:47:14,676:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 138.11s, LR: 0.00050, Train Loss: 0.3340, Train Acc: 85.2948,
                        Val Loss: 0.3585, Val Acc: 84.2230, Test Acc: 84.4812
2022-09-05 11:47:14,677:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 8/1000
2022-09-05 11:49:40,589:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 145.91s, LR: 0.00050, Train Loss: 0.3325, Train Acc: 85.3406,
                        Val Loss: 0.3903, Val Acc: 83.2569, Test Acc: 83.3657
2022-09-05 11:49:40,589:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 9/1000
2022-09-05 11:51:57,596:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 84.8116 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:51:57,597:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.01s, LR: 0.00050, Train Loss: 0.3321, Train Acc: 85.4010,
                        Val Loss: 0.3441, Val Acc: 84.6566, Test Acc: 84.8116
2022-09-05 11:51:57,597:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 10/1000
2022-09-05 11:54:14,601:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.00s, LR: 0.00050, Train Loss: 0.3320, Train Acc: 85.3623,
                        Val Loss: 0.3874, Val Acc: 82.5063, Test Acc: 82.7226
2022-09-05 11:54:14,601:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 11/1000
2022-09-05 11:56:35,527:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 85.0682 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:56:35,527:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 140.93s, LR: 0.00050, Train Loss: 0.3314, Train Acc: 85.4087,
                        Val Loss: 0.3415, Val Acc: 84.8873, Test Acc: 85.0682
2022-09-05 11:56:35,527:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 12/1000
2022-09-05 11:58:53,753:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 85.1843 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 11:58:53,753:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 138.23s, LR: 0.00050, Train Loss: 0.3303, Train Acc: 85.4606,
                        Val Loss: 0.3401, Val Acc: 85.1286, Test Acc: 85.1843
2022-09-05 11:58:53,753:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 13/1000
2022-09-05 12:01:12,983:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 139.23s, LR: 0.00050, Train Loss: 0.3300, Train Acc: 85.5027,
                        Val Loss: 0.3495, Val Acc: 84.6788, Test Acc: 84.7301
2022-09-05 12:01:12,983:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 14/1000
2022-09-05 12:03:30,332:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.35s, LR: 0.00050, Train Loss: 0.3290, Train Acc: 85.5164,
                        Val Loss: 0.3460, Val Acc: 84.6818, Test Acc: 84.8556
2022-09-05 12:03:30,333:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 15/1000
2022-09-05 12:05:48,196:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 85.5518 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 12:05:48,197:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.86s, LR: 0.00050, Train Loss: 0.3292, Train Acc: 85.5035,
                        Val Loss: 0.3341, Val Acc: 85.2528, Test Acc: 85.5518
2022-09-05 12:05:48,197:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 16/1000
2022-09-05 12:08:04,055:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 135.86s, LR: 0.00050, Train Loss: 0.3288, Train Acc: 85.5358,
                        Val Loss: 0.3340, Val Acc: 85.2634, Test Acc: 85.3730
2022-09-05 12:08:04,056:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 17/1000
2022-09-05 12:10:23,081:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 139.02s, LR: 0.00050, Train Loss: 0.3283, Train Acc: 85.5594,
                        Val Loss: 0.3483, Val Acc: 84.4087, Test Acc: 84.5194
2022-09-05 12:10:23,081:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 18/1000
2022-09-05 12:12:53,725:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 150.64s, LR: 0.00050, Train Loss: 0.3277, Train Acc: 85.5886,
                        Val Loss: 0.3329, Val Acc: 85.3041, Test Acc: 85.5184
2022-09-05 12:12:53,726:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 19/1000
2022-09-05 12:15:13,964:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 140.24s, LR: 0.00050, Train Loss: 0.3269, Train Acc: 85.6366,
                        Val Loss: 0.3380, Val Acc: 85.2164, Test Acc: 85.3486
2022-09-05 12:15:13,964:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 20/1000
2022-09-05 12:17:30,498:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.53s, LR: 0.00050, Train Loss: 0.3267, Train Acc: 85.6545,
                        Val Loss: 0.3462, Val Acc: 84.6457, Test Acc: 84.8018
2022-09-05 12:17:30,498:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 21/1000
2022-09-05 12:19:47,445:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.95s, LR: 0.00050, Train Loss: 0.3260, Train Acc: 85.6598,
                        Val Loss: 0.3670, Val Acc: 83.5257, Test Acc: 83.7923
2022-09-05 12:19:47,445:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 22/1000
2022-09-05 12:22:11,872:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 144.43s, LR: 0.00050, Train Loss: 0.3255, Train Acc: 85.6825,
                        Val Loss: 0.3451, Val Acc: 84.8122, Test Acc: 84.7967
2022-09-05 12:22:11,872:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 23/1000
2022-09-05 12:24:29,807:main_SBMs_node_classification.py:190 -   train_val_pipeline(): Saving best model with test accuracy: 85.6952 to out/SBMs_node_classification_b26-bnorm-alt-ngape8-softmax-lapcheckpoints/GraphTransformer_SBM_PATTERN_GPU0_16_16_11h13m52s_on_Sep_05_2022/MODELS_
2022-09-05 12:24:29,807:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.93s, LR: 0.00050, Train Loss: 0.3255, Train Acc: 85.7230,
                        Val Loss: 0.3323, Val Acc: 85.5253, Test Acc: 85.6952
2022-09-05 12:24:29,807:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 24/1000
2022-09-05 12:26:46,127:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.32s, LR: 0.00050, Train Loss: 0.3244, Train Acc: 85.7364,
                        Val Loss: 0.3433, Val Acc: 84.9042, Test Acc: 85.1485
2022-09-05 12:26:46,127:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 25/1000
2022-09-05 12:29:01,985:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 135.86s, LR: 0.00050, Train Loss: 0.3242, Train Acc: 85.7309,
                        Val Loss: 0.3404, Val Acc: 85.0185, Test Acc: 85.1064
2022-09-05 12:29:01,986:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 26/1000
2022-09-05 12:31:18,427:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.44s, LR: 0.00050, Train Loss: 0.3234, Train Acc: 85.7671,
                        Val Loss: 0.3336, Val Acc: 85.4400, Test Acc: 85.4538
2022-09-05 12:31:18,427:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 27/1000
2022-09-05 12:33:35,245:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 136.82s, LR: 0.00050, Train Loss: 0.3229, Train Acc: 85.8326,
                        Val Loss: 0.3439, Val Acc: 84.8942, Test Acc: 84.9280
2022-09-05 12:33:35,246:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 28/1000
2022-09-05 12:35:52,440:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.19s, LR: 0.00050, Train Loss: 0.3221, Train Acc: 85.8314,
                        Val Loss: 0.3600, Val Acc: 83.9723, Test Acc: 84.1449
2022-09-05 12:35:52,440:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 29/1000
2022-09-05 12:38:10,923:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 138.48s, LR: 0.00050, Train Loss: 0.3219, Train Acc: 85.8438,
                        Val Loss: 0.3349, Val Acc: 85.3327, Test Acc: 85.4600
2022-09-05 12:38:10,923:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 30/1000
2022-09-05 12:40:28,704:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 137.78s, LR: 0.00050, Train Loss: 0.3208, Train Acc: 85.8904,
                        Val Loss: 0.3483, Val Acc: 84.6703, Test Acc: 84.6593
2022-09-05 12:40:28,705:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 31/1000
2022-09-05 12:42:47,313:main_SBMs_node_classification.py:212 -   train_val_pipeline(): 	Time: 138.61s, LR: 0.00050, Train Loss: 0.3199, Train Acc: 85.8956,
                        Val Loss: 0.3437, Val Acc: 84.8685, Test Acc: 85.0148
2022-09-05 12:42:47,314:main_SBMs_node_classification.py:170 -   train_val_pipeline(): Epoch 32/1000
