/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
cuda available with GPU: NVIDIA GeForce GTX TITAN X
[I] Loading dataset CYCLES...
train, test, val sizes : 9000 10000 1000
[I] Finished loading.
[I] Data load time: 14.1188s
Using learned automata position encoding
MODEL DETAILS:

MODEL/Total parameters: GatedGCN 105823
Training Graphs:  200
Validation Graphs:  1000
Test Graphs:  10000
Number of Classes:  2
Using learned automata position encoding
Epoch 1/1000
	Time: 2027.08s, LR: 0.00010, Train Loss: 0.9236, Train Acc: 0.5000,
                        Val Loss: 0.7510, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 2/1000
	Time: 1990.42s, LR: 0.00010, Train Loss: 0.6834, Train Acc: 0.4600,
                        Val Loss: 0.6947, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 3/1000
	Time: 1992.80s, LR: 0.00010, Train Loss: 0.7158, Train Acc: 0.4950,
                        Val Loss: 0.6960, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 4/1000
	Time: 1992.03s, LR: 0.00010, Train Loss: 0.7219, Train Acc: 0.5150,
                        Val Loss: 0.7013, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 5/1000
	Time: 1995.86s, LR: 0.00010, Train Loss: 0.7084, Train Acc: 0.4400,
                        Val Loss: 0.7060, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 6/1000
	Time: 1998.65s, LR: 0.00010, Train Loss: 0.7037, Train Acc: 0.5050,
                        Val Loss: 0.7131, Val Acc: 0.4980, Test Acc: 0.5010
Epoch 7/1000
	Time: 2001.84s, LR: 0.00010, Train Loss: 0.7182, Train Acc: 0.4900,
                        Val Loss: 0.7436, Val Acc: 0.4940, Test Acc: 0.4988
Epoch 8/1000
	Time: 1995.60s, LR: 0.00010, Train Loss: 0.7038, Train Acc: 0.4500,
                        Val Loss: 0.8180, Val Acc: 0.4950, Test Acc: 0.4994
Epoch 9/1000
	Time: 2006.02s, LR: 0.00010, Train Loss: 0.6973, Train Acc: 0.5400,
                        Val Loss: 0.9116, Val Acc: 0.4950, Test Acc: 0.4995
Epoch 10/1000
	Time: 1997.11s, LR: 0.00010, Train Loss: 0.7088, Train Acc: 0.4750,
                        Val Loss: 0.9595, Val Acc: 0.4950, Test Acc: 0.4988
Epoch 11/1000
	Time: 1997.35s, LR: 0.00010, Train Loss: 0.6929, Train Acc: 0.4600,
                        Val Loss: 1.0410, Val Acc: 0.4960, Test Acc: 0.4995
Epoch 12/1000
	Time: 2005.93s, LR: 0.00010, Train Loss: 0.6926, Train Acc: 0.4850,
                        Val Loss: 1.1646, Val Acc: 0.4960, Test Acc: 0.4991
Epoch 13/1000
	Time: 1989.27s, LR: 0.00010, Train Loss: 0.7043, Train Acc: 0.4600,
                        Val Loss: 1.2930, Val Acc: 0.4930, Test Acc: 0.4985
Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.
Epoch 14/1000
	Time: 1979.70s, LR: 0.00005, Train Loss: 0.6843, Train Acc: 0.5000,
                        Val Loss: 1.1193, Val Acc: 0.5000, Test Acc: 0.4998
Epoch 15/1000
	Time: 1979.42s, LR: 0.00005, Train Loss: 0.6988, Train Acc: 0.4700,
                        Val Loss: 1.0455, Val Acc: 0.4950, Test Acc: 0.4987
Epoch 16/1000
	Time: 1998.84s, LR: 0.00005, Train Loss: 0.6920, Train Acc: 0.5100,
                        Val Loss: 0.9391, Val Acc: 0.4960, Test Acc: 0.4992
Epoch 17/1000
	Time: 2003.76s, LR: 0.00005, Train Loss: 0.6965, Train Acc: 0.4850,
                        Val Loss: 0.9339, Val Acc: 0.5030, Test Acc: 0.4996
Epoch 18/1000
	Time: 2007.29s, LR: 0.00005, Train Loss: 0.7026, Train Acc: 0.5300,
                        Val Loss: 0.9521, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 19/1000
	Time: 1996.16s, LR: 0.00005, Train Loss: 0.6948, Train Acc: 0.5250,
                        Val Loss: 0.9990, Val Acc: 0.4990, Test Acc: 0.4984
Epoch 20/1000
	Time: 1986.76s, LR: 0.00005, Train Loss: 0.7003, Train Acc: 0.5000,
                        Val Loss: 1.0077, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 21/1000
	Time: 1987.23s, LR: 0.00005, Train Loss: 0.6866, Train Acc: 0.5500,
                        Val Loss: 1.0148, Val Acc: 0.5000, Test Acc: 0.4997
Epoch 22/1000
	Time: 1988.32s, LR: 0.00005, Train Loss: 0.7106, Train Acc: 0.4400,
                        Val Loss: 1.0142, Val Acc: 0.5040, Test Acc: 0.4996
-----------------------------------------------------------------------------------------
Max_time for training elapsed 12.00 hours, so stopping
Test Accuracy: 0.4996
Train Accuracy: 0.5050
Convergence Time (Epochs): 21.0000
TOTAL TIME TAKEN: 45703.7942s
AVG TIME PER EPOCH: 1996.2480s
