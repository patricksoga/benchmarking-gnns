/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
cuda available with GPU: NVIDIA GeForce GTX TITAN X
[I] Loading dataset CYCLES...
train, test, val sizes : 9000 10000 1000
[I] Finished loading.
[I] Data load time: 13.7398s
Using learned automata position encoding
MODEL DETAILS:

MODEL/Total parameters: GIN 103504
Training Graphs:  200
Validation Graphs:  1000
Test Graphs:  10000
Number of Classes:  2
Using learned automata position encoding
Epoch 1/1000
	Time: 1643.07s, LR: 0.00010, Train Loss: 16.2412, Train Acc: 0.5550,
                        Val Loss: 1.0742, Val Acc: 0.5030, Test Acc: 0.5004
Epoch 2/1000
	Time: 1636.07s, LR: 0.00010, Train Loss: 6.4692, Train Acc: 0.5550,
                        Val Loss: 7.5667, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 3/1000
	Time: 1634.69s, LR: 0.00010, Train Loss: 3.3097, Train Acc: 0.5500,
                        Val Loss: 11.0346, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 4/1000
	Time: 1635.87s, LR: 0.00010, Train Loss: 1.5800, Train Acc: 0.6150,
                        Val Loss: 12.8739, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 5/1000
	Time: 1633.62s, LR: 0.00010, Train Loss: 1.4976, Train Acc: 0.6850,
                        Val Loss: 8.3045, Val Acc: 0.5000, Test Acc: 0.5001
Epoch 6/1000
	Time: 1634.75s, LR: 0.00010, Train Loss: 1.4430, Train Acc: 0.6200,
                        Val Loss: 4.6231, Val Acc: 0.5210, Test Acc: 0.5205
Epoch 7/1000
	Time: 1633.76s, LR: 0.00010, Train Loss: 1.3266, Train Acc: 0.6800,
                        Val Loss: 2.8546, Val Acc: 0.5890, Test Acc: 0.5844
Epoch 8/1000
	Time: 1633.97s, LR: 0.00010, Train Loss: 2.0107, Train Acc: 0.6700,
                        Val Loss: 1.5743, Val Acc: 0.6780, Test Acc: 0.6788
Epoch 9/1000
	Time: 1633.94s, LR: 0.00010, Train Loss: 1.6672, Train Acc: 0.6050,
                        Val Loss: 1.6083, Val Acc: 0.6670, Test Acc: 0.6781
Epoch 10/1000
	Time: 1635.48s, LR: 0.00010, Train Loss: 1.5982, Train Acc: 0.6650,
                        Val Loss: 8.1209, Val Acc: 0.5000, Test Acc: 0.5016
Epoch 11/1000
	Time: 1635.16s, LR: 0.00010, Train Loss: 1.6042, Train Acc: 0.6500,
                        Val Loss: 3.4405, Val Acc: 0.5610, Test Acc: 0.5738
Epoch 12/1000
	Time: 1634.37s, LR: 0.00010, Train Loss: 1.7164, Train Acc: 0.6050,
                        Val Loss: 3.8182, Val Acc: 0.5690, Test Acc: 0.5842
Epoch 00012: reducing learning rate of group 0 to 5.0000e-05.
Epoch 13/1000
	Time: 1630.45s, LR: 0.00005, Train Loss: 1.3421, Train Acc: 0.6700,
                        Val Loss: 1.8926, Val Acc: 0.6240, Test Acc: 0.6473
Epoch 14/1000
	Time: 1626.93s, LR: 0.00005, Train Loss: 1.0322, Train Acc: 0.6950,
                        Val Loss: 2.6485, Val Acc: 0.4970, Test Acc: 0.5127
Epoch 15/1000
	Time: 1660.51s, LR: 0.00005, Train Loss: 1.1999, Train Acc: 0.5900,
                        Val Loss: 8.0952, Val Acc: 0.5020, Test Acc: 0.5035
Epoch 16/1000
	Time: 1698.90s, LR: 0.00005, Train Loss: 1.1956, Train Acc: 0.6600,
                        Val Loss: 2.1383, Val Acc: 0.5720, Test Acc: 0.6023
Epoch 17/1000
	Time: 1632.12s, LR: 0.00005, Train Loss: 0.9920, Train Acc: 0.6050,
                        Val Loss: 10.6885, Val Acc: 0.4940, Test Acc: 0.4993
Epoch 18/1000
	Time: 1662.35s, LR: 0.00005, Train Loss: 1.1618, Train Acc: 0.6200,
                        Val Loss: 5.8647, Val Acc: 0.5030, Test Acc: 0.5077
Epoch 19/1000
	Time: 1627.71s, LR: 0.00005, Train Loss: 1.1441, Train Acc: 0.6550,
                        Val Loss: 4.8250, Val Acc: 0.5050, Test Acc: 0.5081
Epoch 20/1000
	Time: 1618.61s, LR: 0.00005, Train Loss: 0.9413, Train Acc: 0.6500,
                        Val Loss: 2.1385, Val Acc: 0.6340, Test Acc: 0.6246
Epoch 21/1000
	Time: 1619.42s, LR: 0.00005, Train Loss: 1.1061, Train Acc: 0.6100,
                        Val Loss: 1.6173, Val Acc: 0.6100, Test Acc: 0.6168
Epoch 22/1000
	Time: 1619.31s, LR: 0.00005, Train Loss: 1.0655, Train Acc: 0.6700,
                        Val Loss: 1.5230, Val Acc: 0.5940, Test Acc: 0.5989
Epoch 23/1000
	Time: 1617.70s, LR: 0.00005, Train Loss: 1.0009, Train Acc: 0.6700,
                        Val Loss: 4.4630, Val Acc: 0.4970, Test Acc: 0.5003
Epoch 00023: reducing learning rate of group 0 to 2.5000e-05.
Epoch 24/1000
	Time: 1618.72s, LR: 0.00003, Train Loss: 0.9066, Train Acc: 0.6900,
                        Val Loss: 2.0121, Val Acc: 0.5920, Test Acc: 0.5689
Epoch 25/1000
	Time: 1618.84s, LR: 0.00003, Train Loss: 1.0208, Train Acc: 0.6350,
                        Val Loss: 1.3497, Val Acc: 0.6560, Test Acc: 0.6407
Epoch 26/1000
	Time: 1620.32s, LR: 0.00003, Train Loss: 0.7596, Train Acc: 0.6950,
                        Val Loss: 2.0414, Val Acc: 0.6010, Test Acc: 0.6226
Epoch 27/1000
	Time: 1619.75s, LR: 0.00003, Train Loss: 0.7972, Train Acc: 0.6850,
                        Val Loss: 1.5159, Val Acc: 0.6260, Test Acc: 0.6422
-----------------------------------------------------------------------------------------
Max_time for training elapsed 12.00 hours, so stopping
Test Accuracy: 0.6422
Train Accuracy: 0.6600
Convergence Time (Epochs): 26.0000
TOTAL TIME TAKEN: 45553.4792s
AVG TIME PER EPOCH: 1633.9398s
