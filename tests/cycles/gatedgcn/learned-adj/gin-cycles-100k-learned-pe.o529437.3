/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:391: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
[I] Loading dataset CYCLES...
train, test, val sizes : 9000 10000 1000
[I] Finished loading.
[I] Data load time: 14.5782s
Using learned automata position encoding
MODEL DETAILS:

MODEL/Total parameters: GatedGCN 105823
Training Graphs:  200
Validation Graphs:  1000
Test Graphs:  10000
Number of Classes:  2
Using learned automata position encoding
Epoch 1/1000
	Time: 1140.04s, LR: 0.00010, Train Loss: 0.9236, Train Acc: 0.5000,
                        Val Loss: 0.7510, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 2/1000
	Time: 1142.66s, LR: 0.00010, Train Loss: 0.6834, Train Acc: 0.4600,
                        Val Loss: 0.6947, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 3/1000
	Time: 1140.76s, LR: 0.00010, Train Loss: 0.7158, Train Acc: 0.4950,
                        Val Loss: 0.6960, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 4/1000
	Time: 1138.97s, LR: 0.00010, Train Loss: 0.7219, Train Acc: 0.5150,
                        Val Loss: 0.7013, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 5/1000
	Time: 1142.32s, LR: 0.00010, Train Loss: 0.7084, Train Acc: 0.4400,
                        Val Loss: 0.7060, Val Acc: 0.5000, Test Acc: 0.5000
Epoch 6/1000
	Time: 1141.97s, LR: 0.00010, Train Loss: 0.7037, Train Acc: 0.5050,
                        Val Loss: 0.7131, Val Acc: 0.4980, Test Acc: 0.5010
Epoch 7/1000
	Time: 1140.58s, LR: 0.00010, Train Loss: 0.7182, Train Acc: 0.4900,
                        Val Loss: 0.7436, Val Acc: 0.4940, Test Acc: 0.4988
Epoch 8/1000
	Time: 1138.93s, LR: 0.00010, Train Loss: 0.7038, Train Acc: 0.4500,
                        Val Loss: 0.8180, Val Acc: 0.4950, Test Acc: 0.4994
Epoch 9/1000
	Time: 1141.73s, LR: 0.00010, Train Loss: 0.6973, Train Acc: 0.5400,
                        Val Loss: 0.9116, Val Acc: 0.4950, Test Acc: 0.4995
Epoch 10/1000
	Time: 1147.70s, LR: 0.00010, Train Loss: 0.7088, Train Acc: 0.4750,
                        Val Loss: 0.9595, Val Acc: 0.4950, Test Acc: 0.4988
Epoch 11/1000
	Time: 1146.17s, LR: 0.00010, Train Loss: 0.6929, Train Acc: 0.4600,
                        Val Loss: 1.0410, Val Acc: 0.4960, Test Acc: 0.4995
Epoch 12/1000
	Time: 1145.70s, LR: 0.00010, Train Loss: 0.6926, Train Acc: 0.4850,
                        Val Loss: 1.1646, Val Acc: 0.4960, Test Acc: 0.4991
Epoch 13/1000
	Time: 1144.40s, LR: 0.00010, Train Loss: 0.7043, Train Acc: 0.4600,
                        Val Loss: 1.2930, Val Acc: 0.4930, Test Acc: 0.4985
Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.
Epoch 14/1000
	Time: 1146.99s, LR: 0.00005, Train Loss: 0.6843, Train Acc: 0.5000,
                        Val Loss: 1.1193, Val Acc: 0.5000, Test Acc: 0.4998
Epoch 15/1000
	Time: 1148.39s, LR: 0.00005, Train Loss: 0.6988, Train Acc: 0.4700,
                        Val Loss: 1.0455, Val Acc: 0.4950, Test Acc: 0.4987
Epoch 16/1000
	Time: 1146.68s, LR: 0.00005, Train Loss: 0.6920, Train Acc: 0.5100,
                        Val Loss: 0.9391, Val Acc: 0.4960, Test Acc: 0.4992
Epoch 17/1000
	Time: 1147.69s, LR: 0.00005, Train Loss: 0.6965, Train Acc: 0.4850,
                        Val Loss: 0.9339, Val Acc: 0.5030, Test Acc: 0.4996
Epoch 18/1000
	Time: 1146.51s, LR: 0.00005, Train Loss: 0.7026, Train Acc: 0.5300,
                        Val Loss: 0.9521, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 19/1000
	Time: 1147.53s, LR: 0.00005, Train Loss: 0.6948, Train Acc: 0.5250,
                        Val Loss: 0.9990, Val Acc: 0.4990, Test Acc: 0.4984
Epoch 20/1000
	Time: 1145.97s, LR: 0.00005, Train Loss: 0.7003, Train Acc: 0.5000,
                        Val Loss: 1.0077, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 21/1000
	Time: 1144.90s, LR: 0.00005, Train Loss: 0.6866, Train Acc: 0.5500,
                        Val Loss: 1.0148, Val Acc: 0.5000, Test Acc: 0.4997
Epoch 22/1000
	Time: 1142.16s, LR: 0.00005, Train Loss: 0.7106, Train Acc: 0.4400,
                        Val Loss: 1.0142, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 23/1000
	Time: 1139.88s, LR: 0.00005, Train Loss: 0.7034, Train Acc: 0.4900,
                        Val Loss: 1.0382, Val Acc: 0.5040, Test Acc: 0.4996
Epoch 24/1000
	Time: 1140.37s, LR: 0.00005, Train Loss: 0.6916, Train Acc: 0.5350,
                        Val Loss: 1.0616, Val Acc: 0.5050, Test Acc: 0.4991
Epoch 00024: reducing learning rate of group 0 to 2.5000e-05.
Epoch 25/1000
	Time: 1138.79s, LR: 0.00003, Train Loss: 0.6893, Train Acc: 0.6300,
                        Val Loss: 0.9673, Val Acc: 0.5050, Test Acc: 0.4995
Epoch 26/1000
	Time: 1140.11s, LR: 0.00003, Train Loss: 0.7015, Train Acc: 0.4850,
                        Val Loss: 0.8924, Val Acc: 0.5060, Test Acc: 0.4994
Epoch 27/1000
	Time: 1141.47s, LR: 0.00003, Train Loss: 0.6911, Train Acc: 0.5200,
                        Val Loss: 0.8539, Val Acc: 0.5040, Test Acc: 0.4992
Epoch 28/1000
	Time: 1140.83s, LR: 0.00003, Train Loss: 0.6893, Train Acc: 0.5200,
                        Val Loss: 0.8063, Val Acc: 0.5060, Test Acc: 0.4993
Epoch 29/1000
	Time: 1124.19s, LR: 0.00003, Train Loss: 0.6897, Train Acc: 0.5350,
                        Val Loss: 0.8327, Val Acc: 0.5060, Test Acc: 0.4997
Epoch 30/1000
	Time: 1117.91s, LR: 0.00003, Train Loss: 0.6949, Train Acc: 0.5050,
                        Val Loss: 0.8413, Val Acc: 0.5050, Test Acc: 0.4995
Epoch 31/1000
	Time: 1117.37s, LR: 0.00003, Train Loss: 0.6885, Train Acc: 0.5100,
                        Val Loss: 0.8195, Val Acc: 0.5060, Test Acc: 0.4996
Epoch 32/1000
	Time: 1122.61s, LR: 0.00003, Train Loss: 0.6880, Train Acc: 0.5000,
                        Val Loss: 0.8085, Val Acc: 0.5060, Test Acc: 0.4994
Epoch 33/1000
	Time: 1123.17s, LR: 0.00003, Train Loss: 0.6972, Train Acc: 0.4700,
                        Val Loss: 0.7996, Val Acc: 0.5050, Test Acc: 0.4997
Epoch 34/1000
	Time: 1124.76s, LR: 0.00003, Train Loss: 0.6894, Train Acc: 0.5100,
                        Val Loss: 0.8239, Val Acc: 0.5050, Test Acc: 0.5001
Epoch 35/1000
	Time: 1123.51s, LR: 0.00003, Train Loss: 0.6929, Train Acc: 0.5050,
                        Val Loss: 0.8560, Val Acc: 0.5030, Test Acc: 0.5000
Epoch 00035: reducing learning rate of group 0 to 1.2500e-05.
Epoch 36/1000
	Time: 1123.13s, LR: 0.00001, Train Loss: 0.6949, Train Acc: 0.5200,
                        Val Loss: 0.7714, Val Acc: 0.5050, Test Acc: 0.4997
Epoch 37/1000
	Time: 1120.79s, LR: 0.00001, Train Loss: 0.6808, Train Acc: 0.5400,
                        Val Loss: 0.7444, Val Acc: 0.5060, Test Acc: 0.4992
Epoch 38/1000
	Time: 1117.35s, LR: 0.00001, Train Loss: 0.6894, Train Acc: 0.4950,
                        Val Loss: 0.7192, Val Acc: 0.5050, Test Acc: 0.4994
-----------------------------------------------------------------------------------------
Max_time for training elapsed 12.00 hours, so stopping
Test Accuracy: 0.4994
Train Accuracy: 0.5000
Convergence Time (Epochs): 37.0000
TOTAL TIME TAKEN: 44221.8424s
AVG TIME PER EPOCH: 1137.5001s
