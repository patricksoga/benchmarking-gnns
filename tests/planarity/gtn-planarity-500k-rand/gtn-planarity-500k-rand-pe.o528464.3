/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_PLANARITY_graph_classification.py:383: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_PLANARITY_graph_classification.py:383: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
cuda available with GPU: NVIDIA GeForce GTX 1080 Ti
[I] Loading dataset Planarity...
train, test, val sizes : 7000 1500 1500
[I] Finished loading.
[I] Data load time: 7.1246s
MODEL DETAILS:

MODEL/Total parameters: GraphTransformer 551222
Training Graphs:  7000
Validation Graphs:  1500
Test Graphs:  1500
Number of Classes:  2
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [02:37<?, ?it/s, lr=0.0005, test_acc=0.516, time=157, train_acc=0.606, train_loss=0.673, val_acc=0.497, val_loss=0.858]Epoch 0:   0%|          | 1/1000 [02:37<43:42:55, 157.53s/it, lr=0.0005, test_acc=0.516, time=157, train_acc=0.606, train_loss=0.673, val_acc=0.497, val_loss=0.858]Epoch 1:   0%|          | 1/1000 [02:37<43:42:55, 157.53s/it, lr=0.0005, test_acc=0.516, time=157, train_acc=0.606, train_loss=0.673, val_acc=0.497, val_loss=0.858]Epoch 1:   0%|          | 1/1000 [05:10<43:42:55, 157.53s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.614, train_loss=0.661, val_acc=0.498, val_loss=0.816]Epoch 1:   0%|          | 2/1000 [05:11<43:02:35, 155.27s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.614, train_loss=0.661, val_acc=0.498, val_loss=0.816]Epoch 2:   0%|          | 2/1000 [05:11<43:02:35, 155.27s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.614, train_loss=0.661, val_acc=0.498, val_loss=0.816]Epoch 2:   0%|          | 2/1000 [07:45<43:02:35, 155.27s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.633, train_loss=0.645, val_acc=0.499, val_loss=0.841]Epoch 2:   0%|          | 3/1000 [07:45<42:49:21, 154.62s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.633, train_loss=0.645, val_acc=0.499, val_loss=0.841]Epoch 3:   0%|          | 3/1000 [07:45<42:49:21, 154.62s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.633, train_loss=0.645, val_acc=0.499, val_loss=0.841]Epoch 3:   0%|          | 3/1000 [10:18<42:49:21, 154.62s/it, lr=0.0005, test_acc=0.516, time=153, train_acc=0.625, train_loss=0.647, val_acc=0.498, val_loss=0.766]Epoch 3:   0%|          | 4/1000 [10:18<42:37:57, 154.09s/it, lr=0.0005, test_acc=0.516, time=153, train_acc=0.625, train_loss=0.647, val_acc=0.498, val_loss=0.766]Epoch 4:   0%|          | 4/1000 [10:18<42:37:57, 154.09s/it, lr=0.0005, test_acc=0.516, time=153, train_acc=0.625, train_loss=0.647, val_acc=0.498, val_loss=0.766]Epoch 4:   0%|          | 4/1000 [12:52<42:37:57, 154.09s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.624, train_loss=0.644, val_acc=0.499, val_loss=0.795]Epoch 4:   0%|          | 5/1000 [12:52<42:33:29, 153.98s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.624, train_loss=0.644, val_acc=0.499, val_loss=0.795]Epoch 5:   0%|          | 5/1000 [12:52<42:33:29, 153.98s/it, lr=0.0005, test_acc=0.517, time=154, train_acc=0.624, train_loss=0.644, val_acc=0.499, val_loss=0.795]Epoch 5:   0%|          | 5/1000 [15:25<42:33:29, 153.98s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.633, train_loss=0.641, val_acc=0.499, val_loss=0.717]Epoch 5:   1%|          | 6/1000 [15:25<42:27:54, 153.80s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.633, train_loss=0.641, val_acc=0.499, val_loss=0.717]Epoch 6:   1%|          | 6/1000 [15:25<42:27:54, 153.80s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.633, train_loss=0.641, val_acc=0.499, val_loss=0.717]Epoch 6:   1%|          | 6/1000 [17:59<42:27:54, 153.80s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.638, train_loss=0.64, val_acc=0.499, val_loss=0.79]  Epoch 6:   1%|          | 7/1000 [17:59<42:23:52, 153.71s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.638, train_loss=0.64, val_acc=0.499, val_loss=0.79]Epoch 7:   1%|          | 7/1000 [17:59<42:23:52, 153.71s/it, lr=0.0005, test_acc=0.517, time=153, train_acc=0.638, train_loss=0.64, val_acc=0.499, val_loss=0.79]Epoch 7:   1%|          | 7/1000 [20:32<42:23:52, 153.71s/it, lr=0.0005, test_acc=0.485, time=153, train_acc=0.636, train_loss=0.638, val_acc=0.503, val_loss=0.705]Epoch 7:   1%|          | 8/1000 [20:32<42:17:35, 153.48s/it, lr=0.0005, test_acc=0.485, time=153, train_acc=0.636, train_loss=0.638, val_acc=0.503, val_loss=0.705]Epoch 8:   1%|          | 8/1000 [20:32<42:17:35, 153.48s/it, lr=0.0005, test_acc=0.485, time=153, train_acc=0.636, train_loss=0.638, val_acc=0.503, val_loss=0.705]Epoch 8:   1%|          | 8/1000 [23:04<42:17:35, 153.48s/it, lr=0.0005, test_acc=0.62, time=153, train_acc=0.639, train_loss=0.638, val_acc=0.65, val_loss=0.676]  Epoch 8:   1%|          | 9/1000 [23:04<42:11:47, 153.29s/it, lr=0.0005, test_acc=0.62, time=153, train_acc=0.639, train_loss=0.638, val_acc=0.65, val_loss=0.676]Epoch 9:   1%|          | 9/1000 [23:04<42:11:47, 153.29s/it, lr=0.0005, test_acc=0.62, time=153, train_acc=0.639, train_loss=0.638, val_acc=0.65, val_loss=0.676]Epoch 9:   1%|          | 9/1000 [25:35<42:11:47, 153.29s/it, lr=0.0005, test_acc=0.517, time=150, train_acc=0.634, train_loss=0.639, val_acc=0.499, val_loss=0.777]Epoch 9:   1%|          | 10/1000 [25:35<41:55:09, 152.43s/it, lr=0.0005, test_acc=0.517, time=150, train_acc=0.634, train_loss=0.639, val_acc=0.499, val_loss=0.777]Epoch 10:   1%|          | 10/1000 [25:35<41:55:09, 152.43s/it, lr=0.0005, test_acc=0.517, time=150, train_acc=0.634, train_loss=0.639, val_acc=0.499, val_loss=0.777]Epoch 10:   1%|          | 10/1000 [28:03<41:55:09, 152.43s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.635, train_loss=0.641, val_acc=0.499, val_loss=0.772]Epoch 10:   1%|          | 11/1000 [28:03<41:28:49, 150.99s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.635, train_loss=0.641, val_acc=0.499, val_loss=0.772]Epoch 11:   1%|          | 11/1000 [28:03<41:28:49, 150.99s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.635, train_loss=0.641, val_acc=0.499, val_loss=0.772]Epoch 11:   1%|          | 11/1000 [30:30<41:28:49, 150.99s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.637, train_loss=0.639, val_acc=0.499, val_loss=0.739]Epoch 11:   1%|          | 12/1000 [30:30<41:09:35, 149.98s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.637, train_loss=0.639, val_acc=0.499, val_loss=0.739]Epoch 12:   1%|          | 12/1000 [30:30<41:09:35, 149.98s/it, lr=0.0005, test_acc=0.517, time=148, train_acc=0.637, train_loss=0.639, val_acc=0.499, val_loss=0.739]Epoch 12:   1%|          | 12/1000 [32:58<41:09:35, 149.98s/it, lr=0.0005, test_acc=0.483, time=148, train_acc=0.638, train_loss=0.636, val_acc=0.501, val_loss=0.857]Epoch 12:   1%|▏         | 13/1000 [32:58<40:57:39, 149.40s/it, lr=0.0005, test_acc=0.483, time=148, train_acc=0.638, train_loss=0.636, val_acc=0.501, val_loss=0.857]Epoch 13:   1%|▏         | 13/1000 [32:58<40:57:39, 149.40s/it, lr=0.0005, test_acc=0.483, time=148, train_acc=0.638, train_loss=0.636, val_acc=0.501, val_loss=0.857]Epoch 13:   1%|▏         | 13/1000 [35:26<40:57:39, 149.40s/it, lr=0.0005, test_acc=0.523, time=147, train_acc=0.642, train_loss=0.633, val_acc=0.507, val_loss=0.801]Epoch 13:   1%|▏         | 14/1000 [35:26<40:45:20, 148.80s/it, lr=0.0005, test_acc=0.523, time=147, train_acc=0.642, train_loss=0.633, val_acc=0.507, val_loss=0.801]Epoch 14:   1%|▏         | 14/1000 [35:26<40:45:20, 148.80s/it, lr=0.0005, test_acc=0.523, time=147, train_acc=0.642, train_loss=0.633, val_acc=0.507, val_loss=0.801]Epoch 14:   1%|▏         | 14/1000 [37:53<40:45:20, 148.80s/it, lr=0.0005, test_acc=0.529, time=147, train_acc=0.644, train_loss=0.632, val_acc=0.512, val_loss=0.705]Epoch 14:   2%|▏         | 15/1000 [37:53<40:35:15, 148.34s/it, lr=0.0005, test_acc=0.529, time=147, train_acc=0.644, train_loss=0.632, val_acc=0.512, val_loss=0.705]Epoch 15:   2%|▏         | 15/1000 [37:53<40:35:15, 148.34s/it, lr=0.0005, test_acc=0.529, time=147, train_acc=0.644, train_loss=0.632, val_acc=0.512, val_loss=0.705]Epoch 15:   2%|▏         | 15/1000 [40:22<40:35:15, 148.34s/it, lr=0.0005, test_acc=0.561, time=149, train_acc=0.649, train_loss=0.63, val_acc=0.54, val_loss=0.688]  Epoch 15:   2%|▏         | 16/1000 [40:22<40:36:59, 148.60s/it, lr=0.0005, test_acc=0.561, time=149, train_acc=0.649, train_loss=0.63, val_acc=0.54, val_loss=0.688]Epoch 16:   2%|▏         | 16/1000 [40:22<40:36:59, 148.60s/it, lr=0.0005, test_acc=0.561, time=149, train_acc=0.649, train_loss=0.63, val_acc=0.54, val_loss=0.688]Epoch 16:   2%|▏         | 16/1000 [42:57<40:36:59, 148.60s/it, lr=0.0005, test_acc=0.521, time=155, train_acc=0.639, train_loss=0.63, val_acc=0.504, val_loss=0.839]Epoch 16:   2%|▏         | 17/1000 [42:57<41:03:59, 150.40s/it, lr=0.0005, test_acc=0.521, time=155, train_acc=0.639, train_loss=0.63, val_acc=0.504, val_loss=0.839]Epoch 17:   2%|▏         | 17/1000 [42:57<41:03:59, 150.40s/it, lr=0.0005, test_acc=0.521, time=155, train_acc=0.639, train_loss=0.63, val_acc=0.504, val_loss=0.839]Epoch 17:   2%|▏         | 17/1000 [45:32<41:03:59, 150.40s/it, lr=0.0005, test_acc=0.515, time=155, train_acc=0.639, train_loss=0.634, val_acc=0.497, val_loss=0.775]Epoch 17:   2%|▏         | 18/1000 [45:32<41:23:04, 151.72s/it, lr=0.0005, test_acc=0.515, time=155, train_acc=0.639, train_loss=0.634, val_acc=0.497, val_loss=0.775]Epoch 18:   2%|▏         | 18/1000 [45:32<41:23:04, 151.72s/it, lr=0.0005, test_acc=0.515, time=155, train_acc=0.639, train_loss=0.634, val_acc=0.497, val_loss=0.775]Epoch 18:   2%|▏         | 18/1000 [48:06<41:23:04, 151.72s/it, lr=0.0005, test_acc=0.555, time=154, train_acc=0.641, train_loss=0.633, val_acc=0.536, val_loss=0.699]Epoch 18:   2%|▏         | 19/1000 [48:06<41:31:39, 152.40s/it, lr=0.0005, test_acc=0.555, time=154, train_acc=0.641, train_loss=0.633, val_acc=0.536, val_loss=0.699]Epoch 19:   2%|▏         | 19/1000 [48:06<41:31:39, 152.40s/it, lr=0.0005, test_acc=0.555, time=154, train_acc=0.641, train_loss=0.633, val_acc=0.536, val_loss=0.699]Epoch 19:   2%|▏         | 19/1000 [50:39<41:31:39, 152.40s/it, lr=0.0005, test_acc=0.515, time=154, train_acc=0.639, train_loss=0.629, val_acc=0.497, val_loss=0.798]Epoch 19:   2%|▏         | 20/1000 [50:39<41:36:00, 152.82s/it, lr=0.0005, test_acc=0.515, time=154, train_acc=0.639, train_loss=0.629, val_acc=0.497, val_loss=0.798]Epoch 20:   2%|▏         | 20/1000 [50:39<41:36:00, 152.82s/it, lr=0.0005, test_acc=0.515, time=154, train_acc=0.639, train_loss=0.629, val_acc=0.497, val_loss=0.798]Epoch 20:   2%|▏         | 20/1000 [53:13<41:36:00, 152.82s/it, lr=0.00025, test_acc=0.515, time=154, train_acc=0.648, train_loss=0.625, val_acc=0.497, val_loss=1.24]Epoch 20:   2%|▏         | 21/1000 [53:13<41:37:35, 153.07s/it, lr=0.00025, test_acc=0.515, time=154, train_acc=0.648, train_loss=0.625, val_acc=0.497, val_loss=1.24]Epoch 21:   2%|▏         | 21/1000 [53:13<41:37:35, 153.07s/it, lr=0.00025, test_acc=0.515, time=154, train_acc=0.648, train_loss=0.625, val_acc=0.497, val_loss=1.24]Epoch 21:   2%|▏         | 21/1000 [53:54<41:52:55, 154.01s/it, lr=0.00025, test_acc=0.515, time=154, train_acc=0.648, train_loss=0.625, val_acc=0.497, val_loss=1.24]
Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.
Traceback (most recent call last):
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/train/train_PLANARITY_graph_classification.py", line 27, in train_epoch_sparse
    batch_pos_enc = batch_graphs.ndata['pos_enc'].to(device)
  File "/afs/crc.nd.edu/user/p/psoga/.conda/envs/gnn/lib/python3.10/site-packages/dgl/view.py", line 67, in __getitem__
    return self._graph._get_n_repr(self._ntid, self._nodes)[key]
  File "/afs/crc.nd.edu/user/p/psoga/.conda/envs/gnn/lib/python3.10/site-packages/dgl/frame.py", line 523, in __getitem__
    return self._columns[name].data
KeyError: 'pos_enc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_PLANARITY_graph_classification.py", line 420, in <module>
    main()    
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_PLANARITY_graph_classification.py", line 415, in main
    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_PLANARITY_graph_classification.py", line 157, in train_val_pipeline
    epoch_train_loss, epoch_train_acc, optimizer = train_epoch(model, optimizer, device, train_loader, epoch)
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/train/train_PLANARITY_graph_classification.py", line 33, in train_epoch_sparse
    batch_scores = model.forward(batch_graphs, batch_x, batch_e)
  File "/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/nets/PLANARITY_graph_classification/graph_transformer.py", line 90, in forward
torch._C._LinAlgError: linalg.solve: The diagonal element 2625 is zero, the solve could not be completed because the input matrix is singular.
