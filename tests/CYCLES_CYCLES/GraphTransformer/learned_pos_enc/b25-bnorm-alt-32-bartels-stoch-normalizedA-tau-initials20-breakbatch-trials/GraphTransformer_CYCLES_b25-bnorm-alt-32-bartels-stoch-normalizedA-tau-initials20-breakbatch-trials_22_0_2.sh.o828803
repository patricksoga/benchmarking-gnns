/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:351: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:351: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/.conda/envs/gnn/lib/python3.10/site-packages/dgl/heterograph.py:3719: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:

	DGLGraph.adjacency_matrix(transpose, scipy_fmt="csr").

  dgl_warning('DGLGraph.adjacency_matrix_scipy is deprecated. '
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/layers/pe_layer.py:370: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352645774/work/aten/src/ATen/native/Copy.cpp:250.)
  pe = pe.transpose(1, 0).type(torch.float32)
[I] Loading dataset CYCLES...
train, test, val sizes : 9000 10000 1000
[I] Finished loading.
[I] Data load time: 12.6168s
Epoch 00017: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.
Epoch 00066: reducing learning rate of group 0 to 3.1250e-05.
Epoch 00077: reducing learning rate of group 0 to 1.5625e-05.
Epoch 00091: reducing learning rate of group 0 to 7.8125e-06.
Epoch 00111: reducing learning rate of group 0 to 3.9063e-06.
Epoch 00122: reducing learning rate of group 0 to 1.9531e-06.
Epoch 00138: reducing learning rate of group 0 to 9.7656e-07.
