/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:351: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/main_CYCLES_graph_classification.py:351: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  num_classes = len(np.unique(np.array(dataset.train[:][1])))
/afs/crc.nd.edu/user/p/psoga/benchmarking-gnns/data/positional_encs.py:204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352645774/work/torch/csrc/utils/tensor_numpy.cpp:172.)
  pe = torch.from_numpy(pe_mat.diagonal()).float()
[I] Loading dataset CYCLES...
train, test, val sizes : 9000 10000 1000
[I] Finished loading.
[I] Data load time: 13.3115s
using 1 automata/automaton
using 1 automata/automaton
Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00059: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00070: reducing learning rate of group 0 to 6.2500e-05.
Epoch 00081: reducing learning rate of group 0 to 3.1250e-05.
Epoch 00092: reducing learning rate of group 0 to 1.5625e-05.
Epoch 00103: reducing learning rate of group 0 to 7.8125e-06.
Epoch 00114: reducing learning rate of group 0 to 3.9063e-06.
Epoch 00125: reducing learning rate of group 0 to 1.9531e-06.
Epoch 00136: reducing learning rate of group 0 to 9.7656e-07.
